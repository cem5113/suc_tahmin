#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# update_911_daily.py â€” Girdi: sf_crime.csv â†’ Ã‡Ä±ktÄ±lar:
#   1) fr_crime_events_daily.csv  (event-level + gÃ¼nlÃ¼k 911 Ã¶zellikleri)
#   2) fr_crime_grid_daily.csv    (GEOIDÃ—date grid + gÃ¼nlÃ¼k 911 Ã¶zellikleri)
#
# Notlar:
# - 911 ham/Ã¶zetten datetime â†’ date; GEOIDÃ—date sayÄ±mÄ± (n_911_day) + last1d/3d/7d (shift(1))
# - Join yalnÄ±zca (GEOID, date) ile yapÄ±lÄ±r (saat Ã¶nemsiz)
# - GEOID yoksa lat/lon â†’ GEOID eÅŸlemesi yapÄ±lÄ±r (census geojson gerekir)

from __future__ import annotations
import os, zipfile
from pathlib import Path
from typing import Optional
import pandas as pd
import geopandas as gpd

# =========================
# BASIC UTILS
# =========================
def log(msg: str): print(msg, flush=True)

def ensure_parent(path: str):
    Path(path).parent.mkdir(parents=True, exist_ok=True)

def safe_save_csv(df: pd.DataFrame, path: str):
    try:
        ensure_parent(path)
        tmp = path + ".tmp"
        df.to_csv(tmp, index=False)
        os.replace(tmp, path)
        log(f"ðŸ’¾ Kaydedildi: {path} (satÄ±r={len(df):,})")
    except Exception as e:
        b = path + ".bak"
        try:
            df.to_csv(b, index=False)
        except Exception:
            pass
        log(f"âŒ Kaydetme hatasÄ±: {path} â€” Yedek: {b}\n{e}")

def to_date(s):
    return pd.to_datetime(s, errors="coerce").dt.date

def normalize_geoid(s: pd.Series, target_len: int) -> pd.Series:
    s = s.astype(str).str.extract(r"(\d+)", expand=False)
    return s.str[:target_len].str.zfill(target_len)

def first_existing(paths) -> Optional[Path]:
    for p in paths:
        if p and Path(p).exists():
            return Path(p)
    return None

# =========================
# CONFIG
# =========================
ARTIFACT_ZIP = Path(os.getenv("ARTIFACT_ZIP", "artifact/sf-crime-pipeline-output.zip"))
ARTIFACT_DIR = Path(os.getenv("ARTIFACT_DIR", "artifact_unzipped"))

# GÄ°RÄ°Åž
INPUT_CRIME_FILENAME = os.getenv("FR_CRIME_FILE", "sf_crime.csv")

# Ã‡IKIÅžLAR
OUTPUT_EVENTS_DAILY  = os.getenv("FR_EVENTS_DAILY_OUT", "fr_crime_events_daily.csv")
OUTPUT_GRID_DAILY    = os.getenv("FR_GRID_DAILY_OUT",   "fr_crime_grid_daily.csv")
OUTPUT_DIR = Path(os.getenv("FR_OUTPUT_DIR", "crime_prediction_data"))

# Aday yollar
def build_candidates():
    return {
        "FR_911": [
            ARTIFACT_DIR / "sf_911_last_5_year_y.csv",
            ARTIFACT_DIR / "sf_911_last_5_year.csv",
            Path("crime_prediction_data") / "sf_911_last_5_year_y.csv",
            Path("crime_prediction_data") / "sf_911_last_5_year.csv",
        ],
        "CENSUS": [
            ARTIFACT_DIR / "sf_census_blocks_with_population.geojson",
            Path("crime_prediction_data") / "sf_census_blocks_with_population.geojson",
            Path("./sf_census_blocks_with_population.geojson"),
        ],
        "CRIME": [
            ARTIFACT_DIR / INPUT_CRIME_FILENAME,
            Path("crime_prediction_data") / INPUT_CRIME_FILENAME,
            Path(INPUT_CRIME_FILENAME),
        ],
    }

# =========================
# ZIP HELPERS
# =========================
def _is_within_directory(directory: Path, target: Path) -> bool:
    try:
        directory = directory.resolve()
        target = target.resolve()
        return str(target).startswith(str(directory))
    except Exception:
        return False

def safe_unzip(zip_path: Path, dest_dir: Path):
    if not zip_path.exists():
        log(f"â„¹ï¸ Artifact ZIP bulunamadÄ±: {zip_path} â€” klasÃ¶rlerden denenecek.")
        return
    dest_dir.mkdir(parents=True, exist_ok=True)
    log(f"ðŸ“¦ ZIP aÃ§Ä±lÄ±yor: {zip_path} â†’ {dest_dir}")
    with zipfile.ZipFile(zip_path, 'r') as zf:
        for m in zf.infolist():
            out_path = dest_dir / m.filename
            if not _is_within_directory(dest_dir, out_path.parent):
                raise RuntimeError(f"Zip path outside target dir engellendi: {m.filename}")
            if m.is_dir():
                out_path.mkdir(parents=True, exist_ok=True); continue
            out_path.parent.mkdir(parents=True, exist_ok=True)
            with zf.open(m, 'r') as src, open(out_path, 'wb') as dst:
                dst.write(src.read())
    log("âœ… ZIP Ã§Ä±karma tamam.")

# =========================
# GEO HELPERS
# =========================
def _load_blocks(CENSUS_GEOJSON_CANDIDATES) -> tuple[gpd.GeoDataFrame, int]:
    census_path = first_existing(CENSUS_GEOJSON_CANDIDATES)
    if census_path is None:
        raise FileNotFoundError("âŒ GEOID poligon dosyasÄ± yok: sf_census_blocks_with_population.geojson")

    gdf_blocks = gpd.read_file(census_path)
    if "GEOID" not in gdf_blocks.columns:
        cand = [c for c in gdf_blocks.columns if str(c).upper().startswith("GEOID")]
        if not cand:
            raise ValueError("GeoJSON iÃ§inde GEOID benzeri sÃ¼tun yok.")
        gdf_blocks = gdf_blocks.rename(columns={cand[0]: "GEOID"})

    tlen = gdf_blocks["GEOID"].astype(str).str.len().mode().iat[0]
    gdf_blocks["GEOID"] = normalize_geoid(gdf_blocks["GEOID"], int(tlen))

    if gdf_blocks.crs is None:
        gdf_blocks.set_crs("EPSG:4326", inplace=True)
    elif gdf_blocks.crs.to_epsg() != 4326:
        gdf_blocks = gdf_blocks.to_crs(4326)
    return gdf_blocks, int(tlen)

def ensure_geoid_from_latlon(df: pd.DataFrame, CENSUS_GEOJSON_CANDIDATES) -> pd.DataFrame:
    if "GEOID" in df.columns and df["GEOID"].notna().any():
        return df

    lat_col = next((c for c in ["latitude", "lat", "y"] if c in df.columns), None)
    lon_col = next((c for c in ["longitude", "lon", "x"] if c in df.columns), None)
    if not lat_col or not lon_col:
        raise ValueError("âŒ Veride GEOID yok ve lat/lon bulunamadÄ±.")

    gdf_blocks, tlen = _load_blocks(CANDS["CENSUS"])

    tmp = df.copy()
    tmp[lat_col] = pd.to_numeric(tmp[lat_col], errors="coerce")
    tmp[lon_col] = pd.to_numeric(tmp[lon_col], errors="coerce")
    tmp = tmp.dropna(subset=[lat_col, lon_col]).copy()

    pts = gpd.GeoDataFrame(tmp, geometry=gpd.points_from_xy(tmp[lon_col], tmp[lat_col]), crs="EPSG:4326")
    joined = gpd.sjoin(pts, gdf_blocks[["GEOID", "geometry"]], how="left", predicate="within")
    out = pd.DataFrame(joined.drop(columns=["geometry", "index_right"], errors="ignore"))
    out["GEOID"] = normalize_geoid(out["GEOID"], tlen)
    return out

# =========================
# 911 DAILY SUMMARY (NO HOURS)
# =========================
def read_911_daily(FR_911_CANDIDATES, CENSUS_GEOJSON_CANDIDATES) -> pd.DataFrame:
    src = first_existing(FR_911_CANDIDATES)
    if src is None:
        raise FileNotFoundError("âŒ 911 verisi bulunamadÄ± (zip veya klasÃ¶r).")
    log(f"ðŸ“¥ 911 kaynaÄŸÄ± yÃ¼kleniyor: {src}")

    df = pd.read_csv(src, low_memory=False, dtype={"GEOID":"string"})
    # Zaman kolonu tespiti
    ts_col = next((c for c in ["received_time","received_datetime","datetime","timestamp",
                               "call_received_datetime","date"] if c in df.columns), None)
    if ts_col is None:
        raise ValueError("911 verisinde datetime/iÃ§eren bir zaman kolonu bulunamadÄ±.")

    # GEOID yoksa lat/lon â†’ GEOID
    if "GEOID" not in df.columns or df["GEOID"].isna().all():
        log("â„¹ï¸ 911 verisinde GEOID yok; lat/lon â†’ GEOID hesaplanacak.")
        df = ensure_geoid_from_latlon(df, CANDS["CENSUS"])

    # date Ã¼ret
    df["date"] = to_date(df[ts_col])
    df = df.dropna(subset=["GEOID","date"]).copy()

    # GÃ¼nlÃ¼k sayÄ±m (GEOID Ã— date)
    day = (df.groupby(["GEOID","date"], as_index=False)
             .size()
             .rename(columns={"size":"n_911_day"}))

    # Rolling (sÄ±zÄ±ntÄ±sÄ±z: shift(1))
    day = day.sort_values(["GEOID","date"]).reset_index(drop=True)

    day["n_911_last1d"] = (
        day.groupby("GEOID")["n_911_day"]
           .transform(lambda s: s.shift(1).fillna(0))
    ).astype("float32")

    def roll_sum(s: pd.Series, W: int) -> pd.Series:
        return s.shift(1).rolling(W, min_periods=1).sum()

    day["n_911_last3d"] = (
        day.groupby("GEOID")["n_911_day"].transform(lambda s: roll_sum(s, 3)).fillna(0)
    ).astype("float32")

    day["n_911_last7d"] = (
        day.groupby("GEOID")["n_911_day"].transform(lambda s: roll_sum(s, 7)).fillna(0)
    ).astype("float32")

    return day

# =========================
# MAIN
# =========================
def main():
    global CANDS  # ensure_geoid_from_latlon iÃ§inde kullanÄ±lÄ±yor

    # 0) ZIP varsa Ã§Ä±kar
    safe_unzip(ARTIFACT_ZIP, ARTIFACT_DIR)

    # 1) Aday yollar
    CANDS = build_candidates()

    # 2) 911 gÃ¼nlÃ¼k Ã¶zet
    fr911_daily = read_911_daily(CANDS["FR_911"], CANDS["CENSUS"])
    log(f"ðŸ“Š 911 gÃ¼nlÃ¼k Ã¶zet: {fr911_daily.shape[0]:,} satÄ±r Ã— {fr911_daily.shape[1]} sÃ¼tun")

    # 3) sf_crime.csv (girdi)
    crime_path = first_existing(CANDS["CRIME"])
    if crime_path is None:
        raise FileNotFoundError("âŒ sf_crime.csv bulunamadÄ±.")
    crime = pd.read_csv(crime_path, low_memory=False, dtype={"GEOID":"string"})
    log(f"ðŸ“¥ sf_crime.csv: {crime_path} â€” satÄ±r: {len(crime):,}")

    # 4) GEOID normalize / gerekirse lat-lon â†’ GEOID
    try:
        gdf_blocks, tlen = _load_blocks(CANDS["CENSUS"])
    except Exception:
        tlen = 11
    if "GEOID" in crime.columns and crime["GEOID"].notna().any():
        crime["GEOID"] = normalize_geoid(crime["GEOID"], tlen)
    else:
        log("â„¹ï¸ sf_crime: GEOID yok; lat/lon â†’ GEOID hesaplanacak.")
        crime = ensure_geoid_from_latlon(crime, CANDS["CENSUS"])
        crime["GEOID"] = normalize_geoid(crime["GEOID"], tlen)

    # 5) DATE kolonu (olay saati Ã–NEMSÄ°Z; datetime â†’ date)
    if "date" in crime.columns:
        crime["date"] = to_date(crime["date"])
    else:
        dt_col = next((c for c in ["datetime","event_datetime","occurred_at","timestamp"] if c in crime.columns), None)
        if dt_col is None:
            raise ValueError("âŒ sf_crime.csv iÃ§inde 'date' veya 'datetime' benzeri bir kolon yok.")
        crime["date"] = to_date(crime[dt_col])

    # 6) EVENTS_DAILY: (GEOID, date) ile 911 join
    keys = ["GEOID","date"]
    events_daily = crime.merge(fr911_daily, on=keys, how="left")
    for c in ["n_911_day","n_911_last1d","n_911_last3d","n_911_last7d"]:
        if c in events_daily.columns:
            events_daily[c] = pd.to_numeric(events_daily[c], errors="coerce").fillna(0)

    # 7) GRID_DAILY: olaylarÄ± GEOIDÃ—date topla + 911 Ã¶zetlerini eÅŸleÅŸtir
    #    - crime_count_day: o gÃ¼n GEOID iÃ§indeki olay sayÄ±sÄ±
    #    - Y_day: (crime_count_day > 0) ikili etiket (iÅŸine yarÄ±yorsa)
    agg_crime = (events_daily
                 .groupby(keys, as_index=False)
                 .size()
                 .rename(columns={"size":"crime_count_day"}))
    grid_daily = agg_crime.merge(fr911_daily, on=keys, how="left")
    grid_daily["crime_count_day"] = pd.to_numeric(grid_daily["crime_count_day"], errors="coerce").fillna(0).astype(int)
    grid_daily["Y_day"] = (grid_daily["crime_count_day"] > 0).astype("int8")

    for c in ["n_911_day","n_911_last1d","n_911_last3d","n_911_last7d"]:
        if c in grid_daily.columns:
            grid_daily[c] = pd.to_numeric(grid_daily[c], errors="coerce").fillna(0)

    # 8) Yaz
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    out_events = OUTPUT_DIR / OUTPUT_EVENTS_DAILY
    out_grid   = OUTPUT_DIR / OUTPUT_GRID_DAILY

    safe_save_csv(events_daily, str(out_events))
    safe_save_csv(grid_daily,   str(out_grid))

    # 9) KÄ±sa Ã¶nizleme
    try:
        log("â€”â€” fr_crime_events_daily.csv â€” Ã¶rnek â€”")
        cols = ["GEOID","date","n_911_day","n_911_last1d","n_911_last3d","n_911_last7d"]
        log(events_daily[[c for c in cols if c in events_daily.columns]].head(8).to_string(index=False))
    except Exception:
        pass
    try:
        log("â€”â€” fr_crime_grid_daily.csv â€” Ã¶rnek â€”")
        cols = ["GEOID","date","crime_count_day","Y_day","n_911_day","n_911_last1d","n_911_last3d","n_911_last7d"]
        log(grid_daily[[c for c in cols if c in grid_daily.columns]].head(8).to_string(index=False))
    except Exception:
        pass

if __name__ == "__main__":
    main()
