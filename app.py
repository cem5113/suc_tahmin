from __future__ import annotations
from typing import Optional, Union, Dict, List, Tuple, Any

import streamlit as st
import pandas as pd
import requests
import re
import os, json, subprocess, sys
from pathlib import Path
import io, zipfile
from datetime import datetime, timezone

import matplotlib.pyplot as plt
import numpy as np  # ‚ñ∂Ô∏è eklendi: a≈üaƒüƒ±da np.nanmean vb. kullanƒ±lƒ±yor

# --- Forensic rapor yardƒ±mcƒ± (varsa import et, yoksa stub kullan) ---
try:
    from scripts.forensic_report import build_forensic_report
except Exception:
    def build_forensic_report(**kwargs):
        return None

# -----------------------------------------------------------------------------
# Global Kurulum
# -----------------------------------------------------------------------------
st.set_page_config(page_title="Veri G√ºncelleme", layout="wide")

try:
    ROOT = Path(__file__).resolve().parent
except NameError:
    ROOT = Path.cwd()

DATA_DIR = ROOT / "crime_prediction_data"
SCRIPTS_DIR = ROOT / "scripts"
DATA_DIR.mkdir(parents=True, exist_ok=True)
SCRIPTS_DIR.mkdir(parents=True, exist_ok=True)
SEARCH_DIRS = [SCRIPTS_DIR, ROOT]

PIPELINE = [
    {"name": "update_crime",      "alts": ["build_crime_grid", "crime_grid_build"]},
    {"name": "update_911",        "alts": ["enrich_911"]},
    {"name": "update_311",        "alts": ["enrich_311"]},
    {"name": "update_population", "alts": ["enrich_population"]},
    {"name": "update_bus",        "alts": ["enrich_bus"]},
    {"name": "update_train",      "alts": ["enrich_train"]},
    {"name": "update_poi",        "alts": ["pipeline_make_sf_crime_06", "app_poi_to_06", "enrich_poi"]},
    {"name": "update_police_gov", "alts": ["enrich_police_gov_06_to_07", "enrich_police_gov", "enrich_police"]},
    {"name": "update_weather",    "alts": ["enrich_weather"]},
]

# -----------------------------------------------------------------------------
# AƒûIR BAƒûIMLILIKLAR ƒ∞√áƒ∞N LAZY IMPORT
# -----------------------------------------------------------------------------
def _load_ml_deps():
    try:
        import shap
        from sklearn.model_selection import TimeSeriesSplit
        from sklearn.metrics import roc_auc_score, brier_score_loss, mean_absolute_error
        from sklearn.calibration import CalibratedClassifierCV
        from sklearn.inspection import PartialDependenceDisplay
        from lightgbm import LGBMClassifier, LGBMRegressor
        from lime.lime_tabular import LimeTabularExplainer
        return {
            "np": np,
            "shap": shap,
            "TimeSeriesSplit": TimeSeriesSplit,
            "roc_auc_score": roc_auc_score,
            "brier_score_loss": brier_score_loss,
            "mean_absolute_error": mean_absolute_error,
            "CalibratedClassifierCV": CalibratedClassifierCV,
            "PartialDependenceDisplay": PartialDependenceDisplay,
            "LGBMClassifier": LGBMClassifier,
            "LGBMRegressor": LGBMRegressor,
            "LimeTabularExplainer": LimeTabularExplainer,
        }
    except ModuleNotFoundError as e:
        missing = getattr(e, "name", "bir paket")
        st.error(
            f"üß± Gerekli paket eksik: **{missing}**. "
            "L√ºtfen sol men√ºden **'0) Gereklilikleri y√ºkle'** d√ºƒümesini kullanƒ±n "
            "ve kurulumdan sonra **Rerun** yapƒ±n."
        )
        st.stop()

# -----------------------------------------------------------------------------
# Yardƒ±mcƒ±lar
# -----------------------------------------------------------------------------
def pick_url(key: str, default: str) -> str:
    # √ñncelik: 1) st.secrets  2) ENV  3) default
    try:
        if key in st.secrets and st.secrets[key]:
            return str(st.secrets[key])
    except Exception:
        pass
    return os.getenv(key, default)

def _mask_token(u: str) -> str:
    try:
        return re.sub(r'(\$\$app_token=)[^&]+', r'\1‚Ä¢‚Ä¢‚Ä¢', str(u))
    except Exception:
        return str(u)

def _human_bytes(n: int) -> str:
    if n is None:
        return "-"
    step = 1024.0
    for u in ["B", "KB", "MB", "GB", "TB"]:
        if n < step:
            return f"{n:.0f} {u}" if u == "B" else f"{n:.1f} {u}"
        n /= step
    return f"{n:.1f} PB"

def _fmt_dt(ts: Optional[float]) -> str:
    if ts is None:
        return "-"
    dt = datetime.fromtimestamp(ts, tz=timezone.utc).astimezone()
    return dt.strftime("%Y-%m-%d %H:%M:%S")

def _age_str(ts: Optional[float]) -> str:
    if ts is None:
        return "-"
    delta = datetime.now().timestamp() - ts
    if delta < 60:
        return f"{int(delta)} sn"
    if delta < 3600:
        return f"{int(delta // 60)} dk"
    if delta < 86400:
        return f"{int(delta // 3600)} sa"
    return f"{int(delta // 86400)} g"

def ensure_script(filename: str) -> Optional[Path]:
    """
    Verilen dosya adƒ±nƒ± SEARCH_DIRS i√ßinde arar; bulursa Path d√∂ner, yoksa None.
    """
    cand = filename if filename.endswith(".py") else f"{filename}.py"
    for base in SEARCH_DIRS:
        p = Path(base) / cand
        if p.exists() and p.is_file():
            return p
    return None

def _candidate_names(base: str, locale: str) -> List[str]:
    if locale and locale != "default":
        return [
            f"{base}.fr.py",
            f"{base}_fr.py",
            f"{base}.{locale}.py",
            f"{base}-{locale}.py",
            f"{base}.py",
        ]
    return [f"{base}.py"]

def resolve_script(entry: dict, locale: str = "default") -> Optional[Path]:
    # 1) asƒ±l ad i√ßin
    for cand in _candidate_names(entry["name"], locale):
        p = ensure_script(cand)
        if p:
            return p
    # 2) alternatif adlar i√ßin
    for alt in entry.get("alts", []):
        for cand in _candidate_names(alt, locale):
            pp = ensure_script(cand)
            if pp:
                return pp
    return None

def run_script(path: Path) -> bool:
    st.write(f"‚ñ∂Ô∏è {path.name} √ßalƒ±≈ütƒ±rƒ±lƒ±yor‚Ä¶")
    placeholder = st.empty()
    lines: List[str] = []
    try:
        proc = subprocess.Popen(
            [sys.executable, "-u", str(path)],
            cwd=str(ROOT),
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            env={**os.environ, "PYTHONUNBUFFERED": "1"},
        )
        while True:
            line = proc.stdout.readline()
            if not line and proc.poll() is not None:
                break
            if line:
                lines.append(line.rstrip())
                lines = lines[-400:]
                placeholder.code("\n".join(lines))
        rc = proc.wait()
        if rc == 0:
            st.success(f"‚úÖ {path.name} tamamlandƒ±")
            return True
        else:
            st.error(f"‚ùå {path.name} hata verdi (exit={rc})")
            return False
    except Exception as e:
        st.error(f"üö® {path.name} √ßaƒürƒ±lamadƒ±: {e}")
        return False

# -----------------------------------------------------------------------------
# ENV ve URL‚Äôler
# -----------------------------------------------------------------------------
CRIME_CSV_LATEST = pick_url(
    "CRIME_CSV_URL",
    "https://github.com/cem5113/crime_prediction_data/releases/latest/download/sf_crime_y.csv",
)

RAW_911_URL = pick_url(
    "RAW_911_URL",
    "https://github.com/cem5113/crime_prediction_data/releases/download/v1.0.1/sf_911_last_5_year_y.csv",
)

SF311_URL = pick_url(
    "SF311_URL",
    "https://github.com/cem5113/crime_prediction_data/releases/download/v1.0.2/sf_311_last_5_years_y.csv",
)

# CSV-ONLY: N√ºfus verisi yerel dosyadan okunacak
DEFAULT_POP_CSV = str((Path(os.environ.get("CRIME_DATA_DIR", "crime_prediction_data")) / "sf_population.csv").resolve())
POPULATION_PATH = pick_url("POPULATION_PATH", DEFAULT_POP_CSV)

# G√ºvenlik: URL verilirse reddet (CSV-only mod)
if re.match(r"^https?://", str(POPULATION_PATH), flags=re.I):
    POPULATION_PATH = DEFAULT_POP_CSV

os.environ["POPULATION_PATH"] = str(POPULATION_PATH)

# ‚¨áÔ∏è 911 artƒ±mlƒ± √ßekim i√ßin API ayarlarƒ±
SF911_API_URL       = pick_url("SF911_API_URL", "https://data.sfgov.org/resource/2zdj-bwza.json")
SF911_AGENCY_FILTER = pick_url("SF911_AGENCY_FILTER", "agency like '%Police%'")
SF911_API_TOKEN     = pick_url("SF911_API_TOKEN", "")

# √áocuk s√ºre√ßlerin de aynƒ± deƒüerleri g√∂rmesi i√ßin ENV
os.environ["CRIME_CSV_URL"] = CRIME_CSV_LATEST
os.environ["RAW_911_URL"]   = RAW_911_URL
os.environ["SF311_URL"]     = SF311_URL
os.environ["GEOID_LEN"]     = os.environ.get("GEOID_LEN", "11")

GEOID_LEN = int(os.environ.get("GEOID_LEN", "11"))

def _norm_geoid(s: pd.Series, L: int = GEOID_LEN) -> pd.Series:
    return (
        s.astype(str)
         .str.extract(r"(\d+)", expand=False)
         .str[:L]
         .str.zfill(L)
    )

os.environ["SF911_API_URL"]       = SF911_API_URL
os.environ["SF911_AGENCY_FILTER"] = SF911_AGENCY_FILTER
if SF911_API_TOKEN:
    os.environ["SF911_API_TOKEN"] = SF911_API_TOKEN

SOCS_APP_TOKEN = st.secrets.get("SOCS_APP_TOKEN", os.environ.get("SOCS_APP_TOKEN", ""))
if SOCS_APP_TOKEN:
    os.environ["SOCS_APP_TOKEN"] = SOCS_APP_TOKEN

# LATEST veya 2022/2023 gibi belirli yƒ±l
os.environ["ACS_YEAR"] = st.secrets.get("ACS_YEAR", os.environ.get("ACS_YEAR", "LATEST"))

# Virg√ºlle filtre (bo≈ü bƒ±rak = t√ºm kategoriler)
os.environ["DEMOG_WHITELIST"] = st.secrets.get(
    "DEMOG_WHITELIST",
    os.environ.get("DEMOG_WHITELIST", "")
)

# --- GitHub Actions entegrasyonu ---
GITHUB_REPO = os.environ.get("GITHUB_REPO", "cem5113/crime_prediction_data")
GITHUB_WORKFLOW = os.environ.get("GITHUB_WORKFLOW", "full_pipeline.yml")

ARTIFACT_NAMES = [
    "crime-pipeline-output",       # ‚úÖ yeni
    "sf-crime-pipeline-output",    # (geri uyum)
]

def _gh_headers():
    token = st.secrets.get("GH_TOKEN") or os.environ.get("GH_TOKEN")
    if not token:
        raise RuntimeError("GH_TOKEN gerekli (Streamlit secrets veya env).")
    return {"Authorization": f"Bearer {token}", "Accept": "application/vnd.github+json"}

def fetch_file_from_latest_artifact(pick_names: List[str], artifact_names: Optional[List[str]] = None) -> bytes | None:
    names_to_try = artifact_names or ARTIFACT_NAMES
    runs_url = f"https://api.github.com/repos/{GITHUB_REPO}/actions/runs?per_page=20"
    runs = requests.get(runs_url, headers=_gh_headers(), timeout=30).json()
    run_ids = [r["id"] for r in runs.get("workflow_runs", []) if r.get("conclusion") == "success"]

    for rid in run_ids:
        arts_url = f"https://api.github.com/repos/{GITHUB_REPO}/actions/runs/{rid}/artifacts"
        arts = requests.get(arts_url, headers=_gh_headers(), timeout=30).json().get("artifacts", [])
        for a in arts:
            if a.get("expired", False):
                continue
            if a.get("name") not in names_to_try:
                continue
            dl = requests.get(a["archive_download_url"], headers=_gh_headers(), timeout=60)
            zf = zipfile.ZipFile(io.BytesIO(dl.content))
            names = zf.namelist()

            # 1) Doƒürudan dosya aramasƒ±
            for pick in pick_names:
                for cand in (pick, f"crime_prediction_data/{pick}"):
                    if cand in names:
                        return zf.read(cand)

            # 2) "paquet_run_*.zip" i√ßinden ara (artifact ZIP'i i√ßinde ikinci bir zip olabilir)
            paquet_inner = [n for n in names if re.search(r"^paquet_run_\d+\.zip$", n)]
            for pin in paquet_inner:
                with zf.open(pin) as inner_blob:
                    with zipfile.ZipFile(io.BytesIO(inner_blob.read())) as inner_zip:
                        inner_names = inner_zip.namelist()
                        for pick in pick_names:
                            # hem d√ºz ad hem de fr_eda/ altƒ±ndan e≈üle≈üme
                            exacts = [pick, f"fr_eda/{pick}", f"crime_prediction_data/{pick}"]
                            for ex in exacts:
                                if ex in inner_names:
                                    return inner_zip.read(ex)
                        # adƒ±n sonu e≈üle≈üsin (fallback)
                        for n in inner_names:
                            if any(n.endswith(p) for p in pick_names):
                                return inner_zip.read(n)

            # 3) Son-√ßare: isim sonu e≈üle≈ümesi
            for n in names:
                if any(n.endswith(p) for p in pick_names):
                    return zf.read(n)
    return None

def _resolve_workflow_id(target: str):
    import requests, os
    url = f"https://api.github.com/repos/{GITHUB_REPO}/actions/workflows?per_page=100"
    r = requests.get(url, headers=_gh_headers(), timeout=30); r.raise_for_status()
    ws = r.json().get("workflows", [])
    # 1) Dosya adƒ±yla e≈üle≈üme
    for w in ws:
        if os.path.basename(str(w.get("path",""))) == target:
            return w.get("id")
    # 2) G√∂r√ºnen adla e≈üle≈üme (yedek)
    for w in ws:
        if str(w.get("name","")).strip().lower() == target.strip().lower():
            return w.get("id")
    return None

def dispatch_workflow(persist="artifact", force=True, ref="main", variant=None, top_k=None):
    import json as _json, requests
    target = os.environ.get("GITHUB_WORKFLOW", "full_pipeline.yml")
    wid = _resolve_workflow_id(target)
    if not wid:
        return {"ok": False, "status": 404, "text": f"Workflow bulunamadƒ±: {target}"}
    url = f"https://api.github.com/repos/{GITHUB_REPO}/actions/workflows/{wid}/dispatches"
    inputs = {"persist": persist, "force": "true" if force else "false"}
    if variant: inputs["variant"] = str(variant)
    if top_k:   inputs["top_k"]   = str(top_k)
    payload = {"ref": ref, "inputs": inputs}
    r = requests.post(url, headers=_gh_headers(), data=_json.dumps(payload), timeout=30)
    return {"ok": r.status_code in (204, 201), "status": r.status_code, "text": r.text}

def diag_workflow(target: str | None = None, ref: str | None = None):
    """Workflow dispatch 422 tanƒ±sƒ±: dosya adƒ±/ad, branch ve YAML i√ßeriƒüini kontrol eder (PyYAML yoksa YAML analizi atlanƒ±r)."""
    import os, base64, urllib.parse
    import requests
    try:
        import yaml  # PyYAML yoksa YAML analizi atlanƒ±r
    except Exception:
        yaml = None

    try:
        target = target or os.environ.get("GITHUB_WORKFLOW", "full_pipeline.yml")
        ref = ref or "main"

        st.write(f"üéØ Hedef workflow: `{target}` ¬∑ ref: `{ref}`")

        # 1) Workflow listesi
        url_list = f"https://api.github.com/repos/{GITHUB_REPO}/actions/workflows?per_page=100"
        r = requests.get(url_list, headers=_gh_headers(), timeout=30)
        if r.status_code != 200:
            st.error(f"Workflow listesi okunamadƒ±: {r.status_code} {r.text}")
            return
        ws = r.json().get("workflows", []) or []

        if ws:
            st.caption(f"Toplam {len(ws)} workflow bulundu.")
            listing = "\n".join([f"- {w.get('name')}  | id={w.get('id')}  | path={w.get('path')}" for w in ws])
            st.code(listing if listing else "-", language="text")
        else:
            st.warning("Repo‚Äôda hi√ß workflow g√∂r√ºnm√ºyor.")
            return

        # 2) ID √ß√∂z: dosya adƒ± -> g√∂r√ºn√ºr ad
        wid = None
        for w in ws:
            if os.path.basename(str(w.get("path",""))) == target:
                wid = w.get("id")
                break
        if wid is None:
            for w in ws:
                if str(w.get("name","")).strip().lower() == target.strip().lower():
                    wid = w.get("id")
                    break
        if wid is None:
            st.error(f"‚ùå Hedef workflow bulunamadƒ±: `{target}` (dosya adƒ± ya da g√∂r√ºnen ad e≈üle≈ümedi)")
            return
        st.success(f"‚úÖ √á√∂z√ºlen workflow id: {wid}")

        # 3) YAML i√ßeriƒüi (contents API)
        url_w = f"https://api.github.com/repos/{GITHUB_REPO}/actions/workflows/{wid}"
        rw = requests.get(url_w, headers=_gh_headers(), timeout=30)
        if rw.status_code != 200:
            st.warning(f"Workflow ayrƒ±ntƒ±sƒ± okunamadƒ±: {rw.status_code} {rw.text}")
            return
        wpath = rw.json().get("path", "")
        st.write(f"üóÇÔ∏è Dosya yolu: `{wpath}`")

        url_contents = f"https://api.github.com/repos/{GITHUB_REPO}/contents/{urllib.parse.quote(wpath)}?ref={urllib.parse.quote(ref)}"
        rc = requests.get(url_contents, headers=_gh_headers(), timeout=30)
        if rc.status_code != 200 or rc.json().get("encoding") != "base64":
            st.error(f"YAML okunamadƒ±: {rc.status_code} {rc.text[:200]}")
            return
        yaml_text = base64.b64decode(rc.json()["content"]).decode("utf-8", errors="replace")
        st.code(yaml_text, language="yaml")

        # 4) (opsiyonel) YAML parse edip workflow_dispatch kontrol√º
        if yaml is not None:
            try:
                y = yaml.safe_load(yaml_text) or {}
                on_section = y.get("on") or {}
                has_dispatch = False
                inputs_info = {}

                if isinstance(on_section, dict):
                    if "workflow_dispatch" in on_section:
                        has_dispatch = True
                        wd = on_section.get("workflow_dispatch") or {}
                        if isinstance(wd, dict):
                            inputs_info = wd.get("inputs", {}) or {}
                elif isinstance(on_section, list):
                    has_dispatch = "workflow_dispatch" in on_section

                if has_dispatch:
                    st.success("‚úÖ Bu workflow YAML'ƒ±nda `workflow_dispatch` tanƒ±mlƒ±.")
                else:
                    st.error("‚ùå Bu workflow YAML'ƒ±nda `workflow_dispatch` YOK. 422‚Äônin ana nedeni bu.")

                if inputs_info:
                    st.write("üß© `workflow_dispatch.inputs` tanƒ±mƒ±:")
                    st.json(inputs_info)
                else:
                    st.caption("Bu workflow i√ßin √∂zel inputs tanƒ±mƒ± yok (veya bo≈ü).")
            except Exception as e:
                st.warning(f"YAML parse edilemedi: {e}")
        else:
            st.info("PyYAML y√ºkl√º deƒüil; YAML analizi atlandƒ±. (ƒ∞stersen requirements.txt ‚Üí PyYAML ekleyebilirsin)")

        # 5) Branch kontrol√º
        url_branch = f"https://api.github.com/repos/{GITHUB_REPO}/branches/{urllib.parse.quote(ref)}"
        rb = requests.get(url_branch, headers=_gh_headers(), timeout=30)
        if rb.status_code == 200:
            st.success(f"‚úÖ Branch mevcut: {ref}")
        else:
            st.warning(f"‚ö†Ô∏è Branch bulunamadƒ±: {ref} ({rb.status_code})")

        # 6) √ñrnek payload bilgisi
        st.write({"ref": ref, "inputs": {"persist": "artifact", "force": "true"}})
        st.caption(f"Endpoint: /repos/{GITHUB_REPO}/actions/workflows/{wid}/dispatches")

    except Exception as e:
        st.error(f"Diag hata: {e}")

def _get_last_run_by_workflow():
    url = f"https://api.github.com/repos/{GITHUB_REPO}/actions/workflows/{GITHUB_WORKFLOW}/runs?per_page=1"
    r = requests.get(url, headers=_gh_headers(), timeout=30)
    if r.status_code != 200:
        return None, r.status_code, r.text
    arr = r.json().get("workflow_runs", [])
    return (arr[0] if arr else None), 200, ""

def _render_last_run_status(container):
    if not (st.secrets.get("GH_TOKEN") or os.environ.get("GH_TOKEN")):
        container.info("GH_TOKEN yok; GitHub durumunu okuyamƒ±yorum.")
        return
    try:
        run, code, msg = _get_last_run_by_workflow()
        if not run:
            container.info("Bu workflow i√ßin run bulunamadƒ±.")
            return
        status = run.get("status")
        concl  = run.get("conclusion") or "-"
        started = run.get("run_started_at")
        html_url = run.get("html_url")
        container.markdown(
            f"**Son ko≈üum:** `{status}` / `{concl}` ¬∑ ba≈ülama: `{started}`  ¬∑  [GitHub‚Äôda a√ß]({html_url})"
        )
    except Exception as e:
        container.warning(f"Durum okunamadƒ±: {e}")

def fetch_latest_artifact_df() -> Optional[pd.DataFrame]:
    try:
        runs_url = f"https://api.github.com/repos/{GITHUB_REPO}/actions/runs?per_page=20"
        runs = requests.get(runs_url, headers=_gh_headers(), timeout=30).json()
        run_ids = [r["id"] for r in runs.get("workflow_runs", []) if r.get("conclusion") == "success"]

        for rid in run_ids:
            arts_url = f"https://api.github.com/repos/{GITHUB_REPO}/actions/runs/{rid}/artifacts"
            arts = requests.get(arts_url, headers=_gh_headers(), timeout=30).json().get("artifacts", [])
            for a in arts:
                if a.get("expired", False) or a.get("name") not in ARTIFACT_NAMES:
                    continue
                dl = requests.get(a["archive_download_url"], headers=_gh_headers(), timeout=60)
                zf = zipfile.ZipFile(io.BytesIO(dl.content))

                # √ñnce d√ºzden dene
                for pick in ("crime_prediction_data/sf_crime_08.csv", "sf_crime_08.csv"):
                    if pick in zf.namelist():
                        with zf.open(pick) as f:
                            df = pd.read_csv(f, low_memory=False)
                            if "date" in df.columns:
                                df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date
                            elif "datetime" in df.columns:
                                df["date"] = pd.to_datetime(df["datetime"], errors="coerce").dt.date
                            return df

                # paquet_run_*.zip i√ßinden dene
                for n in zf.namelist():
                    if re.search(r"^paquet_run_\d+\.zip$", n):
                        with zf.open(n) as inner_blob:
                            with zipfile.ZipFile(io.BytesIO(inner_blob.read())) as inner_zip:
                                for pick in ("sf_crime_08.csv", "crime_prediction_data/sf_crime_08.csv"):
                                    if pick in inner_zip.namelist():
                                        with inner_zip.open(pick) as f:
                                            df = pd.read_csv(f, low_memory=False)
                                            if "date" in df.columns:
                                                df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date
                                            elif "datetime" in df.columns:
                                                df["date"] = pd.to_datetime(df["datetime"], errors="coerce").dt.date
                                            return df
        return None
    except Exception as e:
        st.warning(f"Artifact indirilemedi: {e}")
        return None

# -----------------------------------------------------------------------------
# 08 ‚Üí 09 D√∂n√º≈ü√ºm Yardƒ±mcƒ±larƒ±
# -----------------------------------------------------------------------------
def _group_rare_labels(
    df: pd.DataFrame,
    col: str,
    min_prop: Optional[float] = None,
    min_count: Optional[int] = None,
    other_label: str = "Other",
    out_stats_path: Optional[Path] = None,
) -> pd.Series:
    if col not in df.columns:
        return pd.Series([None] * len(df), index=df.index)

    s = df[col].astype(str).str.strip()
    total = len(s)
    vc = s.value_counts(dropna=False)

    env_prop = os.environ.get("RARE_MIN_PROP")
    env_count = os.environ.get("RARE_MIN_COUNT")
    if min_prop is None and env_prop:
        try:
            min_prop = float(env_prop)
        except Exception:
            pass
    if min_count is None and env_count:
        try:
            min_count = int(env_count)
        except Exception:
            pass

    if min_prop is None and min_count is None:
        min_prop, min_count = 0.01, 200

    rare_mask = pd.Series(False, index=vc.index)
    if min_prop is not None:
        rare_mask |= (vc / max(total, 1)) < float(min_prop)
    if min_count is not None:
        rare_mask |= vc < int(min_count)

    rare_values = set(vc[rare_mask].index)
    grouped = s.where(~s.isin(rare_values), other_label)

    if out_stats_path is not None:
        out_stats_path.parent.mkdir(parents=True, exist_ok=True)
        stats_df = pd.DataFrame({
            col: vc.index,
            "count": vc.values,
            "prop": vc.values / max(total, 1),
            "is_rare": vc.index.map(lambda v: v in rare_values)
        })
        try:
            stats_df.to_csv(out_stats_path, index=False)
        except Exception:
            pass

    return grouped

def clean_and_save_crime_09(input_obj: Union[str, pd.DataFrame] = "sf_crime_08.csv", output_path: str = "sf_crime_09.csv"):
    # input_obj hem DataFrame hem de dosya yolu olabilir
    if isinstance(input_obj, pd.DataFrame):
        df = input_obj.copy()
    else:
        df = pd.read_csv(input_obj, dtype={"GEOID": str})

    # üõ† FIX-1: GEOID normalizasyonu (zfill kaldƒ±rƒ±ldƒ±, sadece sayƒ±sal √ßekirdek)
    if "GEOID" in df.columns:
        df["GEOID"] = (
            df["GEOID"]
            .astype(str)
            .str.extract(r"(\d+)", expand=False)
        )

    if "category" in df.columns:
        df["category"] = df["category"].astype(str).str.strip().str.title()

    # Rare class grouping
    try:
        out_dir = Path(output_path).parent if isinstance(output_path, str) else Path(".")
        if "category" in df.columns:
            df["category_grouped"] = _group_rare_labels(
                df, "category", min_prop=None, min_count=None,
                other_label="Other", out_stats_path=out_dir / "rare_stats_category.csv"
            )

        if "subcategory" in df.columns:
            df["subcategory"] = df["subcategory"].astype(str).str.strip().str.title()
            df["subcategory_grouped"] = _group_rare_labels(
                df, "subcategory", min_prop=None, min_count=None,
                other_label="Other", out_stats_path=out_dir / "rare_stats_subcategory.csv"
            )

        try:
            st.caption("üîé Rare grouping uygulandƒ± (category/subcategory). ƒ∞statistikler CSV olarak kaydedildi.")
        except Exception:
            pass
    except Exception as _e:
        try:
            st.warning(f"Rare grouping atlandƒ±: {str(_e)}")
        except Exception:
            print(f"Rare grouping atlandƒ±: {_e}")

    # Tip d√∂n√º≈üt√ºr√ºc√ºler
    def to_int(df_, col, default=0):
        if col in df_.columns:
            df_[col] = (
                pd.to_numeric(df_[col], errors="coerce")
                .fillna(default)
                .round()
                .astype("Int64")
            )

    def to_float(df_, col, default=0.0):
        if col in df_.columns:
            df_[col] = (
                pd.to_numeric(df_[col], errors="coerce")
                .fillna(default)
                .astype(float)
            )

    # Saya√ß kolonlarƒ±
    int_count_cols = [
        "crime_count",
        "911_request_count_hour_range",
        "911_request_count_daily(before_24_hours)",
        "311_request_count",
        "bus_stop_count",
        "train_stop_count",
        "poi_total_count",
    ]
    for c in int_count_cols:
        to_int(df, c, default=0)

    # Risk skoru
    to_float(df, "poi_risk_score", default=0.0)

    # Binary kolonlar
    def to_binary(df_, col):
        if col in df_.columns:
            m = {
                "true": 1, "t": 1, "yes": 1, "y": 1, "1": 1, "evet": 1,
                "false": 0, "f": 0, "no": 0, "n": 0, "0": 0, "hayƒ±r": 0, "hayir": 0
            }
            s = df_[col].replace({True: 1, False: 0})
            s = s.astype(str).str.strip().str.lower().map(m)
            df_[col] = pd.to_numeric(s, errors="coerce").fillna(0).astype("Int64")

    for c in ["is_near_police", "is_near_government"]:
        to_binary(df, c)

    # Mesafe kolonlarƒ±
    for c in ["distance_to_bus", "distance_to_train", "distance_to_police", "distance_to_government_building"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce").fillna(9999.0).astype(float)

    # Range kolonlarƒ± (int kategoriler)
    for c in ["bus_stop_count_range", "train_stop_count_range", "poi_total_count_range", "poi_risk_score_range"]:
        to_int(df, c, default=0)

    for c in ["distance_to_bus_range", "distance_to_train_range", "distance_to_police_range", "distance_to_government_building_range"]:
        if c in df.columns:
            s = pd.to_numeric(df[c], errors="coerce")
            max_cat = int(s.max(skipna=True)) if pd.notna(s.max(skipna=True)) else 3
            df[c] = s.fillna(max_cat).round().astype("Int64")

    # N√ºfus (median ile doldur)
    if "population" in df.columns:
        df["population"] = pd.to_numeric(df["population"], errors="coerce")
        median_pop = df["population"].median(skipna=True)
        median_pop = 0 if pd.isna(median_pop) else median_pop
        df["population"] = df["population"].fillna(median_pop)

    # POI dominant type
    if "poi_dominant_type" in df.columns:
        # üõ† FIX-2: Bo≈ü stringleri de None say, sonra "None" ile doldur
        df["poi_dominant_type"] = (
            df["poi_dominant_type"]
            .replace({"": np.nan})
            .fillna("None")
            .astype(str)
        )

    # Tarih normalize
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date
    elif "datetime" in df.columns and "date" not in df.columns:
        df["date"] = pd.to_datetime(df["datetime"], errors="coerce").dt.date

    # üõ† FIX-3: Near-repeat g√ºvenli hale getirildi (crime_count yoksa Y_label'dan t√ºret)
    try:
        need = {"date", "GEOID"}
        has_crime_count = "crime_count" in df.columns

        if not has_crime_count and "Y_label" in df.columns:
            df["crime_count"] = (
                pd.to_numeric(df["Y_label"], errors="coerce")
                .fillna(0)
                .astype(int)
            )
            has_crime_count = True

        if need.issubset(df.columns) and has_crime_count:
            # kategori kolonu √∂ncelik sƒ±rasƒ±
            cat_col = None
            for cc in ["category_grouped", "subcategory_grouped", "category"]:
                if cc in df.columns:
                    cat_col = cc
                    break

            if cat_col:
                tmp = df[["date", "GEOID", cat_col, "crime_count"]].copy()
                tmp["date"] = pd.to_datetime(tmp["date"], errors="coerce").dt.date
                g = (tmp.groupby(["GEOID", cat_col, "date"], as_index=False)["crime_count"].sum())
                g["date"] = pd.to_datetime(g["date"])
                g = g.sort_values(["GEOID", cat_col, "date"])

                def _roll_counts(x):
                    x = x.set_index("date").asfreq("D", fill_value=0)
                    x["nr_7d"]  = x["crime_count"].rolling("7D").sum().shift(1)
                    x["nr_14d"] = x["crime_count"].rolling("14D").sum().shift(1)
                    return x.reset_index()

                g2 = (g.groupby(["GEOID", cat_col]).apply(_roll_counts).reset_index(level=[0, 1]).reset_index(drop=True))
                g2["date"] = g2["date"].dt.date

                df = df.merge(
                    g2[["GEOID", cat_col, "date", "nr_7d", "nr_14d"]],
                    on=["GEOID", cat_col, "date"], how="left"
                )
                for c in ["nr_7d", "nr_14d"]:
                    df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0).astype(float)
    except Exception as _e:
        print(f"near-repeat uyarƒ±: {_e}")

    # Dƒ±≈üsal deƒüi≈üken √∂rnek skoru
    try:
        ext_map_path = Path(DATA_DIR) / "crime_type_externals_map.json"
        if ext_map_path.exists():
            with open(ext_map_path, "r", encoding="utf-8") as f:
                type_map = json.load(f)
            key_col = "category_grouped" if "category_grouped" in df.columns else (
                      "category" if "category" in df.columns else None)
            if key_col:
                def _ext_score(row):
                    cols = type_map.get(str(row[key_col]), [])
                    vals = []
                    for c in cols:
                        if c in df.columns:
                            try:
                                vals.append(float(row.get(c, 0)))
                            except Exception:
                                pass
                    return float(np.nanmean(vals)) if len(vals) > 0 else np.nan
                df["externals_type_score"] = df.apply(_ext_score, axis=1)
                df["externals_type_score"] = df["externals_type_score"].fillna(0.0).astype(float)
    except Exception as _e:
        print(f"dƒ±≈üsal deƒüi≈üken uyarƒ±: {_e}")

    preview_cols = [c for c in ["nr_7d", "nr_14d", "nei_7d_sum", "externals_type_score"] if c in df.columns]
    if preview_cols:
        try:
            st.caption("üß© Yeni mek√¢nsal-zamansal √∂zellikler (ilk 20 satƒ±r):")
            st.dataframe(df[preview_cols].head(20))
        except Exception:
            pass

    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(output_path, index=False)
    print(f"‚úÖ {output_path} kaydedildi. Satƒ±r sayƒ±sƒ±: {len(df)}")
    return df

def load_sf_crime_08(local_path: Path) -> Optional[pd.DataFrame]:
    """√ñnce yerel dosyayƒ± dene; yoksa artifact‚Äôtan √ßek. Ardƒ±ndan crime_mix varsa grid ile merge et."""
    def _normalize_date_cols(df_: pd.DataFrame) -> pd.DataFrame:
        if "date" in df_.columns:
            df_["date"] = pd.to_datetime(df_["date"], errors="coerce").dt.date
        elif "datetime" in df_.columns and "date" not in df_.columns:
            df_["date"] = pd.to_datetime(df_["datetime"], errors="coerce").dt.date
        return df_

    df: Optional[pd.DataFrame] = None
    try:
        if local_path.exists():
            df = pd.read_csv(local_path, low_memory=False)
            df = _normalize_date_cols(df)
    except Exception as e:
        st.warning(f"Yerel sf_crime_08.csv okunamadƒ±: {e}")

    if df is None:
        df = fetch_latest_artifact_df()
        if df is None:
            return None
        df = _normalize_date_cols(df)

    # crime_mix merge (grid) ‚Äî opsiyonel
    try:
        _out_dir  = local_path.parent
        _grid_path = _out_dir / "sf_crime_grid_full_labeled.csv"
        if _grid_path.exists():
            grid = pd.read_csv(_grid_path, dtype={"GEOID": str}, low_memory=False)

            keys = ["GEOID", "season", "day_of_week", "event_hour"]
            if set(keys).issubset(grid.columns) and "crime_mix" in grid.columns and set(keys).issubset(df.columns):

                df["GEOID"]   = _norm_geoid(df["GEOID"])
                grid["GEOID"] = _norm_geoid(grid["GEOID"])

                for c in ["day_of_week", "event_hour"]:
                    df[c]   = pd.to_numeric(df[c], errors="coerce").astype("Int64")
                    grid[c] = pd.to_numeric(grid[c], errors="coerce").astype("Int64")

                merged = df.merge(
                    grid[keys + ["crime_mix"]],
                    on=keys, how="left", suffixes=("", "_grid"), validate="many_to_one"
                )

                if "crime_mix_grid" in merged.columns:
                    if "crime_mix" not in merged.columns:
                        merged["crime_mix"] = ""
                    merged["crime_mix"] = merged["crime_mix"].astype(str)
                    merged["crime_mix"] = merged["crime_mix"].where(
                        merged["crime_mix"].str.len() > 0,
                        merged["crime_mix_grid"].fillna("")
                    )
                    merged = merged.drop(columns=["crime_mix_grid"], errors="ignore")
                df = merged
        else:
            print(f"crime_mix merge atlandƒ±: grid bulunamadƒ± ‚Üí {_grid_path}")
    except Exception as _e:
        print(f"crime_mix merge uyarƒ±sƒ±: {_e}")

    return df

def load_city_crime_08(prefix: str, data_dir: Path) -> Optional[pd.DataFrame]:
    """{prefix}_crime_08.csv'yi y√ºkler ve date kolonunu normalize eder."""
    path = data_dir / f"{prefix}_crime_08.csv"
    if prefix.lower() == "sf":
        return load_sf_crime_08(path)
    if not path.exists():
        return None
    try:
        df = pd.read_csv(path, low_memory=False)
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date
        elif "datetime" in df.columns:
            df["date"] = pd.to_datetime(df["datetime"], errors="coerce").dt.date
        return df
    except Exception as e:
        st.warning(f"{path.name} okunamadƒ±: {e}")
        return None

def process_city_to_09(prefix: str, data_dir: Path) -> Optional[pd.DataFrame]:
    """{prefix}_crime_08 ‚Üí temizle (_08_clean) ‚Üí neighbors ‚Üí {prefix}_crime_09 √ºretir."""
    df08 = load_city_crime_08(prefix, data_dir)
    if df08 is None:
        st.info(f"{prefix}_crime_08.csv bulunamadƒ±.")
        return None

    out_clean = data_dir / f"{prefix}_crime_08_clean.csv"
    clean_and_save_crime_09(df08, str(out_clean))
    st.success(f"‚úÖ {out_clean.name} kaydedildi.")

    graph_script = resolve_script({"name": "update_neighbors_graph", "alts": ["update_neighbors_graph.py", "neighbors_graph", "neighbors_graph.py"]})
    feat_script  = resolve_script({"name": "update_neighbors",       "alts": ["update_neighbors.py"]})

    # prefix'e √∂zel neighbors varsa onu kullan; yoksa genel
    neighbor_file_pref = data_dir / f"{prefix}_neighbors.csv"
    neighbor_file_gen  = data_dir / "neighbors.csv"
    neighbor_file_use  = neighbor_file_pref if neighbor_file_pref.exists() else neighbor_file_gen

    if not neighbor_file_use.exists() and graph_script:
        ok_graph = run_script(graph_script)
        st.success("üó∫Ô∏è neighbors.csv √ºretildi.") if ok_graph else st.warning("neighbors graph ba≈üarƒ±sƒ±z.")
        if ok_graph:
            neighbor_file_use = neighbor_file_gen  # grafikten sonra genel √ºretildi varsayalƒ±m

    if feat_script:
        os.environ["NEIGHBOR_FILE"]        = os.environ.get("NEIGHBOR_FILE", str(neighbor_file_use))
        os.environ["NEIGHBOR_INPUT_CSV"]   = str(out_clean)
        os.environ["NEIGHBOR_OUTPUT_CSV"]  = str(data_dir / f"{prefix}_crime_09.csv")
        os.environ["NEIGHBOR_WINDOW_DAYS"] = os.environ.get("NEIGHBOR_WINDOW_DAYS", "7")
        os.environ["NEIGHBOR_LAG_DAYS"]    = os.environ.get("NEIGHBOR_LAG_DAYS", "1")

        ok_feat = run_script(feat_script)
        if ok_feat:
            st.success(f"üß© {prefix}_crime_09.csv √ºretildi (nei_7d_sum eklendi).")
            try:
                return pd.read_csv(data_dir / f"{prefix}_crime_09.csv", low_memory=False)
            except Exception:
                return None
        else:
            st.warning("update_neighbors.py √ßalƒ±≈ütƒ±rƒ±lamadƒ±; logu kontrol edin.")
    else:
        st.info("update_neighbors.py bulunamadƒ± (scripts klas√∂r√ºne ekleyin).")

    return None

# -----------------------------------------------------------------------------
# Dosya Listeleme / D√∂n√º≈üt√ºrme Yardƒ±mcƒ±larƒ±
# -----------------------------------------------------------------------------
def list_files_sorted(
    include: Optional[List[Union[str, Path]]] = None,
    base_dir: Optional[Path] = None,
    pattern: str = "*.csv",
    ascending: bool = True,
    include_missing: bool = True,
) -> pd.DataFrame:
    bdir = base_dir or DATA_DIR
    rows: List[Dict[str, Any]] = []

    if include is None:
        include = []
        for prefix in ["sf", "fr"]:
            include += [str(bdir / f"{prefix}_crime_{i:02d}.csv") for i in range(1, 10)]
            include += [str(bdir / f"{prefix}_crime_y.csv")]
        include += [str(bdir / "sf_crime_grid_full_labeled.csv")]
        for p in bdir.glob(pattern):
            include.append(str(p))

    seen = set()
    for x in include:
        p = Path(x)
        key = str(p.resolve()) if p.exists() else str(p)
        if key in seen:
            continue
        seen.add(key)

        exists = p.exists()
        try:
            st_ = p.stat() if exists else None
            mtime = st_.st_mtime if st_ else None
            size  = st_.st_size  if st_ else None
        except Exception:
            mtime, size = None, None

        if exists or include_missing:
            rows.append({
                "file": p.name,
                "path": str(p),
                "exists": bool(exists),
                "size": _human_bytes(size),
                "modified": _fmt_dt(mtime),
                "age": _age_str(mtime),
                "_mtime": mtime,
            })

    df = pd.DataFrame(rows)
    if not df.empty:
        df = df.sort_values("_mtime", ascending=ascending, na_position="last").drop(columns=["_mtime"])
    return df

def convert_csv_dir_to_parquet(
    input_dir: Path,
    output_dir: Path,
    pattern: str = "*.csv",
    compression: str = "zstd",
    stats: bool = True
) -> pd.DataFrame:
    output_dir.mkdir(parents=True, exist_ok=True)
    rows = []

    try:
        import polars as pl
        use_polars = True
    except Exception:
        use_polars = False

    if not use_polars:
        try:
            import pyarrow  # noqa: F401
        except Exception:
            raise RuntimeError("Ne polars ne de pyarrow mevcut. L√ºtfen 'pip install polars pyarrow' kurun.")

    for p in sorted(Path(input_dir).glob(pattern)):
        if not p.is_file():
            continue
        out = Path(output_dir) / (p.stem + ".parquet")
        try:
            if use_polars:
                df_pl = pl.read_csv(str(p))
                df_pl.write_parquet(str(out), compression=compression)
                n_rows = df_pl.height
            else:
                df_pd = pd.read_csv(p, low_memory=False)
                df_pd.to_parquet(out, compression=compression, index=False, engine="pyarrow")
                n_rows = len(df_pd)

            src_sz = p.stat().st_size if p.exists() else None
            dst_sz = out.stat().st_size if out.exists() else None
            rows.append({
                "src": str(p.name),
                "dst": str(out.name),
                "rows": n_rows,
                "src_size": src_sz,
                "dst_size": dst_sz,
            })
        except Exception as e:
            rows.append({
                "src": str(p.name),
                "dst": str(out.name),
                "rows": None,
                "src_size": None,
                "dst_size": None,
                "error": str(e),
            })

    res = pd.DataFrame(rows)
    if stats and not res.empty:
        try:
            res["src_size_mb"] = (res["src_size"].astype("float") / (1024**2)).round(3)
            res["dst_size_mb"] = (res["dst_size"].astype("float") / (1024**2)).round(3)
            res["ratio"] = (res["dst_size"].astype("float") / res["src_size"].astype("float")).round(3)
        except Exception:
            pass
    return res

# -----------------------------------------------------------------------------
# ƒ∞ndirilebilir kaynaklar
# -----------------------------------------------------------------------------
DOWNLOADS = {
    "Su√ß Taban CSV (Release latest)": {
        "url": CRIME_CSV_LATEST,
        "path": str(DATA_DIR / "sf_crime_y.csv"),
    },
    "Tahmin Grid Verisi (GEOID √ó Zaman + Y_label)": {
        "url": "https://raw.githubusercontent.com/cem5113/crime_prediction_data/main/sf_crime_grid_full_labeled.csv",
        "path": str(DATA_DIR / "sf_crime_grid_full_labeled.csv"),
        "allow_artifact": True,
        "artifact_picks": ["sf_crime_grid_full_labeled.csv"],
    },
    "911 √áaƒürƒ±larƒ± (√∂zet)": {
        "url": RAW_911_URL,
        "path": str(DATA_DIR / "sf_911_last_5_year_y.csv"),
    },
    "311 √áaƒürƒ±larƒ± (√∂zet)": {
        "url": SF311_URL,
        "path": str(DATA_DIR / "sf_311_last_5_years_y.csv"),
    },
    "Otob√ºs Duraklarƒ±": {
        "url": "https://raw.githubusercontent.com/cem5113/crime_prediction_data/main/sf_bus_stops_with_geoid.csv",
        "path": str(DATA_DIR / "sf_bus_stops_with_geoid.csv"),
    },
    "Tren Duraklarƒ±": {
        "url": "https://raw.githubusercontent.com/cem5113/crime_prediction_data/main/sf_train_stops_with_geoid.csv",
        "path": str(DATA_DIR / "sf_train_stops_with_geoid.csv"),
    },
    "POI GeoJSON": {
        "url": "https://raw.githubusercontent.com/cem5113/crime_prediction_data/main/sf_pois.geojson",
        "path": str(DATA_DIR / "sf_pois.geojson"),
        "is_json": True,
    },
    "N√ºfus Verisi": {
        "url": "",
        "path": str(DATA_DIR / "sf_population.csv"),
        "local_src": str(POPULATION_PATH),
        "is_local_csv": True,
    },
    "POI Risk Skorlarƒ±": {
        "url": "https://raw.githubusercontent.com/cem5113/crime_prediction_data/main/risky_pois_dynamic.json",
        "path": str(DATA_DIR / "risky_pois_dynamic.json"),
        "is_json": True,
    },
    "Polis ƒ∞stasyonlarƒ±": {
        "url": "https://raw.githubusercontent.com/cem5113/crime_prediction_data/main/sf_police_stations.csv",
        "path": str(DATA_DIR / "sf_police_stations.csv"),
    },
    "Devlet Binalarƒ±": {
        "url": "https://raw.githubusercontent.com/cem5113/crime_prediction_data/main/sf_government_buildings.csv",
        "path": str(DATA_DIR / "sf_government_buildings.csv"),
    },
    "Hava Durumu": {
        "url": "https://raw.githubusercontent.com/cem5113/crime_prediction_data/main/sf_weather_5years.csv",
        "path": str(DATA_DIR / "sf_weather_5years.csv"),
    },
}

def download_and_preview(name, url, file_path, is_json=False, allow_artifact_fallback=False, artifact_picks=None):
    st.markdown(f"### üîπ {name}")
    st.caption(f"URL: {_mask_token(url)}")
    ok = False

    # Yerel kopya (N√ºfus) ‚Äî CSV-only mod
    meta = DOWNLOADS.get(name, {})
    if meta.get("is_local_csv"):
        try:
            src = Path(meta["local_src"])
            dst = Path(file_path)
            dst.parent.mkdir(parents=True, exist_ok=True)
            if src.exists():
                dst.write_bytes(src.read_bytes())
                ok = True
                st.info("Yerel CSV kopyalandƒ±.")
            else:
                st.warning(f"Yerel dosya bulunamadƒ±: {src}")
        except Exception as e:
            st.warning(f"Yerel kopya hatasƒ±: {e}")

    if not ok and url:
        try:
            r = requests.get(url, timeout=60)
            r.raise_for_status()
            Path(file_path).parent.mkdir(parents=True, exist_ok=True)
            if is_json:
                Path(file_path).write_text(r.text, encoding="utf-8")
            else:
                with open(file_path, "wb") as f:
                    f.write(r.content)
            ok = True
        except Exception as e:
            st.warning(f"Raw indirme ba≈üarƒ±sƒ±z: {e}")

    if not ok and allow_artifact_fallback:
        try:
            blob = fetch_file_from_latest_artifact(artifact_picks or [os.path.basename(file_path)])
            if blob:
                Path(file_path).parent.mkdir(parents=True, exist_ok=True)
                with open(file_path, "wb") as f:
                    f.write(blob)
                ok = True
                st.info("Dosya artifact'tan alƒ±ndƒ±.")
        except Exception as e:
            st.warning(f"Artifact fallback ba≈üarƒ±sƒ±z: {e}")

    if not ok:
        st.error(f"‚ùå {name} indirilemedi.")
        return

    # √ñnizleme
    try:
        if is_json:
            try:
                data = json.loads(Path(file_path).read_text(encoding="utf-8"))
                if isinstance(data, dict):
                    st.json(data)
                elif isinstance(data, list):
                    st.json(data[:3])
                else:
                    st.code(str(data)[:1000])
            except Exception:
                st.code(Path(file_path).read_text(encoding="utf-8")[:2000])
        else:
            head = pd.read_csv(file_path, nrows=3)
            cols = pd.read_csv(file_path, nrows=0).columns.tolist()
            st.dataframe(head)
            st.caption(f"üìå S√ºtunlar: {cols}")
        st.success("‚úÖ ƒ∞ndirildi.")
    except Exception as e:
        st.info("√ñnizleme ba≈üarƒ±sƒ±z; dosya indirildi.")
        st.code(f"√ñnizleme hatasƒ±: {e}")

# -----------------------------------------------------------------------------
# UI ‚Äî Ba≈ülƒ±k ve Sidebar
# -----------------------------------------------------------------------------
st.title("üì¶ G√ºnl√ºk Su√ß Tahmin Zenginle≈ütirme ve G√ºncelleme Paneli")

# --- SIDEBAR ---
with st.sidebar:
    st.markdown("### GitHub Actions")

    # Sidebar i√ßinde expander a√ßacaksan 'st.sidebar.expander' kullan
    with st.sidebar.expander("ACS Ayarlarƒ± (Demografi)"):
        acs_year_default = os.environ.get("ACS_YEAR", "LATEST")
        whitelist_default = os.environ.get("DEMOG_WHITELIST", "")
        level_default = os.environ.get("CENSUS_GEO_LEVEL", "auto")

        acs_year_in = st.text_input(
            label="ACS_YEAR (LATEST veya YYYY)",
            value=str(acs_year_default or "LATEST"),
            key="acs_year_in",
            help="5-year ACS i√ßin en son yƒ±lƒ± kullanmak genelde uygundur."
        )

        whitelist_in = st.text_input(
            label="DEMOG_WHITELIST (virg√ºll√º; bo≈ü = hepsi)",
            value=str(whitelist_default or ""),
            key="demog_whitelist_in",
            help='√ñrn: "population,median_income,education". Metin e≈üle≈ümesiyle filtreler.'
        )

        levels = ["auto", "tract", "blockgroup", "block"]
        try:
            idx = levels.index(level_default) if level_default in levels else 0
        except Exception:
            idx = 0
        level_in = st.selectbox(
            "CENSUS_GEO_LEVEL",
            levels,
            index=idx,
            key="census_geo_level_in",
            help="N√ºfus GEOID e≈üle≈üme seviyesi. `auto` √ßoƒüu durumda yeterlidir."
        )
        os.environ["CENSUS_GEO_LEVEL"] = level_in

        pop_default = os.environ.get("POPULATION_PATH", str(POPULATION_PATH))
        pop_url_in = st.text_input(
            label="POPULATION_PATH (YEREL CSV YOLU)",
            value=str(pop_default or ""),
            key="population_path_in",
            help="√ñrn: crime_prediction_data/sf_population.csv (URL kabul edilmez)."
        )

        _v = str(acs_year_in).strip()
        if _v.upper() == "LATEST":
            os.environ["ACS_YEAR"] = "LATEST"
        else:
            _digits = re.sub(r"\D", "", _v)
            os.environ["ACS_YEAR"] = _digits if len(_digits) == 4 else "LATEST"

        os.environ["DEMOG_WHITELIST"] = str(whitelist_in or "")

        if re.match(r"^https?://", str(pop_url_in), flags=re.I):
            st.error("CSV-only mod: URL kabul edilmez. Yerel bir CSV yolu girin.")
        else:
            os.environ["POPULATION_PATH"] = pop_url_in or str(POPULATION_PATH)

# --- ANA SAYFA (sidebar DI≈ûI) ---
with st.container():
    exp = st.expander("üïµÔ∏è Workflow tanƒ± (dispatch)", expanded=False)
    with exp:
        # (BURASI TEK EXPANDER ‚Äî i√ß i√ße expander YOK!)
        wf_in = st.text_input(
            "Workflow (dosya adƒ± veya g√∂r√ºnen ad)",
            os.environ.get("GITHUB_WORKFLOW", "full_pipeline.yml")
        )
        ref_in = st.text_input("Ref (branch/tag)", "main")

        if st.button("Tanƒ±yƒ± √áalƒ±≈ütƒ±r"):
            diag_workflow(target=wf_in, ref=ref_in)

        VARIANTS = ["default", "fr"]
        variant = st.selectbox(
            "Pipeline varyantƒ±",
            VARIANTS, index=0,
            help="default: update_*.py ‚Ä¢ fr: update_*.fr.py / update_*_fr.py"
        )
        os.environ["PIPELINE_VARIANT"] = variant

        persist = st.selectbox(
            "√áƒ±ktƒ±yƒ± saklama modu",
            ["artifact", "commit", "none"], index=0,
            help="artifact: repo‚Äôyu bozmadan sakla ‚Ä¢ commit: repo‚Äôya yaz ‚Ä¢ none: sadece log"
        )

        force_bypass = st.checkbox(
            "07:00 kapƒ±sƒ±nƒ± yok say (force)",
            value=True,
            help="ƒ∞≈üaretli ise saat filtresi devre dƒ±≈üƒ± kalƒ±r ve pipeline her saatte √ßalƒ±≈üƒ±r."
        )

        status_box = st.empty()
        _render_last_run_status(status_box)

        col_run, col_refresh = st.columns(2)
        with col_run:
            if st.button("üöÄ Full pipeline‚Äôƒ± Actions‚Äôta √ßalƒ±≈ütƒ±r"):
                if not (st.secrets.get("GH_TOKEN") or os.environ.get("GH_TOKEN")):
                    st.error("GH_TOKEN tanƒ±mlƒ± deƒüil (Streamlit secrets veya env).")
                else:
                    try:
                        r = dispatch_workflow(persist=persist, force=force_bypass)
                        if r["ok"]:
                            st.success(f"Workflow tetiklendi (persist={persist}, force={force_bypass}). Runs‚Äôƒ± kontrol et.")
                        else:
                            st.error(f"Tetikleme ba≈üarƒ±sƒ±z: {r['status']} {r['text']}")
                    except Exception as e:
                        st.error(f"Hata: {e}")

        with col_refresh:
            if st.button("üì° Son durumu yenile"):
                _render_last_run_status(status_box)

    with st.sidebar.expander("ACS Ayarlarƒ± (Demografi)"):
        acs_year_default = os.environ.get("ACS_YEAR", "LATEST")
        whitelist_default = os.environ.get("DEMOG_WHITELIST", "")
        level_default = os.environ.get("CENSUS_GEO_LEVEL", "auto")

        acs_year_in = st.text_input(
            label="ACS_YEAR (LATEST veya YYYY)",
            value=str(acs_year_default or "LATEST"),
            key="acs_year_in",
            help="5-year ACS i√ßin en son yƒ±lƒ± kullanmak genelde uygundur."
        )

        whitelist_in = st.text_input(
            label="DEMOG_WHITELIST (virg√ºll√º; bo≈ü = hepsi)",
            value=str(whitelist_default or ""),
            key="demog_whitelist_in",
            help='√ñrn: "population,median_income,education". Metin e≈üle≈ümesiyle filtreler.'
        )

        levels = ["auto", "tract", "blockgroup", "block"]
        try:
            idx = levels.index(level_default) if level_default in levels else 0
        except Exception:
            idx = 0
        level_in = st.selectbox(
            "CENSUS_GEO_LEVEL",
            levels,
            index=idx,
            key="census_geo_level_in",
            help="N√ºfus GEOID e≈üle≈üme seviyesi. `auto` √ßoƒüu durumda yeterlidir."
        )
        os.environ["CENSUS_GEO_LEVEL"] = level_in

        pop_default = os.environ.get("POPULATION_PATH", str(POPULATION_PATH))
        pop_url_in = st.text_input(
            label="POPULATION_PATH (YEREL CSV YOLU)",
            value=str(pop_default or ""),
            key="population_path_in",
            help="√ñrn: crime_prediction_data/sf_population.csv (URL kabul edilmez)."
        )

        _v = str(acs_year_in).strip()
        if _v.upper() == "LATEST":
            os.environ["ACS_YEAR"] = "LATEST"
        else:
            _digits = re.sub(r"\D", "", _v)
            os.environ["ACS_YEAR"] = _digits if len(_digits) == 4 else "LATEST"

        os.environ["DEMOG_WHITELIST"] = str(whitelist_in or "")

        if re.match(r"^https?://", str(pop_url_in), flags=re.I):
            st.error("CSV-only mod: URL kabul edilmez. Yerel bir CSV yolu girin.")
        else:
            os.environ["POPULATION_PATH"] = pop_url_in or str(POPULATION_PATH)

# -----------------------------------------------------------------------------
# 0) (Opsiyonel) requirements y√ºkleme
# -----------------------------------------------------------------------------
st.markdown("### 0) (Opsiyonel) Gereklilikleri y√ºkle")
if st.button("üì¶ requirements.txt y√ºkle"):
    try:
        req = ROOT / "requirements.txt"
        if req.exists():
            out = subprocess.run(
                [sys.executable, "-m", "pip", "install", "-r", str(req)],
                cwd=str(ROOT),
                capture_output=True,
                text=True,
            )
            st.code(out.stdout or "")
            if out.returncode == 0:
                st.success("‚úÖ Gereklilikler y√ºklendi.")
            else:
                st.error("‚ùå Kurulumda hata!")
                st.code(out.stderr or "")
        else:
            st.warning("‚ö†Ô∏è requirements.txt bulunamadƒ±.")
    except Exception as e:
        st.error(f"Kurulum √ßaƒürƒ±sƒ± ba≈üarƒ±sƒ±z: {e}")

# -----------------------------------------------------------------------------
# 1) (Opsiyonel) Verileri indir ve √∂nizle
# -----------------------------------------------------------------------------
st.markdown("### 1) (Opsiyonel) Verileri indir ve √∂nizle")
if st.button("üì• Verileri ƒ∞ndir ve √ñnizle (ƒ∞lk 3 Satƒ±r)"):
    for name, info in DOWNLOADS.items():
        download_and_preview(
            name,
            info.get("url", ""),
            info["path"],
            is_json=info.get("is_json", False),
            allow_artifact_fallback=info.get("allow_artifact", False),
            artifact_picks=info.get("artifact_picks"),
        )
    st.success("‚úÖ ƒ∞ndirme tamamlandƒ±.")

# -----------------------------------------------------------------------------
# 1.5) Dosyalarƒ± tarihe g√∂re sƒ±rala
# -----------------------------------------------------------------------------
st.markdown("### 1.5) Dosyalarƒ± tarihe g√∂re sƒ±rala")
colA, colB, colC = st.columns([1, 1, 2])
with colA:
    order = st.radio("Sƒ±ralama", ["Eski ‚Üí Yeni", "Yeni ‚Üí Eski"], horizontal=True, index=0)
with colB:
    show_missing = st.checkbox("Eksikleri de g√∂ster", value=True)
with colC:
    patt = st.text_input(
        "Desen (glob)", "*.csv",
        help="√ñrn: sf_crime_*.csv",
        key="glob_list_files"
    )

asc = (order == "Eski ‚Üí Yeni")
df_files = list_files_sorted(pattern=patt, ascending=asc, include_missing=show_missing)
if df_files.empty:
    st.info("E≈üle≈üen dosya yok.")
else:
    st.dataframe(df_files, use_container_width=True)

# -----------------------------------------------------------------------------
# 1.6) CSV ‚Üí Parquet d√∂n√º≈üt√ºr
# -----------------------------------------------------------------------------
st.markdown("### 1.6) CSV ‚Üí Parquet d√∂n√º≈üt√ºr")
with st.expander("üîÑ CSV‚Äôleri Parquet‚Äôe √ßevir (zstd)"):
    in_dir = st.text_input(
        "Girdi klas√∂r√º", value=str(DATA_DIR),
        help="√ñrn: crime_prediction_data/",
        key="csv2parquet_in_dir"
    )
    out_dir = st.text_input(
        "√áƒ±ktƒ± klas√∂r√º", value=str(ROOT / "parquet_out"),
        help="√ñrn: parquet_out/",
        key="csv2parquet_out_dir"
    )
    patt_in = st.text_input(
        "Desen (glob)", "*.csv",
        help="√ñrn: sf_crime_*.csv",
        key="csv2parquet_glob"
    )
    comp = st.selectbox(
        "Sƒ±kƒ±≈ütƒ±rma",
        ["zstd", "snappy", "gzip", "brotli", "uncompressed"],
        index=0,
        key="csv2parquet_codec"
    )
    want_stats = st.checkbox(
        "√ñzet/stats √ºret", value=True,
        key="csv2parquet_stats"
    )

    if st.button("üß∞ D√∂n√º≈üt√ºr (CSV ‚Üí Parquet)", key="csv2parquet_run"):
        try:
            res = convert_csv_dir_to_parquet(
                input_dir=Path(in_dir),
                output_dir=Path(out_dir),
                pattern=patt_in,
                compression=comp,
                stats=want_stats
            )
            if res.empty:
                st.info("E≈üle≈üen CSV bulunamadƒ±.")
            else:
                st.success("D√∂n√º≈ü√ºm tamamlandƒ±.")
                st.dataframe(res)
        except Exception as e:
            st.error(f"D√∂n√º≈ü√ºm hatasƒ±: {e}")

# -----------------------------------------------------------------------------
# Tanƒ± ve Cache
# -----------------------------------------------------------------------------
with st.expander("üîé Tanƒ±: Etkin URL/ENV deƒüerleri"):
    st.write("CRIME_CSV_URL (env):", os.environ.get("CRIME_CSV_URL"))
    st.write("RAW_911_URL (env):", os.environ.get("RAW_911_URL"))
    st.write("SF311_URL (env):", os.environ.get("SF311_URL"))

if st.button("‚ôªÔ∏è Streamlit cache temizle"):
    try:
        st.cache_data.clear()
        st.success("Cache temizlendi.")
    except Exception as e:
        st.warning(f"Cache temizlenemedi: {e}")

# -----------------------------------------------------------------------------
# 2) G√ºncelleme ve Zenginle≈ütirme (01 ‚Üí 09)
# -----------------------------------------------------------------------------
st.markdown("### 2) G√ºncelleme ve Zenginle≈ütirme (01 ‚Üí 09)")
if st.button("‚öôÔ∏è G√ºncelleme ve Zenginle≈ütirme (01 ‚Üí 09)"):
    with st.spinner("‚è≥ Scriptler √ßalƒ±≈ütƒ±rƒ±lƒ±yor..."):
        all_ok = True
        for entry in PIPELINE:
            sp = resolve_script(entry, locale=os.environ.get("PIPELINE_VARIANT", "default"))
            if not sp:
                st.warning(f"‚è≠Ô∏è {entry['name']} bulunamadƒ±/indirilemedi, atlanƒ±yor.")
                all_ok = False
                continue
            ok = run_script(sp)
            all_ok = all_ok and ok
    if all_ok:
        st.success("üéâ Pipeline bitti: T√ºm adƒ±mlar ba≈üarƒ±yla tamamlandƒ±.")
    else:
        st.warning("‚ÑπÔ∏è Pipeline tamamlandƒ±; eksik/hatalƒ± adƒ±mlar var. Loglarƒ± kontrol edin.")

# -----------------------------------------------------------------------------
# 3) G√ºncel _08 ‚Üí _09 √ºret (sf + fr)
# -----------------------------------------------------------------------------
st.markdown("### 3) G√ºncel _08 ‚Üí _09 √ºret (sf + fr)")
if st.button("üß™ _08'i temizle ve _09 √ºret (sf & fr)"):
    for prefix in ["sf", "fr"]:
        st.subheader(f"üîπ {prefix.upper()} akƒ±≈üƒ±")
        _ = process_city_to_09(prefix, DATA_DIR)

# √ñnizleme: 08 ve 09 dosyalarƒ± (varsa)
for prefix in ["sf", "fr"]:
    st.subheader(f"üìÑ √ñnizleme ‚Äî {prefix.upper()}")
    p08 = DATA_DIR / f"{prefix}_crime_08.csv"
    p09 = DATA_DIR / f"{prefix}_crime_09.csv"
    if p08.exists():
        try:
            st.markdown(f"**{p08.name} ‚Äî ilk 20 satƒ±r**")
            st.dataframe(pd.read_csv(p08, nrows=20, low_memory=False), use_container_width=True)
        except Exception as e:
            st.info(f"{p08.name} √∂nizlenemedi: {e}")
    if p09.exists():
        try:
            st.markdown(f"**{p09.name} ‚Äî ilk 20 satƒ±r**")
            st.dataframe(pd.read_csv(p09, nrows=20, low_memory=False), use_container_width=True)
        except Exception as e:
            st.info(f"{p09.name} √∂nizlenemedi: {e}")
