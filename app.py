from __future__ import annotations
from typing import Optional, Union, Dict, List, Tuple, Any

import streamlit as st
import pandas as pd
import requests
import re
import os, json, subprocess, sys
from pathlib import Path
import io, zipfile
from datetime import datetime, timezone

import matplotlib.pyplot as plt
import numpy as np  # â–¶ï¸ eklendi: aÅŸaÄŸÄ±da np.nanmean vb. kullanÄ±lÄ±yor

# --- Forensic rapor yardÄ±mcÄ± (varsa import et, yoksa stub kullan) ---
try:
    from scripts.forensic_report import build_forensic_report
except Exception:
    def build_forensic_report(**kwargs):
        return None

# -----------------------------------------------------------------------------
# Global Kurulum
# -----------------------------------------------------------------------------
st.set_page_config(page_title="Veri GÃ¼ncelleme", layout="wide")

try:
    ROOT = Path(__file__).resolve().parent
except NameError:
    ROOT = Path.cwd()

DATA_DIR = ROOT / "crime_prediction_data"
SCRIPTS_DIR = ROOT / "scripts"
DATA_DIR.mkdir(parents=True, exist_ok=True)
SCRIPTS_DIR.mkdir(parents=True, exist_ok=True)
SEARCH_DIRS = [SCRIPTS_DIR, ROOT]

PIPELINE = [
    {"name": "update_crime",      "alts": ["build_crime_grid", "crime_grid_build"]},
    {"name": "update_911",        "alts": ["enrich_911"]},
    {"name": "update_311",        "alts": ["enrich_311"]},
    {"name": "update_population", "alts": ["enrich_population"]},
    {"name": "update_bus",        "alts": ["enrich_bus"]},
    {"name": "update_train",      "alts": ["enrich_train"]},
    {"name": "update_poi",        "alts": ["pipeline_make_sf_crime_06", "app_poi_to_06", "enrich_poi"]},
    {"name": "update_police_gov", "alts": ["enrich_police_gov_06_to_07", "enrich_police_gov", "enrich_police"]},
    {"name": "update_weather",    "alts": ["enrich_weather"]},
]

# -----------------------------------------------------------------------------
# AÄIR BAÄIMLILIKLAR Ä°Ã‡Ä°N LAZY IMPORT
# -----------------------------------------------------------------------------
def _load_ml_deps():
    try:
        import shap
        from sklearn.model_selection import TimeSeriesSplit
        from sklearn.metrics import roc_auc_score, brier_score_loss, mean_absolute_error
        from sklearn.calibration import CalibratedClassifierCV
        from sklearn.inspection import PartialDependenceDisplay
        from lightgbm import LGBMClassifier, LGBMRegressor
        from lime.lime_tabular import LimeTabularExplainer
        return {
            "np": np,
            "shap": shap,
            "TimeSeriesSplit": TimeSeriesSplit,
            "roc_auc_score": roc_auc_score,
            "brier_score_loss": brier_score_loss,
            "mean_absolute_error": mean_absolute_error,
            "CalibratedClassifierCV": CalibratedClassifierCV,
            "PartialDependenceDisplay": PartialDependenceDisplay,
            "LGBMClassifier": LGBMClassifier,
            "LGBMRegressor": LGBMRegressor,
            "LimeTabularExplainer": LimeTabularExplainer,
        }
    except ModuleNotFoundError as e:
        missing = getattr(e, "name", "bir paket")
        st.error(
            f"ğŸ§± Gerekli paket eksik: **{missing}**. "
            "LÃ¼tfen sol menÃ¼den **'0) Gereklilikleri yÃ¼kle'** dÃ¼ÄŸmesini kullanÄ±n "
            "ve kurulumdan sonra **Rerun** yapÄ±n."
        )
        st.stop()

# -----------------------------------------------------------------------------
# YardÄ±mcÄ±lar
# -----------------------------------------------------------------------------
def pick_url(key: str, default: str) -> str:
    # Ã–ncelik: 1) st.secrets  2) ENV  3) default
    try:
        if key in st.secrets and st.secrets[key]:
            return str(st.secrets[key])
    except Exception:
        pass
    return os.getenv(key, default)

def _mask_token(u: str) -> str:
    try:
        return re.sub(r'(\$\$app_token=)[^&]+', r'\1â€¢â€¢â€¢', str(u))
    except Exception:
        return str(u)

def _human_bytes(n: int) -> str:
    if n is None:
        return "-"
    step = 1024.0
    for u in ["B", "KB", "MB", "GB", "TB"]:
        if n < step:
            return f"{n:.0f} {u}" if u == "B" else f"{n:.1f} {u}"
        n /= step
    return f"{n:.1f} PB"

def _fmt_dt(ts: Optional[float]) -> str:
    if ts is None:
        return "-"
    dt = datetime.fromtimestamp(ts, tz=timezone.utc).astimezone()
    return dt.strftime("%Y-%m-%d %H:%M:%S")

def _age_str(ts: Optional[float]) -> str:
    if ts is None:
        return "-"
    delta = datetime.now().timestamp() - ts
    if delta < 60:
        return f"{int(delta)} sn"
    if delta < 3600:
        return f"{int(delta // 60)} dk"
    if delta < 86400:
        return f"{int(delta // 3600)} sa"
    return f"{int(delta // 86400)} g"

def ensure_script(filename: str) -> Optional[Path]:
    """
    Verilen dosya adÄ±nÄ± SEARCH_DIRS iÃ§inde arar; bulursa Path dÃ¶ner, yoksa None.
    """
    cand = filename if filename.endswith(".py") else f"{filename}.py"
    for base in SEARCH_DIRS:
        p = Path(base) / cand
        if p.exists() and p.is_file():
            return p
    return None

def _candidate_names(base: str, locale: str) -> List[str]:
    if locale and locale != "default":
        return [
            f"{base}.fr.py",
            f"{base}_fr.py",
            f"{base}.{locale}.py",
            f"{base}-{locale}.py",
            f"{base}.py",
        ]
    return [f"{base}.py"]

def resolve_script(entry: dict, locale: str = "default") -> Optional[Path]:
    # 1) asÄ±l ad iÃ§in
    for cand in _candidate_names(entry["name"], locale):
        p = ensure_script(cand)
        if p:
            return p
    # 2) alternatif adlar iÃ§in
    for alt in entry.get("alts", []):
        for cand in _candidate_names(alt, locale):
            pp = ensure_script(cand)
            if pp:
                return pp
    return None

def run_script(path: Path) -> bool:
    st.write(f"â–¶ï¸ {path.name} Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yorâ€¦")
    placeholder = st.empty()
    lines: List[str] = []
    try:
        proc = subprocess.Popen(
            [sys.executable, "-u", str(path)],
            cwd=str(ROOT),
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            env={**os.environ, "PYTHONUNBUFFERED": "1"},
        )
        while True:
            line = proc.stdout.readline()
            if not line and proc.poll() is not None:
                break
            if line:
                lines.append(line.rstrip())
                lines = lines[-400:]
                placeholder.code("\n".join(lines))
        rc = proc.wait()
        if rc == 0:
            st.success(f"âœ… {path.name} tamamlandÄ±")
            return True
        else:
            st.error(f"âŒ {path.name} hata verdi (exit={rc})")
            return False
    except Exception as e:
        st.error(f"ğŸš¨ {path.name} Ã§aÄŸrÄ±lamadÄ±: {e}")
        return False

# -----------------------------------------------------------------------------
# ENV ve URLâ€™ler
# -----------------------------------------------------------------------------
CRIME_CSV_LATEST = pick_url(
    "CRIME_CSV_URL",
    "https://github.com/cem5113/crime_prediction_data/releases/latest/download/sf_crime_y.csv",
)

RAW_911_URL = pick_url(
    "RAW_911_URL",
    "https://github.com/cem5113/crime_prediction_data/releases/download/v1.0.1/sf_911_last_5_year_y.csv",
)

SF311_URL = pick_url(
    "SF311_URL",
    "https://github.com/cem5113/crime_prediction_data/releases/download/v1.0.2/sf_311_last_5_years_y.csv",
)

# CSV-ONLY: NÃ¼fus verisi yerel dosyadan okunacak
DEFAULT_POP_CSV = str((Path(os.environ.get("CRIME_DATA_DIR", "crime_prediction_data")) / "sf_population.csv").resolve())
POPULATION_PATH = pick_url("POPULATION_PATH", DEFAULT_POP_CSV)

# GÃ¼venlik: URL verilirse reddet (CSV-only mod)
if re.match(r"^https?://", str(POPULATION_PATH), flags=re.I):
    POPULATION_PATH = DEFAULT_POP_CSV

os.environ["POPULATION_PATH"] = str(POPULATION_PATH)

# â¬‡ï¸ 911 artÄ±mlÄ± Ã§ekim iÃ§in API ayarlarÄ±
SF911_API_URL       = pick_url("SF911_API_URL", "https://data.sfgov.org/resource/2zdj-bwza.json")
SF911_AGENCY_FILTER = pick_url("SF911_AGENCY_FILTER", "agency like '%Police%'")
SF911_API_TOKEN     = pick_url("SF911_API_TOKEN", "")

# Ã‡ocuk sÃ¼reÃ§lerin de aynÄ± deÄŸerleri gÃ¶rmesi iÃ§in ENV
os.environ["CRIME_CSV_URL"] = CRIME_CSV_LATEST
os.environ["RAW_911_URL"]   = RAW_911_URL
os.environ["SF311_URL"]     = SF311_URL
os.environ["GEOID_LEN"]     = os.environ.get("GEOID_LEN", "11")

GEOID_LEN = int(os.environ.get("GEOID_LEN", "11"))

def _norm_geoid(s: pd.Series, L: int = GEOID_LEN) -> pd.Series:
    return (
        s.astype(str)
         .str.extract(r"(\d+)", expand=False)
         .str[:L]
         .str.zfill(L)
    )

os.environ["SF911_API_URL"]       = SF911_API_URL
os.environ["SF911_AGENCY_FILTER"] = SF911_AGENCY_FILTER
if SF911_API_TOKEN:
    os.environ["SF911_API_TOKEN"] = SF911_API_TOKEN

SOCS_APP_TOKEN = st.secrets.get("SOCS_APP_TOKEN", os.environ.get("SOCS_APP_TOKEN", ""))
if SOCS_APP_TOKEN:
    os.environ["SOCS_APP_TOKEN"] = SOCS_APP_TOKEN

# LATEST veya 2022/2023 gibi belirli yÄ±l
os.environ["ACS_YEAR"] = st.secrets.get("ACS_YEAR", os.environ.get("ACS_YEAR", "LATEST"))

# VirgÃ¼lle filtre (boÅŸ bÄ±rak = tÃ¼m kategoriler)
os.environ["DEMOG_WHITELIST"] = st.secrets.get(
    "DEMOG_WHITELIST",
    os.environ.get("DEMOG_WHITELIST", "")
)

# --- GitHub Actions entegrasyonu ---
GITHUB_REPO = os.environ.get("GITHUB_REPO", "cem5113/crime_prediction_data")
GITHUB_WORKFLOW = os.environ.get("GITHUB_WORKFLOW", "full_pipeline.yml")

ARTIFACT_NAMES = [
    "crime-pipeline-output",       # âœ… yeni
    "sf-crime-pipeline-output",    # (geri uyum)
]

def _gh_headers():
    token = st.secrets.get("GH_TOKEN") or os.environ.get("GH_TOKEN")
    if not token:
        raise RuntimeError("GH_TOKEN gerekli (Streamlit secrets veya env).")
    return {"Authorization": f"Bearer {token}", "Accept": "application/vnd.github+json"}

def fetch_file_from_latest_artifact(pick_names: List[str], artifact_names: Optional[List[str]] = None) -> bytes | None:
    names_to_try = artifact_names or ARTIFACT_NAMES
    runs_url = f"https://api.github.com/repos/{GITHUB_REPO}/actions/runs?per_page=20"
    runs = requests.get(runs_url, headers=_gh_headers(), timeout=30).json()
    run_ids = [r["id"] for r in runs.get("workflow_runs", []) if r.get("conclusion") == "success"]

    for rid in run_ids:
        arts_url = f"https://api.github.com/repos/{GITHUB_REPO}/actions/runs/{rid}/artifacts"
        arts = requests.get(arts_url, headers=_gh_headers(), timeout=30).json().get("artifacts", [])
        for a in arts:
            if a.get("expired", False):
                continue
            if a.get("name") not in names_to_try:
                continue
            dl = requests.get(a["archive_download_url"], headers=_gh_headers(), timeout=60)
            zf = zipfile.ZipFile(io.BytesIO(dl.content))
            names = zf.namelist()

            # 1) DoÄŸrudan dosya aramasÄ±
            for pick in pick_names:
                for cand in (pick, f"crime_prediction_data/{pick}"):
                    if cand in names:
                        return zf.read(cand)

            # 2) "paquet_run_*.zip" iÃ§inden ara (artifact ZIP'i iÃ§inde ikinci bir zip olabilir)
            paquet_inner = [n for n in names if re.search(r"^paquet_run_\d+\.zip$", n)]
            for pin in paquet_inner:
                with zf.open(pin) as inner_blob:
                    with zipfile.ZipFile(io.BytesIO(inner_blob.read())) as inner_zip:
                        inner_names = inner_zip.namelist()
                        for pick in pick_names:
                            # hem dÃ¼z ad hem de fr_eda/ altÄ±ndan eÅŸleÅŸme
                            exacts = [pick, f"fr_eda/{pick}", f"crime_prediction_data/{pick}"]
                            for ex in exacts:
                                if ex in inner_names:
                                    return inner_zip.read(ex)
                        # adÄ±n sonu eÅŸleÅŸsin (fallback)
                        for n in inner_names:
                            if any(n.endswith(p) for p in pick_names):
                                return inner_zip.read(n)

            # 3) Son-Ã§are: isim sonu eÅŸleÅŸmesi
            for n in names:
                if any(n.endswith(p) for p in pick_names):
                    return zf.read(n)
    return None

def _resolve_workflow_id(target: str):
    import requests, os
    url = f"https://api.github.com/repos/{GITHUB_REPO}/actions/workflows?per_page=100"
    r = requests.get(url, headers=_gh_headers(), timeout=30); r.raise_for_status()
    ws = r.json().get("workflows", [])
    # 1) Dosya adÄ±yla eÅŸleÅŸme
    for w in ws:
        if os.path.basename(str(w.get("path",""))) == target:
            return w.get("id")
    # 2) GÃ¶rÃ¼nen adla eÅŸleÅŸme (yedek)
    for w in ws:
        if str(w.get("name","")).strip().lower() == target.strip().lower():
            return w.get("id")
    return None

def dispatch_workflow(persist="artifact", force=True, ref="main", variant=None, top_k=None):
    import json as _json, requests
    target = os.environ.get("GITHUB_WORKFLOW", "full_pipeline.yml")
    wid = _resolve_workflow_id(target)
    if not wid:
        return {"ok": False, "status": 404, "text": f"Workflow bulunamadÄ±: {target}"}
    url = f"https://api.github.com/repos/{GITHUB_REPO}/actions/workflows/{wid}/dispatches"
    inputs = {"persist": persist, "force": "true" if force else "false"}
    if variant: inputs["variant"] = str(variant)
    if top_k:   inputs["top_k"]   = str(top_k)
    payload = {"ref": ref, "inputs": inputs}
    r = requests.post(url, headers=_gh_headers(), data=_json.dumps(payload), timeout=30)
    return {"ok": r.status_code in (204, 201), "status": r.status_code, "text": r.text}

def diag_workflow(target: str | None = None, ref: str | None = None):
    """Workflow dispatch 422 tanÄ±sÄ±: dosya adÄ±/ad, branch ve YAML iÃ§eriÄŸini kontrol eder (PyYAML yoksa YAML analizi atlanÄ±r)."""
    import os, base64, urllib.parse
    import requests
    try:
        import yaml  # PyYAML yoksa except'e dÃ¼ÅŸer ve YAML analizi atlanÄ±r
    except Exception:
        yaml = None

    target = target or os.environ.get("GITHUB_WORKFLOW", "full_pipeline.yml")
    ref = ref or "main"

    st.write(f"ğŸ¯ Hedef workflow: `{target}` Â· ref: `{ref}`")

    # 1) Workflow listesini Ã§ek
    url_list = f"https://api.github.com/repos/{GITHUB_REPO}/actions/workflows?per_page=100"
    r = requests.get(url_list, headers=_gh_headers(), timeout=30)
    if r.status_code != 200:
        st.error(f"Workflow listesi okunamadÄ±: {r.status_code} {r.text}")
        return
    ws = r.json().get("workflows", [])
    st.caption(f"Toplam {len(ws)} workflow bulundu.") if ws else st.warning("Repoâ€™da hiÃ§ workflow gÃ¶rÃ¼nmÃ¼yor.")
    if ws:
        st.code("\n".join([f"- {w.get('name')}  | id={w.get('id')}  | path={w.get('path')}" for w in ws]) or "-", language="text")

    # 2) ID Ã§Ã¶z: Ã¶nce dosya adÄ±yla, olmazsa gÃ¶rÃ¼nen adla
    wid, wmeta = None, None
    for w in ws:
        if os.path.basename(str(w.get("path",""))) == target:
            wid, wmeta = w.get("id"), w
            break
    if not wid:
        for w in ws:
            if str(w.get("name","")).strip().lower() == target.strip().lower():
                wid, wmeta = w.get("id"), w
                break
    if not wid:
        st.error(f"âŒ Hedef workflow bulunamadÄ±: `{target}` (dosya adÄ± ya da gÃ¶rÃ¼nen ad eÅŸleÅŸmedi)")
        return
    st.success(f"âœ… Ã‡Ã¶zÃ¼len workflow id: {wid}")

    # 3) YAML iÃ§eriÄŸini getir ve gÃ¶ster
    url_w = f"https://api.github.com/repos/{GITHUB_REPO}/actions/workflows/{wid}"
    rw = requests.get(url_w, headers=_gh_headers(), timeout=30)
    if rw.status_code != 200:
        st.warning(f"Workflow ayrÄ±ntÄ±sÄ± okunamadÄ±: {rw.status_code} {rw.text}")
        return
    wpath = rw.json().get("path", "")
    st.write(f"ğŸ—‚ï¸ Dosya yolu: `{wpath}`")

    url_contents = f"https://api.github.com/repos/{GITHUB_REPO}/contents/{urllib.parse.quote(wpath)}?ref={urllib.parse.quote(ref)}"
    rc = requests.get(url_contents, headers=_gh_headers(), timeout=30)
    if rc.status_code != 200 or rc.json().get("encoding") != "base64":
        st.error(f"YAML okunamadÄ±: {rc.status_code} {rc.text[:200]}")
        return
    yaml_text = base64.b64decode(rc.json()["content"]).decode("utf-8", errors="replace")
    st.code(yaml_text, language="yaml")

    # 4) (Opsiyonel) YAML parse: workflow_dispatch var mÄ±?
    if yaml:
        try:
            y = yaml.safe_load(yaml_text) or {}
            on_section = y.get("on") or {}
            has_dispatch = False
            inputs_info = {}
            if isinstance(on_section, dict):
                if "workflow_dispatch" in on_section:
                    has_dispatch = True
                    wd = on_section.get("workflow_dispatch") or {}
                    if isinstance(wd, dict):
                        inputs_info = wd.get("inputs", {}) or {}
            elif isinstance(on_section, list):
                has_dispatch = "workflow_dispatch" in on_section

            if has_dispatch:
                st.success("âœ… Bu workflow YAML'Ä±nda `workflow_dispatch` tanÄ±mlÄ±.")
            else:
                st.error("âŒ Bu workflow YAML'Ä±nda `workflow_dispatch` YOK. 422â€™nin ana nedeni bu.")

            if inputs_info:
                st.write("ğŸ§© `workflow_dispatch.inputs` tanÄ±mÄ±:")
                st.json(inputs_info)
            else:
                st.caption("Bu workflow iÃ§in Ã¶zel inputs tanÄ±mÄ± yok (veya boÅŸ).")
        except Exception as e:
            st.warning(f"YAML parse edilemedi: {e} (PyYAML sÃ¼rÃ¼mÃ¼/format)")

    else:
        st.info("PyYAML yÃ¼klÃ¼ deÄŸil; YAML analizi atlandÄ±. (Ä°stersen requirements.txt â†’ PyYAML ekleyebilirsin)")

    # 5) Branch kontrolÃ¼
    url_branch = f"https://api.github.com/repos/{GITHUB_REPO}/branches/{urllib.parse.quote(ref)}"
    rb = requests.get(url_branch, headers=_gh_headers(), timeout=30)
    st.success(f"âœ… Branch mevcut: {ref}") if rb.status_code == 200 else st.warning(f"âš ï¸ Branch bulunamadÄ±: {ref} ({rb.status_code})")

    # 6) Ã–rnek dispatch payload bilgisini gÃ¶ster
    st.write({"ref": ref, "inputs": {"persist": "artifact", "force": "true"}})
    st.caption(f"Endpoint: /repos/{GITHUB_REPO}/actions/workflows/{wid}/dispatches")

def _get_last_run_by_workflow():
    url = f"https://api.github.com/repos/{GITHUB_REPO}/actions/workflows/{GITHUB_WORKFLOW}/runs?per_page=1"
    r = requests.get(url, headers=_gh_headers(), timeout=30)
    if r.status_code != 200:
        return None, r.status_code, r.text
    arr = r.json().get("workflow_runs", [])
    return (arr[0] if arr else None), 200, ""

def _render_last_run_status(container):
    if not (st.secrets.get("GH_TOKEN") or os.environ.get("GH_TOKEN")):
        container.info("GH_TOKEN yok; GitHub durumunu okuyamÄ±yorum.")
        return
    try:
        run, code, msg = _get_last_run_by_workflow()
        if not run:
            container.info("Bu workflow iÃ§in run bulunamadÄ±.")
            return
        status = run.get("status")
        concl  = run.get("conclusion") or "-"
        started = run.get("run_started_at")
        html_url = run.get("html_url")
        container.markdown(
            f"**Son koÅŸum:** `{status}` / `{concl}` Â· baÅŸlama: `{started}`  Â·  [GitHubâ€™da aÃ§]({html_url})"
        )
    except Exception as e:
        container.warning(f"Durum okunamadÄ±: {e}")

def fetch_latest_artifact_df() -> Optional[pd.DataFrame]:
    try:
        runs_url = f"https://api.github.com/repos/{GITHUB_REPO}/actions/runs?per_page=20"
        runs = requests.get(runs_url, headers=_gh_headers(), timeout=30).json()
        run_ids = [r["id"] for r in runs.get("workflow_runs", []) if r.get("conclusion") == "success"]

        for rid in run_ids:
            arts_url = f"https://api.github.com/repos/{GITHUB_REPO}/actions/runs/{rid}/artifacts"
            arts = requests.get(arts_url, headers=_gh_headers(), timeout=30).json().get("artifacts", [])
            for a in arts:
                if a.get("expired", False) or a.get("name") not in ARTIFACT_NAMES:
                    continue
                dl = requests.get(a["archive_download_url"], headers=_gh_headers(), timeout=60)
                zf = zipfile.ZipFile(io.BytesIO(dl.content))

                # Ã–nce dÃ¼zden dene
                for pick in ("crime_prediction_data/sf_crime_08.csv", "sf_crime_08.csv"):
                    if pick in zf.namelist():
                        with zf.open(pick) as f:
                            df = pd.read_csv(f, low_memory=False)
                            if "date" in df.columns:
                                df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date
                            elif "datetime" in df.columns:
                                df["date"] = pd.to_datetime(df["datetime"], errors="coerce").dt.date
                            return df

                # paquet_run_*.zip iÃ§inden dene
                for n in zf.namelist():
                    if re.search(r"^paquet_run_\d+\.zip$", n):
                        with zf.open(n) as inner_blob:
                            with zipfile.ZipFile(io.BytesIO(inner_blob.read())) as inner_zip:
                                for pick in ("sf_crime_08.csv", "crime_prediction_data/sf_crime_08.csv"):
                                    if pick in inner_zip.namelist():
                                        with inner_zip.open(pick) as f:
                                            df = pd.read_csv(f, low_memory=False)
                                            if "date" in df.columns:
                                                df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date
                                            elif "datetime" in df.columns:
                                                df["date"] = pd.to_datetime(df["datetime"], errors="coerce").dt.date
                                            return df
        return None
    except Exception as e:
        st.warning(f"Artifact indirilemedi: {e}")
        return None

# -----------------------------------------------------------------------------
# 08 â†’ 09 DÃ¶nÃ¼ÅŸÃ¼m YardÄ±mcÄ±larÄ±
# -----------------------------------------------------------------------------
def _group_rare_labels(
    df: pd.DataFrame,
    col: str,
    min_prop: Optional[float] = None,
    min_count: Optional[int] = None,
    other_label: str = "Other",
    out_stats_path: Optional[Path] = None,
) -> pd.Series:
    if col not in df.columns:
        return pd.Series([None] * len(df), index=df.index)

    s = df[col].astype(str).str.strip()
    total = len(s)
    vc = s.value_counts(dropna=False)

    env_prop = os.environ.get("RARE_MIN_PROP")
    env_count = os.environ.get("RARE_MIN_COUNT")
    if min_prop is None and env_prop:
        try:
            min_prop = float(env_prop)
        except Exception:
            pass
    if min_count is None and env_count:
        try:
            min_count = int(env_count)
        except Exception:
            pass

    if min_prop is None and min_count is None:
        min_prop, min_count = 0.01, 200

    rare_mask = pd.Series(False, index=vc.index)
    if min_prop is not None:
        rare_mask |= (vc / max(total, 1)) < float(min_prop)
    if min_count is not None:
        rare_mask |= vc < int(min_count)

    rare_values = set(vc[rare_mask].index)
    grouped = s.where(~s.isin(rare_values), other_label)

    if out_stats_path is not None:
        out_stats_path.parent.mkdir(parents=True, exist_ok=True)
        stats_df = pd.DataFrame({
            col: vc.index,
            "count": vc.values,
            "prop": vc.values / max(total, 1),
            "is_rare": vc.index.map(lambda v: v in rare_values)
        })
        try:
            stats_df.to_csv(out_stats_path, index=False)
        except Exception:
            pass

    return grouped

def clean_and_save_crime_09(input_obj: Union[str, pd.DataFrame] = "sf_crime_08.csv", output_path: str = "sf_crime_09.csv"):
    # input_obj hem DataFrame hem de dosya yolu olabilir
    if isinstance(input_obj, pd.DataFrame):
        df = input_obj.copy()
    else:
        df = pd.read_csv(input_obj, dtype={"GEOID": str})

    # ğŸ›  FIX-1: GEOID normalizasyonu (zfill kaldÄ±rÄ±ldÄ±, sadece sayÄ±sal Ã§ekirdek)
    if "GEOID" in df.columns:
        df["GEOID"] = (
            df["GEOID"]
            .astype(str)
            .str.extract(r"(\d+)", expand=False)
        )

    if "category" in df.columns:
        df["category"] = df["category"].astype(str).str.strip().str.title()

    # Rare class grouping
    try:
        out_dir = Path(output_path).parent if isinstance(output_path, str) else Path(".")
        if "category" in df.columns:
            df["category_grouped"] = _group_rare_labels(
                df, "category", min_prop=None, min_count=None,
                other_label="Other", out_stats_path=out_dir / "rare_stats_category.csv"
            )

        if "subcategory" in df.columns:
            df["subcategory"] = df["subcategory"].astype(str).str.strip().str.title()
            df["subcategory_grouped"] = _group_rare_labels(
                df, "subcategory", min_prop=None, min_count=None,
                other_label="Other", out_stats_path=out_dir / "rare_stats_subcategory.csv"
            )

        try:
            st.caption("ğŸ” Rare grouping uygulandÄ± (category/subcategory). Ä°statistikler CSV olarak kaydedildi.")
        except Exception:
            pass
    except Exception as _e:
        try:
            st.warning(f"Rare grouping atlandÄ±: {str(_e)}")
        except Exception:
            print(f"Rare grouping atlandÄ±: {_e}")

    # Tip dÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ler
    def to_int(df_, col, default=0):
        if col in df_.columns:
            df_[col] = (
                pd.to_numeric(df_[col], errors="coerce")
                .fillna(default)
                .round()
                .astype("Int64")
            )

    def to_float(df_, col, default=0.0):
        if col in df_.columns:
            df_[col] = (
                pd.to_numeric(df_[col], errors="coerce")
                .fillna(default)
                .astype(float)
            )

    # SayaÃ§ kolonlarÄ±
    int_count_cols = [
        "crime_count",
        "911_request_count_hour_range",
        "911_request_count_daily(before_24_hours)",
        "311_request_count",
        "bus_stop_count",
        "train_stop_count",
        "poi_total_count",
    ]
    for c in int_count_cols:
        to_int(df, c, default=0)

    # Risk skoru
    to_float(df, "poi_risk_score", default=0.0)

    # Binary kolonlar
    def to_binary(df_, col):
        if col in df_.columns:
            m = {
                "true": 1, "t": 1, "yes": 1, "y": 1, "1": 1, "evet": 1,
                "false": 0, "f": 0, "no": 0, "n": 0, "0": 0, "hayÄ±r": 0, "hayir": 0
            }
            s = df_[col].replace({True: 1, False: 0})
            s = s.astype(str).str.strip().str.lower().map(m)
            df_[col] = pd.to_numeric(s, errors="coerce").fillna(0).astype("Int64")

    for c in ["is_near_police", "is_near_government"]:
        to_binary(df, c)

    # Mesafe kolonlarÄ±
    for c in ["distance_to_bus", "distance_to_train", "distance_to_police", "distance_to_government_building"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce").fillna(9999.0).astype(float)

    # Range kolonlarÄ± (int kategoriler)
    for c in ["bus_stop_count_range", "train_stop_count_range", "poi_total_count_range", "poi_risk_score_range"]:
        to_int(df, c, default=0)

    for c in ["distance_to_bus_range", "distance_to_train_range", "distance_to_police_range", "distance_to_government_building_range"]:
        if c in df.columns:
            s = pd.to_numeric(df[c], errors="coerce")
            max_cat = int(s.max(skipna=True)) if pd.notna(s.max(skipna=True)) else 3
            df[c] = s.fillna(max_cat).round().astype("Int64")

    # NÃ¼fus (median ile doldur)
    if "population" in df.columns:
        df["population"] = pd.to_numeric(df["population"], errors="coerce")
        median_pop = df["population"].median(skipna=True)
        median_pop = 0 if pd.isna(median_pop) else median_pop
        df["population"] = df["population"].fillna(median_pop)

    # POI dominant type
    if "poi_dominant_type" in df.columns:
        # ğŸ›  FIX-2: BoÅŸ stringleri de None say, sonra "None" ile doldur
        df["poi_dominant_type"] = (
            df["poi_dominant_type"]
            .replace({"": np.nan})
            .fillna("None")
            .astype(str)
        )

    # Tarih normalize
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date
    elif "datetime" in df.columns and "date" not in df.columns:
        df["date"] = pd.to_datetime(df["datetime"], errors="coerce").dt.date

    # ğŸ›  FIX-3: Near-repeat gÃ¼venli hale getirildi (crime_count yoksa Y_label'dan tÃ¼ret)
    try:
        need = {"date", "GEOID"}
        has_crime_count = "crime_count" in df.columns

        if not has_crime_count and "Y_label" in df.columns:
            df["crime_count"] = (
                pd.to_numeric(df["Y_label"], errors="coerce")
                .fillna(0)
                .astype(int)
            )
            has_crime_count = True

        if need.issubset(df.columns) and has_crime_count:
            # kategori kolonu Ã¶ncelik sÄ±rasÄ±
            cat_col = None
            for cc in ["category_grouped", "subcategory_grouped", "category"]:
                if cc in df.columns:
                    cat_col = cc
                    break

            if cat_col:
                tmp = df[["date", "GEOID", cat_col, "crime_count"]].copy()
                tmp["date"] = pd.to_datetime(tmp["date"], errors="coerce").dt.date
                g = (tmp.groupby(["GEOID", cat_col, "date"], as_index=False)["crime_count"].sum())
                g["date"] = pd.to_datetime(g["date"])
                g = g.sort_values(["GEOID", cat_col, "date"])

                def _roll_counts(x):
                    x = x.set_index("date").asfreq("D", fill_value=0)
                    x["nr_7d"]  = x["crime_count"].rolling("7D").sum().shift(1)
                    x["nr_14d"] = x["crime_count"].rolling("14D").sum().shift(1)
                    return x.reset_index()

                g2 = (g.groupby(["GEOID", cat_col]).apply(_roll_counts).reset_index(level=[0, 1]).reset_index(drop=True))
                g2["date"] = g2["date"].dt.date

                df = df.merge(
                    g2[["GEOID", cat_col, "date", "nr_7d", "nr_14d"]],
                    on=["GEOID", cat_col, "date"], how="left"
                )
                for c in ["nr_7d", "nr_14d"]:
                    df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0).astype(float)
    except Exception as _e:
        print(f"near-repeat uyarÄ±: {_e}")

    # DÄ±ÅŸsal deÄŸiÅŸken Ã¶rnek skoru
    try:
        ext_map_path = Path(DATA_DIR) / "crime_type_externals_map.json"
        if ext_map_path.exists():
            with open(ext_map_path, "r", encoding="utf-8") as f:
                type_map = json.load(f)
            key_col = "category_grouped" if "category_grouped" in df.columns else (
                      "category" if "category" in df.columns else None)
            if key_col:
                def _ext_score(row):
                    cols = type_map.get(str(row[key_col]), [])
                    vals = []
                    for c in cols:
                        if c in df.columns:
                            try:
                                vals.append(float(row.get(c, 0)))
                            except Exception:
                                pass
                    return float(np.nanmean(vals)) if len(vals) > 0 else np.nan
                df["externals_type_score"] = df.apply(_ext_score, axis=1)
                df["externals_type_score"] = df["externals_type_score"].fillna(0.0).astype(float)
    except Exception as _e:
        print(f"dÄ±ÅŸsal deÄŸiÅŸken uyarÄ±: {_e}")

    preview_cols = [c for c in ["nr_7d", "nr_14d", "nei_7d_sum", "externals_type_score"] if c in df.columns]
    if preview_cols:
        try:
            st.caption("ğŸ§© Yeni mekÃ¢nsal-zamansal Ã¶zellikler (ilk 20 satÄ±r):")
            st.dataframe(df[preview_cols].head(20))
        except Exception:
            pass

    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(output_path, index=False)
    print(f"âœ… {output_path} kaydedildi. SatÄ±r sayÄ±sÄ±: {len(df)}")
    return df

def load_sf_crime_08(local_path: Path) -> Optional[pd.DataFrame]:
    """Ã–nce yerel dosyayÄ± dene; yoksa artifactâ€™tan Ã§ek. ArdÄ±ndan crime_mix varsa grid ile merge et."""
    def _normalize_date_cols(df_: pd.DataFrame) -> pd.DataFrame:
        if "date" in df_.columns:
            df_["date"] = pd.to_datetime(df_["date"], errors="coerce").dt.date
        elif "datetime" in df_.columns and "date" not in df_.columns:
            df_["date"] = pd.to_datetime(df_["datetime"], errors="coerce").dt.date
        return df_

    df: Optional[pd.DataFrame] = None
    try:
        if local_path.exists():
            df = pd.read_csv(local_path, low_memory=False)
            df = _normalize_date_cols(df)
    except Exception as e:
        st.warning(f"Yerel sf_crime_08.csv okunamadÄ±: {e}")

    if df is None:
        df = fetch_latest_artifact_df()
        if df is None:
            return None
        df = _normalize_date_cols(df)

    # crime_mix merge (grid) â€” opsiyonel
    try:
        _out_dir  = local_path.parent
        _grid_path = _out_dir / "sf_crime_grid_full_labeled.csv"
        if _grid_path.exists():
            grid = pd.read_csv(_grid_path, dtype={"GEOID": str}, low_memory=False)

            keys = ["GEOID", "season", "day_of_week", "event_hour"]
            if set(keys).issubset(grid.columns) and "crime_mix" in grid.columns and set(keys).issubset(df.columns):

                df["GEOID"]   = _norm_geoid(df["GEOID"])
                grid["GEOID"] = _norm_geoid(grid["GEOID"])

                for c in ["day_of_week", "event_hour"]:
                    df[c]   = pd.to_numeric(df[c], errors="coerce").astype("Int64")
                    grid[c] = pd.to_numeric(grid[c], errors="coerce").astype("Int64")

                merged = df.merge(
                    grid[keys + ["crime_mix"]],
                    on=keys, how="left", suffixes=("", "_grid"), validate="many_to_one"
                )

                if "crime_mix_grid" in merged.columns:
                    if "crime_mix" not in merged.columns:
                        merged["crime_mix"] = ""
                    merged["crime_mix"] = merged["crime_mix"].astype(str)
                    merged["crime_mix"] = merged["crime_mix"].where(
                        merged["crime_mix"].str.len() > 0,
                        merged["crime_mix_grid"].fillna("")
                    )
                    merged = merged.drop(columns=["crime_mix_grid"], errors="ignore")
                df = merged
        else:
            print(f"crime_mix merge atlandÄ±: grid bulunamadÄ± â†’ {_grid_path}")
    except Exception as _e:
        print(f"crime_mix merge uyarÄ±sÄ±: {_e}")

    return df

def load_city_crime_08(prefix: str, data_dir: Path) -> Optional[pd.DataFrame]:
    """{prefix}_crime_08.csv'yi yÃ¼kler ve date kolonunu normalize eder."""
    path = data_dir / f"{prefix}_crime_08.csv"
    if prefix.lower() == "sf":
        return load_sf_crime_08(path)
    if not path.exists():
        return None
    try:
        df = pd.read_csv(path, low_memory=False)
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date
        elif "datetime" in df.columns:
            df["date"] = pd.to_datetime(df["datetime"], errors="coerce").dt.date
        return df
    except Exception as e:
        st.warning(f"{path.name} okunamadÄ±: {e}")
        return None

def process_city_to_09(prefix: str, data_dir: Path) -> Optional[pd.DataFrame]:
    """{prefix}_crime_08 â†’ temizle (_08_clean) â†’ neighbors â†’ {prefix}_crime_09 Ã¼retir."""
    df08 = load_city_crime_08(prefix, data_dir)
    if df08 is None:
        st.info(f"{prefix}_crime_08.csv bulunamadÄ±.")
        return None

    out_clean = data_dir / f"{prefix}_crime_08_clean.csv"
    clean_and_save_crime_09(df08, str(out_clean))
    st.success(f"âœ… {out_clean.name} kaydedildi.")

    graph_script = resolve_script({"name": "update_neighbors_graph", "alts": ["update_neighbors_graph.py", "neighbors_graph", "neighbors_graph.py"]})
    feat_script  = resolve_script({"name": "update_neighbors",       "alts": ["update_neighbors.py"]})

    # prefix'e Ã¶zel neighbors varsa onu kullan; yoksa genel
    neighbor_file_pref = data_dir / f"{prefix}_neighbors.csv"
    neighbor_file_gen  = data_dir / "neighbors.csv"
    neighbor_file_use  = neighbor_file_pref if neighbor_file_pref.exists() else neighbor_file_gen

    if not neighbor_file_use.exists() and graph_script:
        ok_graph = run_script(graph_script)
        st.success("ğŸ—ºï¸ neighbors.csv Ã¼retildi.") if ok_graph else st.warning("neighbors graph baÅŸarÄ±sÄ±z.")
        if ok_graph:
            neighbor_file_use = neighbor_file_gen  # grafikten sonra genel Ã¼retildi varsayalÄ±m

    if feat_script:
        os.environ["NEIGHBOR_FILE"]        = os.environ.get("NEIGHBOR_FILE", str(neighbor_file_use))
        os.environ["NEIGHBOR_INPUT_CSV"]   = str(out_clean)
        os.environ["NEIGHBOR_OUTPUT_CSV"]  = str(data_dir / f"{prefix}_crime_09.csv")
        os.environ["NEIGHBOR_WINDOW_DAYS"] = os.environ.get("NEIGHBOR_WINDOW_DAYS", "7")
        os.environ["NEIGHBOR_LAG_DAYS"]    = os.environ.get("NEIGHBOR_LAG_DAYS", "1")

        ok_feat = run_script(feat_script)
        if ok_feat:
            st.success(f"ğŸ§© {prefix}_crime_09.csv Ã¼retildi (nei_7d_sum eklendi).")
            try:
                return pd.read_csv(data_dir / f"{prefix}_crime_09.csv", low_memory=False)
            except Exception:
                return None
        else:
            st.warning("update_neighbors.py Ã§alÄ±ÅŸtÄ±rÄ±lamadÄ±; logu kontrol edin.")
    else:
        st.info("update_neighbors.py bulunamadÄ± (scripts klasÃ¶rÃ¼ne ekleyin).")

    return None

# -----------------------------------------------------------------------------
# Dosya Listeleme / DÃ¶nÃ¼ÅŸtÃ¼rme YardÄ±mcÄ±larÄ±
# -----------------------------------------------------------------------------
def list_files_sorted(
    include: Optional[List[Union[str, Path]]] = None,
    base_dir: Optional[Path] = None,
    pattern: str = "*.csv",
    ascending: bool = True,
    include_missing: bool = True,
) -> pd.DataFrame:
    bdir = base_dir or DATA_DIR
    rows: List[Dict[str, Any]] = []

    if include is None:
        include = []
        for prefix in ["sf", "fr"]:
            include += [str(bdir / f"{prefix}_crime_{i:02d}.csv") for i in range(1, 10)]
            include += [str(bdir / f"{prefix}_crime_y.csv")]
        include += [str(bdir / "sf_crime_grid_full_labeled.csv")]
        for p in bdir.glob(pattern):
            include.append(str(p))

    seen = set()
    for x in include:
        p = Path(x)
        key = str(p.resolve()) if p.exists() else str(p)
        if key in seen:
            continue
        seen.add(key)

        exists = p.exists()
        try:
            st_ = p.stat() if exists else None
            mtime = st_.st_mtime if st_ else None
            size  = st_.st_size  if st_ else None
        except Exception:
            mtime, size = None, None

        if exists or include_missing:
            rows.append({
                "file": p.name,
                "path": str(p),
                "exists": bool(exists),
                "size": _human_bytes(size),
                "modified": _fmt_dt(mtime),
                "age": _age_str(mtime),
                "_mtime": mtime,
            })

    df = pd.DataFrame(rows)
    if not df.empty:
        df = df.sort_values("_mtime", ascending=ascending, na_position="last").drop(columns=["_mtime"])
    return df

def convert_csv_dir_to_parquet(
    input_dir: Path,
    output_dir: Path,
    pattern: str = "*.csv",
    compression: str = "zstd",
    stats: bool = True
) -> pd.DataFrame:
    output_dir.mkdir(parents=True, exist_ok=True)
    rows = []

    try:
        import polars as pl
        use_polars = True
    except Exception:
        use_polars = False

    if not use_polars:
        try:
            import pyarrow  # noqa: F401
        except Exception:
            raise RuntimeError("Ne polars ne de pyarrow mevcut. LÃ¼tfen 'pip install polars pyarrow' kurun.")

    for p in sorted(Path(input_dir).glob(pattern)):
        if not p.is_file():
            continue
        out = Path(output_dir) / (p.stem + ".parquet")
        try:
            if use_polars:
                df_pl = pl.read_csv(str(p))
                df_pl.write_parquet(str(out), compression=compression)
                n_rows = df_pl.height
            else:
                df_pd = pd.read_csv(p, low_memory=False)
                df_pd.to_parquet(out, compression=compression, index=False, engine="pyarrow")
                n_rows = len(df_pd)

            src_sz = p.stat().st_size if p.exists() else None
            dst_sz = out.stat().st_size if out.exists() else None
            rows.append({
                "src": str(p.name),
                "dst": str(out.name),
                "rows": n_rows,
                "src_size": src_sz,
                "dst_size": dst_sz,
            })
        except Exception as e:
            rows.append({
                "src": str(p.name),
                "dst": str(out.name),
                "rows": None,
                "src_size": None,
                "dst_size": None,
                "error": str(e),
            })

    res = pd.DataFrame(rows)
    if stats and not res.empty:
        try:
            res["src_size_mb"] = (res["src_size"].astype("float") / (1024**2)).round(3)
            res["dst_size_mb"] = (res["dst_size"].astype("float") / (1024**2)).round(3)
            res["ratio"] = (res["dst_size"].astype("float") / res["src_size"].astype("float")).round(3)
        except Exception:
            pass
    return res

# -----------------------------------------------------------------------------
# Ä°ndirilebilir kaynaklar
# -----------------------------------------------------------------------------
DOWNLOADS = {
    "SuÃ§ Taban CSV (Release latest)": {
        "url": CRIME_CSV_LATEST,
        "path": str(DATA_DIR / "sf_crime_y.csv"),
    },
    "Tahmin Grid Verisi (GEOID Ã— Zaman + Y_label)": {
        "url": "https://raw.githubusercontent.com/cem5113/crime_prediction_data/main/sf_crime_grid_full_labeled.csv",
        "path": str(DATA_DIR / "sf_crime_grid_full_labeled.csv"),
        "allow_artifact": True,
        "artifact_picks": ["sf_crime_grid_full_labeled.csv"],
    },
    "911 Ã‡aÄŸrÄ±larÄ± (Ã¶zet)": {
        "url": RAW_911_URL,
        "path": str(DATA_DIR / "sf_911_last_5_year_y.csv"),
    },
    "311 Ã‡aÄŸrÄ±larÄ± (Ã¶zet)": {
        "url": SF311_URL,
        "path": str(DATA_DIR / "sf_311_last_5_years_y.csv"),
    },
    "OtobÃ¼s DuraklarÄ±": {
        "url": "https://raw.githubusercontent.com/cem5113/crime_prediction_data/main/sf_bus_stops_with_geoid.csv",
        "path": str(DATA_DIR / "sf_bus_stops_with_geoid.csv"),
    },
    "Tren DuraklarÄ±": {
        "url": "https://raw.githubusercontent.com/cem5113/crime_prediction_data/main/sf_train_stops_with_geoid.csv",
        "path": str(DATA_DIR / "sf_train_stops_with_geoid.csv"),
    },
    "POI GeoJSON": {
        "url": "https://raw.githubusercontent.com/cem5113/crime_prediction_data/main/sf_pois.geojson",
        "path": str(DATA_DIR / "sf_pois.geojson"),
        "is_json": True,
    },
    "NÃ¼fus Verisi": {
        "url": "",
        "path": str(DATA_DIR / "sf_population.csv"),
        "local_src": str(POPULATION_PATH),
        "is_local_csv": True,
    },
    "POI Risk SkorlarÄ±": {
        "url": "https://raw.githubusercontent.com/cem5113/crime_prediction_data/main/risky_pois_dynamic.json",
        "path": str(DATA_DIR / "risky_pois_dynamic.json"),
        "is_json": True,
    },
    "Polis Ä°stasyonlarÄ±": {
        "url": "https://raw.githubusercontent.com/cem5113/crime_prediction_data/main/sf_police_stations.csv",
        "path": str(DATA_DIR / "sf_police_stations.csv"),
    },
    "Devlet BinalarÄ±": {
        "url": "https://raw.githubusercontent.com/cem5113/crime_prediction_data/main/sf_government_buildings.csv",
        "path": str(DATA_DIR / "sf_government_buildings.csv"),
    },
    "Hava Durumu": {
        "url": "https://raw.githubusercontent.com/cem5113/crime_prediction_data/main/sf_weather_5years.csv",
        "path": str(DATA_DIR / "sf_weather_5years.csv"),
    },
}

def download_and_preview(name, url, file_path, is_json=False, allow_artifact_fallback=False, artifact_picks=None):
    st.markdown(f"### ğŸ”¹ {name}")
    st.caption(f"URL: {_mask_token(url)}")
    ok = False

    # Yerel kopya (NÃ¼fus) â€” CSV-only mod
    meta = DOWNLOADS.get(name, {})
    if meta.get("is_local_csv"):
        try:
            src = Path(meta["local_src"])
            dst = Path(file_path)
            dst.parent.mkdir(parents=True, exist_ok=True)
            if src.exists():
                dst.write_bytes(src.read_bytes())
                ok = True
                st.info("Yerel CSV kopyalandÄ±.")
            else:
                st.warning(f"Yerel dosya bulunamadÄ±: {src}")
        except Exception as e:
            st.warning(f"Yerel kopya hatasÄ±: {e}")

    if not ok and url:
        try:
            r = requests.get(url, timeout=60)
            r.raise_for_status()
            Path(file_path).parent.mkdir(parents=True, exist_ok=True)
            if is_json:
                Path(file_path).write_text(r.text, encoding="utf-8")
            else:
                with open(file_path, "wb") as f:
                    f.write(r.content)
            ok = True
        except Exception as e:
            st.warning(f"Raw indirme baÅŸarÄ±sÄ±z: {e}")

    if not ok and allow_artifact_fallback:
        try:
            blob = fetch_file_from_latest_artifact(artifact_picks or [os.path.basename(file_path)])
            if blob:
                Path(file_path).parent.mkdir(parents=True, exist_ok=True)
                with open(file_path, "wb") as f:
                    f.write(blob)
                ok = True
                st.info("Dosya artifact'tan alÄ±ndÄ±.")
        except Exception as e:
            st.warning(f"Artifact fallback baÅŸarÄ±sÄ±z: {e}")

    if not ok:
        st.error(f"âŒ {name} indirilemedi.")
        return

    # Ã–nizleme
    try:
        if is_json:
            try:
                data = json.loads(Path(file_path).read_text(encoding="utf-8"))
                if isinstance(data, dict):
                    st.json(data)
                elif isinstance(data, list):
                    st.json(data[:3])
                else:
                    st.code(str(data)[:1000])
            except Exception:
                st.code(Path(file_path).read_text(encoding="utf-8")[:2000])
        else:
            head = pd.read_csv(file_path, nrows=3)
            cols = pd.read_csv(file_path, nrows=0).columns.tolist()
            st.dataframe(head)
            st.caption(f"ğŸ“Œ SÃ¼tunlar: {cols}")
        st.success("âœ… Ä°ndirildi.")
    except Exception as e:
        st.info("Ã–nizleme baÅŸarÄ±sÄ±z; dosya indirildi.")
        st.code(f"Ã–nizleme hatasÄ±: {e}")

# -----------------------------------------------------------------------------
# UI â€” BaÅŸlÄ±k ve Sidebar
# -----------------------------------------------------------------------------
st.title("ğŸ“¦ GÃ¼nlÃ¼k SuÃ§ Tahmin ZenginleÅŸtirme ve GÃ¼ncelleme Paneli")

with st.sidebar:
    st.markdown("### GitHub Actions")

with st.expander("ğŸ•µï¸ Workflow tanÄ± (dispatch)"):
    wf_in = st.text_input(
        "Workflow (dosya adÄ± veya gÃ¶rÃ¼nen ad)",
        os.environ.get("GITHUB_WORKFLOW", "full_pipeline.yml")
    )
    ref_in = st.text_input("Ref (branch/tag)", "main")
    if st.button("TanÄ±yÄ± Ã‡alÄ±ÅŸtÄ±r"):
        diag_workflow(target=wf_in, ref=ref_in)

    with st.expander("ğŸ•µï¸ Workflow tanÄ± (dispatch)"):
        wf_in = st.text_input("Workflow (dosya adÄ± veya gÃ¶rÃ¼nen ad)",
                              os.environ.get("GITHUB_WORKFLOW", "full_pipeline.yml"))
        ref_in = st.text_input("Ref (branch/tag)", "main")
        if st.button("TanÄ±yÄ± Ã‡alÄ±ÅŸtÄ±r"):
            diag_workflow(target=wf_in, ref=ref_in)

    VARIANTS = ["default", "fr"]
    variant = st.selectbox(
        "Pipeline varyantÄ±",
        VARIANTS, index=0,
        help="default: update_*.py â€¢ fr: update_*.fr.py / update_*_fr.py"
    )
    os.environ["PIPELINE_VARIANT"] = variant

    persist = st.selectbox(
        "Ã‡Ä±ktÄ±yÄ± saklama modu",
        ["artifact", "commit", "none"], index=0,
        help="artifact: repoâ€™yu bozmadan sakla â€¢ commit: repoâ€™ya yaz â€¢ none: sadece log"
    )

    force_bypass = st.checkbox(
        "07:00 kapÄ±sÄ±nÄ± yok say (force)",
        value=True,
        help="Ä°ÅŸaretli ise saat filtresi devre dÄ±ÅŸÄ± kalÄ±r ve pipeline her saatte Ã§alÄ±ÅŸÄ±r."
    )

    status_box = st.empty()
    _render_last_run_status(status_box)

    col_run, col_refresh = st.columns(2)
    with col_run:
        if st.button("ğŸš€ Full pipelineâ€™Ä± Actionsâ€™ta Ã§alÄ±ÅŸtÄ±r"):
            if not (st.secrets.get("GH_TOKEN") or os.environ.get("GH_TOKEN")):
                st.error("GH_TOKEN tanÄ±mlÄ± deÄŸil (Streamlit secrets veya env).")
            else:
                try:
                    r = dispatch_workflow(persist=persist, force=force_bypass)
                    if r["ok"]:
                        st.success(f"Workflow tetiklendi (persist={persist}, force={force_bypass}). Runsâ€™Ä± kontrol et.")
                    else:
                        st.error(f"Tetikleme baÅŸarÄ±sÄ±z: {r['status']} {r['text']}")
                except Exception as e:
                    st.error(f"Hata: {e}")

    with col_refresh:
        if st.button("ğŸ“¡ Son durumu yenile"):
            _render_last_run_status(status_box)

    with st.sidebar.expander("ACS AyarlarÄ± (Demografi)"):
        acs_year_default = os.environ.get("ACS_YEAR", "LATEST")
        whitelist_default = os.environ.get("DEMOG_WHITELIST", "")
        level_default = os.environ.get("CENSUS_GEO_LEVEL", "auto")

        acs_year_in = st.text_input(
            label="ACS_YEAR (LATEST veya YYYY)",
            value=str(acs_year_default or "LATEST"),
            key="acs_year_in",
            help="5-year ACS iÃ§in en son yÄ±lÄ± kullanmak genelde uygundur."
        )

        whitelist_in = st.text_input(
            label="DEMOG_WHITELIST (virgÃ¼llÃ¼; boÅŸ = hepsi)",
            value=str(whitelist_default or ""),
            key="demog_whitelist_in",
            help='Ã–rn: "population,median_income,education". Metin eÅŸleÅŸmesiyle filtreler.'
        )

        levels = ["auto", "tract", "blockgroup", "block"]
        try:
            idx = levels.index(level_default) if level_default in levels else 0
        except Exception:
            idx = 0
        level_in = st.selectbox(
            "CENSUS_GEO_LEVEL",
            levels,
            index=idx,
            key="census_geo_level_in",
            help="NÃ¼fus GEOID eÅŸleÅŸme seviyesi. `auto` Ã§oÄŸu durumda yeterlidir."
        )
        os.environ["CENSUS_GEO_LEVEL"] = level_in

        pop_default = os.environ.get("POPULATION_PATH", str(POPULATION_PATH))
        pop_url_in = st.text_input(
            label="POPULATION_PATH (YEREL CSV YOLU)",
            value=str(pop_default or ""),
            key="population_path_in",
            help="Ã–rn: crime_prediction_data/sf_population.csv (URL kabul edilmez)."
        )

        _v = str(acs_year_in).strip()
        if _v.upper() == "LATEST":
            os.environ["ACS_YEAR"] = "LATEST"
        else:
            _digits = re.sub(r"\D", "", _v)
            os.environ["ACS_YEAR"] = _digits if len(_digits) == 4 else "LATEST"

        os.environ["DEMOG_WHITELIST"] = str(whitelist_in or "")

        if re.match(r"^https?://", str(pop_url_in), flags=re.I):
            st.error("CSV-only mod: URL kabul edilmez. Yerel bir CSV yolu girin.")
        else:
            os.environ["POPULATION_PATH"] = pop_url_in or str(POPULATION_PATH)

# -----------------------------------------------------------------------------
# 0) (Opsiyonel) requirements yÃ¼kleme
# -----------------------------------------------------------------------------
st.markdown("### 0) (Opsiyonel) Gereklilikleri yÃ¼kle")
if st.button("ğŸ“¦ requirements.txt yÃ¼kle"):
    try:
        req = ROOT / "requirements.txt"
        if req.exists():
            out = subprocess.run(
                [sys.executable, "-m", "pip", "install", "-r", str(req)],
                cwd=str(ROOT),
                capture_output=True,
                text=True,
            )
            st.code(out.stdout or "")
            if out.returncode == 0:
                st.success("âœ… Gereklilikler yÃ¼klendi.")
            else:
                st.error("âŒ Kurulumda hata!")
                st.code(out.stderr or "")
        else:
            st.warning("âš ï¸ requirements.txt bulunamadÄ±.")
    except Exception as e:
        st.error(f"Kurulum Ã§aÄŸrÄ±sÄ± baÅŸarÄ±sÄ±z: {e}")

# -----------------------------------------------------------------------------
# 1) (Opsiyonel) Verileri indir ve Ã¶nizle
# -----------------------------------------------------------------------------
st.markdown("### 1) (Opsiyonel) Verileri indir ve Ã¶nizle")
if st.button("ğŸ“¥ Verileri Ä°ndir ve Ã–nizle (Ä°lk 3 SatÄ±r)"):
    for name, info in DOWNLOADS.items():
        download_and_preview(
            name,
            info.get("url", ""),
            info["path"],
            is_json=info.get("is_json", False),
            allow_artifact_fallback=info.get("allow_artifact", False),
            artifact_picks=info.get("artifact_picks"),
        )
    st.success("âœ… Ä°ndirme tamamlandÄ±.")

# -----------------------------------------------------------------------------
# 1.5) DosyalarÄ± tarihe gÃ¶re sÄ±rala
# -----------------------------------------------------------------------------
st.markdown("### 1.5) DosyalarÄ± tarihe gÃ¶re sÄ±rala")
colA, colB, colC = st.columns([1, 1, 2])
with colA:
    order = st.radio("SÄ±ralama", ["Eski â†’ Yeni", "Yeni â†’ Eski"], horizontal=True, index=0)
with colB:
    show_missing = st.checkbox("Eksikleri de gÃ¶ster", value=True)
with colC:
    patt = st.text_input(
        "Desen (glob)", "*.csv",
        help="Ã–rn: sf_crime_*.csv",
        key="glob_list_files"
    )

asc = (order == "Eski â†’ Yeni")
df_files = list_files_sorted(pattern=patt, ascending=asc, include_missing=show_missing)
if df_files.empty:
    st.info("EÅŸleÅŸen dosya yok.")
else:
    st.dataframe(df_files, use_container_width=True)

# -----------------------------------------------------------------------------
# 1.6) CSV â†’ Parquet dÃ¶nÃ¼ÅŸtÃ¼r
# -----------------------------------------------------------------------------
st.markdown("### 1.6) CSV â†’ Parquet dÃ¶nÃ¼ÅŸtÃ¼r")
with st.expander("ğŸ”„ CSVâ€™leri Parquetâ€™e Ã§evir (zstd)"):
    in_dir = st.text_input(
        "Girdi klasÃ¶rÃ¼", value=str(DATA_DIR),
        help="Ã–rn: crime_prediction_data/",
        key="csv2parquet_in_dir"
    )
    out_dir = st.text_input(
        "Ã‡Ä±ktÄ± klasÃ¶rÃ¼", value=str(ROOT / "parquet_out"),
        help="Ã–rn: parquet_out/",
        key="csv2parquet_out_dir"
    )
    patt_in = st.text_input(
        "Desen (glob)", "*.csv",
        help="Ã–rn: sf_crime_*.csv",
        key="csv2parquet_glob"
    )
    comp = st.selectbox(
        "SÄ±kÄ±ÅŸtÄ±rma",
        ["zstd", "snappy", "gzip", "brotli", "uncompressed"],
        index=0,
        key="csv2parquet_codec"
    )
    want_stats = st.checkbox(
        "Ã–zet/stats Ã¼ret", value=True,
        key="csv2parquet_stats"
    )

    if st.button("ğŸ§° DÃ¶nÃ¼ÅŸtÃ¼r (CSV â†’ Parquet)", key="csv2parquet_run"):
        try:
            res = convert_csv_dir_to_parquet(
                input_dir=Path(in_dir),
                output_dir=Path(out_dir),
                pattern=patt_in,
                compression=comp,
                stats=want_stats
            )
            if res.empty:
                st.info("EÅŸleÅŸen CSV bulunamadÄ±.")
            else:
                st.success("DÃ¶nÃ¼ÅŸÃ¼m tamamlandÄ±.")
                st.dataframe(res)
        except Exception as e:
            st.error(f"DÃ¶nÃ¼ÅŸÃ¼m hatasÄ±: {e}")

# -----------------------------------------------------------------------------
# TanÄ± ve Cache
# -----------------------------------------------------------------------------
with st.expander("ğŸ” TanÄ±: Etkin URL/ENV deÄŸerleri"):
    st.write("CRIME_CSV_URL (env):", os.environ.get("CRIME_CSV_URL"))
    st.write("RAW_911_URL (env):", os.environ.get("RAW_911_URL"))
    st.write("SF311_URL (env):", os.environ.get("SF311_URL"))

if st.button("â™»ï¸ Streamlit cache temizle"):
    try:
        st.cache_data.clear()
        st.success("Cache temizlendi.")
    except Exception as e:
        st.warning(f"Cache temizlenemedi: {e}")

# -----------------------------------------------------------------------------
# 2) GÃ¼ncelleme ve ZenginleÅŸtirme (01 â†’ 09)
# -----------------------------------------------------------------------------
st.markdown("### 2) GÃ¼ncelleme ve ZenginleÅŸtirme (01 â†’ 09)")
if st.button("âš™ï¸ GÃ¼ncelleme ve ZenginleÅŸtirme (01 â†’ 09)"):
    with st.spinner("â³ Scriptler Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor..."):
        all_ok = True
        for entry in PIPELINE:
            sp = resolve_script(entry, locale=os.environ.get("PIPELINE_VARIANT", "default"))
            if not sp:
                st.warning(f"â­ï¸ {entry['name']} bulunamadÄ±/indirilemedi, atlanÄ±yor.")
                all_ok = False
                continue
            ok = run_script(sp)
            all_ok = all_ok and ok
    if all_ok:
        st.success("ğŸ‰ Pipeline bitti: TÃ¼m adÄ±mlar baÅŸarÄ±yla tamamlandÄ±.")
    else:
        st.warning("â„¹ï¸ Pipeline tamamlandÄ±; eksik/hatalÄ± adÄ±mlar var. LoglarÄ± kontrol edin.")

# -----------------------------------------------------------------------------
# 3) GÃ¼ncel _08 â†’ _09 Ã¼ret (sf + fr)
# -----------------------------------------------------------------------------
st.markdown("### 3) GÃ¼ncel _08 â†’ _09 Ã¼ret (sf + fr)")
if st.button("ğŸ§ª _08'i temizle ve _09 Ã¼ret (sf & fr)"):
    for prefix in ["sf", "fr"]:
        st.subheader(f"ğŸ”¹ {prefix.upper()} akÄ±ÅŸÄ±")
        _ = process_city_to_09(prefix, DATA_DIR)

# Ã–nizleme: 08 ve 09 dosyalarÄ± (varsa)
for prefix in ["sf", "fr"]:
    st.subheader(f"ğŸ“„ Ã–nizleme â€” {prefix.upper()}")
    p08 = DATA_DIR / f"{prefix}_crime_08.csv"
    p09 = DATA_DIR / f"{prefix}_crime_09.csv"
    if p08.exists():
        try:
            st.markdown(f"**{p08.name} â€” ilk 20 satÄ±r**")
            st.dataframe(pd.read_csv(p08, nrows=20, low_memory=False), use_container_width=True)
        except Exception as e:
            st.info(f"{p08.name} Ã¶nizlenemedi: {e}")
    if p09.exists():
        try:
            st.markdown(f"**{p09.name} â€” ilk 20 satÄ±r**")
            st.dataframe(pd.read_csv(p09, nrows=20, low_memory=False), use_container_width=True)
        except Exception as e:
            st.info(f"{p09.name} Ã¶nizlenemedi: {e}")
