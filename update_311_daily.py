#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
update_311_daily.py â€” 311 Ã¶zetini gÃ¼nlÃ¼k GRID & EVENTS'e sÄ±zÄ±ntÄ±sÄ±z ekler.

ENV
----
CRIME_DATA_DIR           (default: ".")
FR_GRID_DAILY_IN         (girdi grid)   default: "crime_prediction_data/fr_crime_grid_daily.csv"
FR_GRID_DAILY_OUT        (Ã§Ä±ktÄ± grid)   default: "crime_prediction_data/fr_crime_grid_daily.csv"
FR_EVENTS_DAILY_IN       (girdi events) default: "crime_prediction_data/fr_crime_events_daily.csv"
FR_EVENTS_DAILY_OUT      (Ã§Ä±ktÄ± events) default: "crime_prediction_data/fr_crime_events_daily.csv"

FR_311_DAILY_IN          (311 Ã¶zet/ham CSV/Parquet)  Ã–rn: "sf_311_last_5_years.csv"
FR_311_DATE_COL          (311â€™de tarih/datetime alanÄ±; env ile "date" verilebilir)
FR_311_GEOID_COL         (311â€™de GEOID alanÄ±) default: "GEOID"
FR_311_WINSOR_Q          (winsor Ã¼st quantile: "0" kapalÄ±, "0.999" gibi) default: "0"
FR_311_EMA_ALPHAS        (virgÃ¼lle "0.3,0.6") default: "0.3,0.6"

GEOID_LEN                (default: "11")
"""

from __future__ import annotations
import os, sys
from pathlib import Path
import pandas as pd
import numpy as np

pd.options.mode.copy_on_write = True

# ---------------- ENV ----------------
BASE = Path(os.getenv("CRIME_DATA_DIR", ".")).resolve()

GRID_IN  = os.getenv("FR_GRID_DAILY_IN",  "crime_prediction_data/fr_crime_grid_daily.csv").strip()
GRID_OUT = os.getenv("FR_GRID_DAILY_OUT", "crime_prediction_data/fr_crime_grid_daily.csv").strip()

EV_IN    = os.getenv("FR_EVENTS_DAILY_IN",  "crime_prediction_data/fr_crime_events_daily.csv").strip()
EV_OUT   = os.getenv("FR_EVENTS_DAILY_OUT", "crime_prediction_data/fr_crime_events_daily.csv").strip()

N311_IN  = os.getenv("FR_311_DAILY_IN", "").strip()
DATE_COL_ENV = (os.getenv("FR_311_DATE_COL", "") or "").strip()
GEO_COL  = (os.getenv("FR_311_GEOID_COL", "GEOID") or "GEOID").strip()

def _parse_float_env(key: str, default: float) -> float:
    s = str(os.getenv(key, "")).strip()
    if s == "": return default
    try: return float(s)
    except Exception: return default

WIN_Q = _parse_float_env("FR_311_WINSOR_Q", 0.0)   # 0 â†’ kapalÄ±
# EMA alfasÄ±
EMA_ALPHAS = []
for tok in str(os.getenv("FR_311_EMA_ALPHAS","0.3,0.6")).split(","):
    tok = tok.strip()
    if not tok: continue
    try:
        a = float(tok)
        if 0.0 < a < 1.0:
            EMA_ALPHAS.append(a)
    except Exception:
        pass
if not EMA_ALPHAS:
    EMA_ALPHAS = [0.3, 0.6]

GEOID_LEN = int(os.getenv("GEOID_LEN", "11"))

# ---------------- Utils ----------------
def log(msg: str): 
    print(msg, flush=True)

def _norm_geoid(s: pd.Series) -> pd.Series:
    s = s.astype(str).str.extract(r"(\d+)", expand=False)
    return s.str[:GEOID_LEN].str.zfill(GEOID_LEN)

def _read_any(p: Path, nrows: int | None = None) -> pd.DataFrame:
    if p.suffix.lower() == ".parquet":
        df = pd.read_parquet(p)
        return df if nrows is None else df.head(nrows)
    return pd.read_csv(p, low_memory=False, nrows=nrows)

def _winsorize_upper(series: pd.Series, q: float) -> pd.Series:
    if not (0.0 < q < 1.0):
        return series
    try:
        ub = series.quantile(q)
        log(f"ğŸ”§ Winsorize: q={q} Ã¼st={ub}")
        return np.minimum(series, ub)
    except Exception:
        return series

def _choose_date_column(df: pd.DataFrame) -> str:
    """
    Envâ€™de verilen DATE_COL varsa onu kullanÄ±r, yoksa yaygÄ±n adaylar arasÄ±ndan
    en fazla non-null saÄŸlayanÄ± seÃ§er.
    """
    candidates_ordered = []
    if DATE_COL_ENV:
        candidates_ordered.append(DATE_COL_ENV)

    # YaygÄ±n varyantlar (311 setlerinde sÄ±k gÃ¶rÃ¼lenler dahil)
    common = [
        "date",
        "requested_datetime", "requested_date",
        "request_datetime", "request_date",
        "opened_date", "open_dt", "opened",
        "created_date", "created_datetime", "createddate",
        "timestamp", "datetime"
    ]
    for c in common:
        if c not in candidates_ordered:
            candidates_ordered.append(c)

    in_df = [c for c in candidates_ordered if c in df.columns]
    if not in_df:
        raise SystemExit(f"âŒ 311 kaynaÄŸÄ±nda tarih kolonu bulunamadÄ±. "
                         f"Env FR_311_DATE_COL='{DATE_COL_ENV}', adaylar: {common}, "
                         f"mevcut kolonlar: {list(df.columns)}")

    scores = {c: df[c].notna().sum() for c in in_df}
    picked = max(scores, key=scores.get)
    log(f"ğŸ“… 311 tarih kolonu: '{picked}' (env='{DATE_COL_ENV or '-'}', adaylar={in_df}, non_null={scores[picked]})")
    return picked

# ---------------- Load inputs ----------------
def load_grid_events() -> tuple[pd.DataFrame, pd.DataFrame]:
    g_path = BASE / GRID_IN
    e_path = BASE / EV_IN
    if not g_path.exists() or not e_path.exists():
        raise SystemExit(f"âŒ GRID/EVENTS yok: {g_path} / {e_path}")
    grid = _read_any(g_path)
    evts = _read_any(e_path)
    log(f"ğŸ“– Okundu: {GRID_IN} ({len(grid):,}Ã—{len(grid.columns)})")
    log(f"ğŸ“– Okundu: {EV_IN} ({len(evts):,}Ã—{len(evts.columns)})")
    return grid, evts

def load_311_source() -> pd.DataFrame:
    if not N311_IN:
        raise SystemExit("âŒ FR_311_DAILY_IN env boÅŸ.")
    p = Path(N311_IN)
    if not p.exists():
        raise SystemExit(f"âŒ 311 kaynaÄŸÄ± yok: {p}")
    df = _read_any(p)
    log(f"ğŸ“– Okundu: {p} ({len(df):,}Ã—{len(df.columns)})")
    return df

# ---------------- Calendar mask ----------------
def build_calendar_dates(grid: pd.DataFrame) -> pd.DataFrame:
    need = ["GEOID", "date"]
    for c in need:
        if c not in grid.columns:
            raise SystemExit(f"âŒ GRID'de zorunlu kolon yok: {c}")
    cal = grid[need].copy()
    cal["GEOID"] = _norm_geoid(cal["GEOID"])
    cal["date"]  = pd.to_datetime(cal["date"], errors="coerce").dt.date
    cal = cal.dropna().drop_duplicates().reset_index(drop=True)
    log(f"ğŸ—“ï¸  Takvim boyutu: {len(cal):,} (GEOIDÃ—date)")
    return cal

# ---------------- Feature building ----------------
def make_311_daily_counts(df311: pd.DataFrame) -> pd.DataFrame:
    date_col = _choose_date_column(df311)
    if GEO_COL not in df311.columns:
        raise SystemExit(f"âŒ 311 kaynaÄŸÄ±nda {GEO_COL} yok.")

    # Datetime parse â†’ sadece gÃ¼n (TZ-aware/naive her iki durumda gÃ¼venli)
    s = pd.to_datetime(df311[date_col], errors="coerce", utc=True)
    if s.notna().any():
        dcol = s.dt.tz_convert(None).dt.date
    else:
        dcol = pd.to_datetime(df311[date_col], errors="coerce").dt.date

    out = pd.DataFrame({
        "GEOID": _norm_geoid(df311[GEO_COL]),
        "date":  dcol
    }).dropna()

    g = (out.groupby(["GEOID","date"])
             .size()
             .rename("n311")
             .reset_index())

    # winsor
    if 0.0 < WIN_Q < 1.0:
        g["n311"] = _winsorize_upper(pd.to_numeric(g["n311"], errors="coerce"), WIN_Q).astype(float)
    else:
        g["n311"] = pd.to_numeric(g["n311"], errors="coerce").astype(float)
        log("ğŸ”§ Winsorize kapalÄ± (q=0 veya q>=1).")

    return g

def make_311_features(g_daily: pd.DataFrame, cal_dates: pd.DataFrame) -> pd.DataFrame:
    cal = cal_dates.copy()
    cal["GEOID"] = _norm_geoid(cal["GEOID"])
    cal["date"]  = pd.to_datetime(cal["date"], errors="coerce").dt.date
    cal = cal.dropna()

    g = g_daily.copy()
    g["GEOID"] = _norm_geoid(g["GEOID"])
    g["date"]  = pd.to_datetime(g["date"], errors="coerce").dt.date
    g = g.dropna()

    merged = cal.merge(g, on=["GEOID","date"], how="left")
    merged["n311"] = pd.to_numeric(merged["n311"], errors="coerce").fillna(0.0)

    merged = merged.sort_values(["GEOID","date"]).reset_index(drop=True)
    m = merged.set_index(["GEOID","date"]).sort_index()

    # ==== SIZINTI-KAPALI Ã–ZELLÄ°KLER ====
    # TÃ¼m pencereler geÃ§miÅŸi kullansÄ±n diye series'i shift(1) ile Ã¶teleriz.
    base = m["n311"].groupby(level=0).shift(1).fillna(0.0)

    # EMA (yalnÄ±zca geÃ§miÅŸ gÃ¼nler)
    for a in EMA_ALPHAS:
        col = f"n311_ema_a{int(a*10)}"
        m[col] = (
            base.groupby(level=0)
                .transform(lambda s: s.ewm(alpha=a, adjust=False).mean())
                .values
        )

    # Basit pencereler (yalnÄ±zca geÃ§miÅŸ gÃ¼nler)
    m["n311_prev_1d"] = base
    m["n311_sum_3d"]  = (
        base.groupby(level=0)
            .transform(lambda s: s.rolling(window=3, min_periods=1).sum())
            .values
    )

    return m.reset_index()

# ---------------- Merge into GRID & EVENTS ----------------
def merge_into_grid_events(feats: pd.DataFrame, grid: pd.DataFrame, evts: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:
    feats["GEOID"] = _norm_geoid(feats["GEOID"])
    feats["date"]  = pd.to_datetime(feats["date"], errors="coerce").dt.date

    g2 = grid.copy()
    g2["GEOID"] = _norm_geoid(g2["GEOID"])
    g2["date"]  = pd.to_datetime(g2["date"], errors="coerce").dt.date
    k = ["GEOID","date"]
    before = g2.shape
    g2 = g2.merge(feats, on=k, how="left")
    for c in [c for c in g2.columns if c.startswith("n311")]:
        g2[c] = pd.to_numeric(g2[c], errors="coerce").fillna(0.0)
    after = g2.shape
    log(f"ğŸ”— GRID merge: {before} â†’ {after}")

    e2 = evts.copy()
    e2["GEOID"] = _norm_geoid(e2["GEOID"])
    if "date" in e2.columns:
        e2["date"] = pd.to_datetime(e2["date"], errors="coerce").dt.date
        before_e = e2.shape
        e2 = e2.merge(feats, on=["GEOID","date"], how="left")
        for c in [c for c in e2.columns if c.startswith("n311")]:
            e2[c] = pd.to_numeric(e2[c], errors="coerce").fillna(0.0)
        after_e = e2.shape
        log(f"ğŸ”— EVENTS merge (date-based): {before_e} â†’ {after_e}")
    else:
        # Events'te 'date' yoksa GEOID medyanÄ±yla doldur (yumuÅŸak fallback)
        agg = feats.groupby("GEOID", as_index=False).median(numeric_only=True)
        before_e = e2.shape
        e2 = e2.merge(agg, on="GEOID", how="left")
        for c in [c for c in e2.columns if c.startswith("n311")]:
            e2[c] = pd.to_numeric(e2[c], errors="coerce").fillna(0.0)
        after_e = e2.shape
        log(f"ğŸ”— EVENTS merge (calendar-fallback): {before_e} â†’ {after_e}")

    return g2, e2

# ---------------- Save ----------------
def safe_save_csv(df: pd.DataFrame, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = path.as_posix() + ".tmp"
    df.to_csv(tmp, index=False)
    os.replace(tmp, path)

# ---------------- MAIN ----------------
def main():
    log("ğŸš€ enrich_with_311.py (GRID + EVENTS, gÃ¼nlÃ¼k-only, sÄ±zÄ±ntÄ±sÄ±z)")

    grid, evts = load_grid_events()
    df311 = load_311_source()

    g_daily = make_311_daily_counts(df311)
    cal     = build_calendar_dates(grid)
    feats   = make_311_features(g_daily, cal) if len(cal) else pd.DataFrame()

    if feats.empty:
        log("â„¹ï¸ 311 feature set boÅŸ â†’ passthrough (kolon eklemeden kaydet).")
        safe_save_csv(grid, BASE / GRID_OUT)
        safe_save_csv(evts, BASE / EV_OUT)
        return 0

    g2, e2 = merge_into_grid_events(feats, grid, evts)

    safe_save_csv(g2, BASE / GRID_OUT)
    safe_save_csv(e2, BASE / EV_OUT)
    log(f"âœ… YazÄ±ldÄ±: {GRID_OUT} ({len(g2):,} satÄ±r, {len(g2.columns)} sÃ¼tun)")
    log(f"âœ… YazÄ±ldÄ±: {EV_OUT} ({len(e2):,} satÄ±r, {len(e2.columns)} sÃ¼tun)")

    try:
        log(g2.head(5).to_string(index=False))
    except Exception:
        pass
    return 0

if __name__ == "__main__":
    sys.exit(main())
