# update_train.py

import os, io, zipfile, time
from pathlib import Path

import numpy as np
import pandas as pd
import geopandas as gpd
import requests
from scipy.spatial import cKDTree

# =========================
# KÃ¼Ã§Ã¼k yardÄ±mcÄ±lar
# =========================
def ensure_parent(path: str) -> None:
    Path(os.path.dirname(path) or ".").mkdir(parents=True, exist_ok=True)

def safe_save_csv(df: pd.DataFrame, path: str) -> None:
    """Atomic yazÄ±m: tmp â†’ replace; hata halinde .bak bÄ±rak."""
    ensure_parent(path)
    tmp = path + ".tmp"
    try:
        df.to_csv(tmp, index=False)
        os.replace(tmp, path)
        print(f"ðŸ’¾ Kaydedildi: {path}")
    except Exception as e:
        print(f"âŒ Kaydetme hatasÄ±: {path}\n{e}")
        try:
            df.to_csv(path + ".bak", index=False)
            print(f"ðŸ“ Yedek oluÅŸturuldu: {path}.bak")
        except Exception:
            pass

def log_shape(df: pd.DataFrame, label: str) -> None:
    r, c = df.shape
    print(f"ðŸ“Š {label}: {r} satÄ±r Ã— {c} sÃ¼tun")

def log_delta(before_shape, after_shape, label: str) -> None:
    br, bc = before_shape
    ar, ac = after_shape
    print(f"ðŸ”— {label}: {br}Ã—{bc} â†’ {ar}Ã—{ac} (Î”r={ar-br}, Î”c={ac-bc})")

DEFAULT_GEOID_LEN = int(os.getenv("GEOID_LEN", "11"))

def normalize_geoid(series: pd.Series, target_len: int = DEFAULT_GEOID_LEN) -> pd.Series:
    """YalnÄ±zca rakamlarÄ± al, SOLâ€™dan target_len haneyi tut, zfill."""
    s = series.astype(str).str.extract(r"(\d+)", expand=False)
    L = int(target_len)
    return s.str[:L].str.zfill(L)

def freedman_diaconis_bin_count(data: np.ndarray, max_bins: int = 10) -> int:
    data = np.asarray(data)
    data = data[np.isfinite(data)]
    if data.size < 2 or np.allclose(data.min(), data.max()):
        return 1
    q75, q25 = np.percentile(data, [75, 25])
    iqr = q75 - q25
    if iqr <= 0:
        return min(max_bins, max(2, int(np.sqrt(len(data)))))
    bw = 2 * iqr / (len(data) ** (1 / 3))
    if bw <= 0:
        return min(max_bins, max(2, int(np.sqrt(len(data)))))
    return max(2, min(max_bins, int(np.ceil((data.max() - data.min()) / bw))))

# =========================
# ENV / Yollar
# =========================
BASE_DIR = os.getenv("CRIME_DATA_DIR", "crime_prediction_data")
Path(BASE_DIR).mkdir(parents=True, exist_ok=True)

# SuÃ§ girdisi adaylarÄ±
CRIME_CANDIDATES = [
    os.path.join(BASE_DIR, "sf_crime_04.csv"),  # (tercih edilen: bus enrich sonrasÄ±)
    os.path.join(BASE_DIR, "sf_crime_03.csv"),
    os.path.join(BASE_DIR, "sf_crime_02.csv"),
    os.path.join(BASE_DIR, "sf_crime_01.csv"),
    os.path.join(BASE_DIR, "sf_crime.csv"),
]
CRIME_INPUT = next((p for p in CRIME_CANDIDATES if os.path.exists(p)), None)
if CRIME_INPUT is None:
    raise FileNotFoundError("âŒ SuÃ§ girdi dosyasÄ± bulunamadÄ± (sf_crime_04/03/02/01.csv ya da sf_crime.csv).")
print(f"ðŸ“„ Train enrich girdi: {os.path.abspath(CRIME_INPUT)}")

CRIME_OUTPUT = os.path.join(BASE_DIR, "sf_crime_05.csv")

# Ara veri / Ã§Ä±ktÄ± adlarÄ± (kanonik + uyumluluk)
TRAIN_STOPS_WITH_GEOID = os.path.join(BASE_DIR, os.getenv("TRAIN_STOPS_NAME", "sf_train_stops_with_geoid.csv"))
TRAIN_LEGACY_RAW_Y     = os.path.join(BASE_DIR, os.getenv("TRAIN_LEGACY_RAW_Y", "train_y.csv"))  # legacy ham
TRAIN_SUMMARY_NAME     = os.path.join(BASE_DIR, os.getenv("TRAIN_SUMMARY_NAME", "train.csv"))     # legacy Ã¶zet/feature

# Census GeoJSON adaylarÄ±
CENSUS_CANDIDATES = [
    os.path.join(BASE_DIR, "sf_census_blocks_with_population.geojson"),
    os.path.join(".",      "sf_census_blocks_with_population.geojson"),
]

# GTFS kaynaklarÄ± (BART)
GTFS_URLS = [
    os.getenv("BART_GTFS_URL", "https://transitfeeds.com/p/bart/58/latest/download"),
    # Dilersen alternatifler iÃ§in env Ã¼zerinden ek string verebilirsin:
    os.getenv("BART_GTFS_URL_ALT1", "").strip(),
    os.getenv("BART_GTFS_URL_ALT2", "").strip(),
]
GTFS_URLS = [u for u in GTFS_URLS if u]  # boÅŸlarÄ± ayÄ±kla

# Ä°ndirme baÅŸarÄ±sÄ±z olursa cache / stub?
ALLOW_STUB = os.getenv("ALLOW_STUB_ON_API_FAIL", "1").strip().lower() not in ("0", "false")

# =========================
# 1) GTFS stops.txt indir/Ã§Ä±kar (retry/backoff)
# =========================
def download_gtfs_stops(urls: list[str], max_retries: int = 4, backoff_base: float = 1.7) -> pd.DataFrame | None:
    sess = requests.Session()
    for url in urls:
        print(f"ðŸš‰ BART GTFS deneniyor: {url}")
        for attempt in range(max_retries + 1):
            try:
                r = sess.get(url, timeout=60, allow_redirects=True)
                if r.status_code in (429,) or 500 <= r.status_code < 600:
                    if attempt >= max_retries:
                        r.raise_for_status()
                    sleep_s = backoff_base ** (attempt + 1)
                    print(f"âš ï¸ GeÃ§ici hata (HTTP {r.status_code}) â†’ {attempt+1}. deneme, {sleep_s:.1f}s beklemeâ€¦")
                    time.sleep(sleep_s); continue
                r.raise_for_status()
                # Ä°Ã§erik zip olmalÄ±
                buf = io.BytesIO(r.content)
                with zipfile.ZipFile(buf, "r") as zf:
                    members = [m for m in zf.namelist() if m.lower().endswith("stops.txt")]
                    if not members:
                        raise FileNotFoundError("stops.txt GTFS paketinde bulunamadÄ±.")
                    with zf.open(members[0], "r") as f:
                        stops = pd.read_csv(f, dtype={"stop_lat": float, "stop_lon": float})
                return stops
            except Exception as e:
                if attempt >= max_retries:
                    print(f"âŒ GTFS indirme/Ã§Ä±karma hatasÄ± (url={url}): {e}")
                else:
                    sleep_s = backoff_base ** (attempt + 1)
                    print(f"âš ï¸ Hata (url={url}) â†’ tekrar denenecek ({attempt+1}/{max_retries}), {sleep_s:.1f}s bekleme. ({e})")
                    time.sleep(sleep_s)
        print(f"â†ªï¸ URL baÅŸarÄ±sÄ±z: {url}")
    return None

# =========================
# 2) SuÃ§ verisini oku (GEOID evreni)
# =========================
crime = pd.read_csv(CRIME_INPUT, low_memory=False)
log_shape(crime, "CRIME (okundu)")
if "GEOID" not in crime.columns:
    raise KeyError("âŒ SuÃ§ verisinde 'GEOID' kolonu yok.")
crime["GEOID"] = normalize_geoid(crime["GEOID"], DEFAULT_GEOID_LEN)
crime_geoids = pd.Series(crime["GEOID"].unique(), name="GEOID")
print(f"ðŸ§© CRIME farklÄ± GEOID sayÄ±sÄ±: {crime_geoids.size}")

# =========================
# 3) GTFS duraklarÄ±nÄ± edin / cache / stub
# =========================
stops = download_gtfs_stops(GTFS_URLS, max_retries=4, backoff_base=1.7)
if stops is None:
    # cache'ten oku
    if os.path.exists(TRAIN_STOPS_WITH_GEOID):
        print("âš ï¸ GTFS indirilemedi; mevcut cache kullanÄ±lacak:", os.path.abspath(TRAIN_STOPS_WITH_GEOID))
        try:
            stops = pd.read_csv(TRAIN_STOPS_WITH_GEOID, low_memory=False)
        except Exception:
            stops = pd.DataFrame(columns=["stop_lat","stop_lon"])
    elif os.path.exists(TRAIN_LEGACY_RAW_Y):
        print("âš ï¸ GTFS indirilemedi; legacy cache kullanÄ±lacak:", os.path.abspath(TRAIN_LEGACY_RAW_Y))
        try:
            stops = pd.read_csv(TRAIN_LEGACY_RAW_Y, low_memory=False)
        except Exception:
            stops = pd.DataFrame(columns=["stop_lat","stop_lon"])
    elif ALLOW_STUB:
        print("âš ï¸ GTFS ve yerel cache yok â†’ STUB (0 durak, NaN metrik).")
        stops = pd.DataFrame(columns=["stop_lat","stop_lon"])
    else:
        raise SystemExit("âŒ GTFS alÄ±namadÄ± ve cache yok; Ã§Ä±kÄ±lÄ±yor.")

# Kolon isimleri normalize
low = {c.lower(): c for c in stops.columns}
if "stop_lat" not in low or "stop_lon" not in low:
    # bazÄ± GTFS'lerde 'stop_latitude/stop_longitude' olabilir
    for a, b in (("stop_latitude","stop_longitude"), ("latitude","longitude"), ("lat","lon"), ("lat","long")):
        if a in low and b in low:
            stops.rename(columns={low[a]:"stop_lat", low[b]:"stop_lon"}, inplace=True)
            break
else:
    # doÄŸru isimler varsa standardize
    if low["stop_lat"] != "stop_lat":
        stops.rename(columns={low["stop_lat"]:"stop_lat"}, inplace=True)
    if low["stop_lon"] != "stop_lon":
        stops.rename(columns={low["stop_lon"]:"stop_lon"}, inplace=True)

stops["stop_lat"] = pd.to_numeric(stops.get("stop_lat"), errors="coerce")
stops["stop_lon"] = pd.to_numeric(stops.get("stop_lon"), errors="coerce")
stops = stops.dropna(subset=["stop_lat","stop_lon"]).copy()
log_shape(stops, "GTFS stops (temiz)")

# =========================
# 4) Census bloklarÄ± ve GEOID eÅŸleme
# =========================
census_path = next((p for p in CENSUS_CANDIDATES if os.path.exists(p)), None)
blocks_ok = True
if census_path is None:
    print("âš ï¸ NÃ¼fus bloklarÄ± GeoJSON bulunamadÄ±. GEOID eÅŸleme/mesafe â†’ stub.")
    blocks_ok = False

if blocks_ok:
    gdf_blocks = gpd.read_file(census_path)
    # CRS normalize
    if gdf_blocks.crs is None:
        gdf_blocks.set_crs("EPSG:4326", inplace=True, allow_override=True)
    else:
        try:
            epsg = gdf_blocks.crs.to_epsg()
        except Exception:
            epsg = None
        if epsg != 4326:
            gdf_blocks = gdf_blocks.to_crs(epsg=4326)

    gcol = "GEOID" if "GEOID" in gdf_blocks.columns else next(
        (c for c in gdf_blocks.columns if str(c).upper().startswith("GEOID")), None
    )
    if not gcol:
        print("âš ï¸ GeoJSON'da GEOID vari sÃ¼tun yok. Stubâ€™a dÃ¼ÅŸÃ¼lecek.")
        blocks_ok = False
    else:
        gdf_blocks["GEOID"] = normalize_geoid(gdf_blocks[gcol], DEFAULT_GEOID_LEN)
else:
    gdf_blocks = None

if blocks_ok and not stops.empty:
    gdf_stops = gpd.GeoDataFrame(
        stops, geometry=gpd.points_from_xy(stops["stop_lon"], stops["stop_lat"]), crs="EPSG:4326"
    )
    try:
        gdf_joined = gpd.sjoin(gdf_stops, gdf_blocks[["geometry","GEOID"]], how="left", predicate="within")
    except Exception as e:
        print(f"âš ï¸ sjoin(within) baÅŸarÄ±sÄ±z ({e}). sjoin_nearest(max_distance=5 m) deneniyorâ€¦")
        gdf_joined = gpd.sjoin_nearest(gdf_stops, gdf_blocks[["geometry","GEOID"]], how="left", max_distance=5)
    gdf_joined = gdf_joined.drop(columns=["index_right"], errors="ignore")
    gdf_joined["GEOID"] = normalize_geoid(gdf_joined["GEOID"], DEFAULT_GEOID_LEN)
    train_stops_geo = pd.DataFrame(gdf_joined.drop(columns=["geometry"], errors="ignore")).copy()
    log_shape(train_stops_geo, "TRAIN stops â¨¯ GEOID (eÅŸleme)")
else:
    train_stops_geo = stops.copy()
    train_stops_geo["GEOID"] = pd.NA

# =========================
# 5) Ham dosyalarÄ± yaz (kanonik + legacy)
# =========================
safe_save_csv(train_stops_geo, TRAIN_STOPS_WITH_GEOID)   # kanonik
try:
    safe_save_csv(train_stops_geo, TRAIN_LEGACY_RAW_Y)   # legacy uyumluluk
    print(f"âœ… TRAIN ham (kanonik): {TRAIN_STOPS_WITH_GEOID}")
    print(f"â†ªï¸ Legacy kopya (train_y.csv): {TRAIN_LEGACY_RAW_Y}")
except Exception as e:
    print(f"âš ï¸ Legacy train_y.csv yazÄ±lamadÄ±: {e}")

# =========================
# 6) GEOID-level metrikler (distance & count) + binleme
# =========================
if blocks_ok:
    gdf_blocks_3857 = gdf_blocks[["GEOID","geometry"]].copy().to_crs(epsg=3857)
    gdf_blocks_3857["cx"] = gdf_blocks_3857.geometry.centroid.x
    gdf_blocks_3857["cy"] = gdf_blocks_3857.geometry.centroid.y
    blocks_xy = np.vstack([gdf_blocks_3857["cx"].values, gdf_blocks_3857["cy"].values]).T

    gdf_train_xy = gpd.GeoDataFrame(
        train_stops_geo.dropna(subset=["stop_lat","stop_lon"])[["stop_lat","stop_lon"]].copy(),
        geometry=gpd.points_from_xy(
            train_stops_geo.dropna(subset=["stop_lat","stop_lon"])["stop_lon"],
            train_stops_geo.dropna(subset=["stop_lat","stop_lon"])["stop_lat"]
        ),
        crs="EPSG:4326"
    ).to_crs(epsg=3857)

    train_xy = np.vstack([gdf_train_xy.geometry.x.values, gdf_train_xy.geometry.y.values]).T

    geo_metrics = pd.DataFrame({"GEOID": gdf_blocks_3857["GEOID"].values})
    geo_metrics["distance_to_train"] = np.nan
    geo_metrics["train_stop_count"] = 0

    if len(train_xy) > 0 and len(blocks_xy) > 0:
        tree = cKDTree(train_xy)
        nearest_dist, _ = tree.query(blocks_xy, k=1)
        geo_metrics["distance_to_train"] = nearest_dist

        finite_d = nearest_dist[np.isfinite(nearest_dist)]
        if finite_d.size > 0 and np.nanmax(finite_d) > 0:
            radius = float(np.nanpercentile(finite_d, 75))  # p75 yarÄ±Ã§ap
            neighbor_lists = tree.query_ball_point(blocks_xy, r=radius)
            geo_metrics["train_stop_count"] = [len(lst) for lst in neighbor_lists]
            print(f"ðŸŸ¢ SayÄ±m yarÄ±Ã§apÄ± (p75): ~{int(round(radius))} m")
else:
    # GeoJSON yoksa: CRIME GEOID evrenine stub
    geo_metrics = pd.DataFrame({"GEOID": crime_geoids})
    geo_metrics["distance_to_train"] = np.nan
    geo_metrics["train_stop_count"] = 0

# TekilleÅŸtirme + doÄŸrulama
geo_metrics["GEOID"] = normalize_geoid(geo_metrics["GEOID"], DEFAULT_GEOID_LEN)
geo_metrics = geo_metrics.sort_values("GEOID").drop_duplicates("GEOID", keep="first")
assert geo_metrics["GEOID"].is_unique, "TRAIN: GEOID eÅŸsiz deÄŸil!"
log_shape(geo_metrics, "GEOID-bazlÄ± metrikler (ham)")

# Binleme
dist = pd.to_numeric(geo_metrics["distance_to_train"], errors="coerce").replace([np.inf, -np.inf], np.nan)
finite_dist = dist.dropna()
if len(finite_dist) >= 2 and finite_dist.max() > finite_dist.min():
    n_bins = freedman_diaconis_bin_count(finite_dist.to_numpy(), max_bins=10)
    _, edges = pd.qcut(finite_dist, q=n_bins, retbins=True, duplicates="drop")
    labels = [f"{int(round(edges[i]))}â€“{int(round(edges[i+1]))}m" for i in range(len(edges) - 1)]
    geo_metrics["distance_to_train_range"] = pd.cut(dist, bins=edges, labels=labels, include_lowest=True)
else:
    geo_metrics["distance_to_train_range"] = "0â€“0m"

cnt = pd.to_numeric(geo_metrics["train_stop_count"], errors="coerce").fillna(0)
if cnt.nunique() > 1:
    n_c_bins = freedman_diaconis_bin_count(cnt.to_numpy(), max_bins=8)
    _, c_edges = pd.qcut(cnt, q=n_c_bins, retbins=True, duplicates="drop")
    c_labels = [f"{int(round(c_edges[i]))}â€“{int(round(c_edges[i+1]))}" for i in range(len(c_edges) - 1)]
    geo_metrics["train_stop_count_range"] = pd.cut(cnt, bins=c_edges, labels=c_labels, include_lowest=True)
else:
    geo_metrics["train_stop_count_range"] = f"{int(cnt.min())}â€“{int(cnt.max())}"

log_shape(geo_metrics, "GEOID-bazlÄ± metrikler (binlenmiÅŸ)")

# =========================
# 7) GEOID-level Ã¶zeti da yaz (legacy: train.csv)
# =========================
safe_save_csv(geo_metrics, TRAIN_SUMMARY_NAME)
print(f"âœ… TRAIN Ã¶zet (GEOID-level) yazÄ±ldÄ± â†’ {TRAIN_SUMMARY_NAME}")

# =========================
# 8) SuÃ§ verisine GEOID ile merge
# =========================
crime = pd.read_csv(CRIME_INPUT, dtype={"GEOID": str}, low_memory=False)
_before = crime.shape

crime["GEOID"] = normalize_geoid(crime["GEOID"], DEFAULT_GEOID_LEN)
geo_metrics["GEOID"] = normalize_geoid(geo_metrics["GEOID"], DEFAULT_GEOID_LEN)

# YalnÄ±zca CRIME evrenindeki GEOIDâ€™ler
geo_metrics = geo_metrics[geo_metrics["GEOID"].isin(crime["GEOID"].unique())].copy()

crime_enriched = crime.merge(
    geo_metrics,
    on="GEOID",
    how="left",
    validate="many_to_one"  # satÄ±r patlamasÄ± yok
)

log_delta(_before, crime_enriched.shape, "CRIME â¨¯ TRAIN (GEOID enrich)")
log_shape(crime_enriched, "CRIME (train enrich sonrasÄ±)")

# =========================
# 9) Kaydet & Ã¶nizleme
# =========================
safe_save_csv(crime_enriched, CRIME_OUTPUT)
print("ðŸ“¦ Yeni sÃ¼tunlar eklendi (Ã¶rnek):", ["distance_to_train","distance_to_train_range","train_stop_count","train_stop_count_range"])
print(f"âœ… GÃ¼ncellenmiÅŸ veri kaydedildi â†’ {CRIME_OUTPUT}")
try:
    print("sf_crime_05.csv â€” ilk 5 satÄ±r")
    print(crime_enriched.head(5).to_string(index=False))
except Exception:
    pass
