#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
stacking_fr09.py â€” fr_crime_09.csv Ã¼zerinden:
  - CSV temizleme (ParserError Ã¶nlemi) â†’ fr_crime_09_clean.csv
  - Son 365 gÃ¼n Ã¼zerinden class-balanced stacking modeli eÄŸitimi
  - RandomForest ile numerik feature importance (exogenous effects)
Ã‡Ä±ktÄ±lar (CRIME_DATA_DIR altÄ±nda):
  - fr_crime_09_clean.csv
  - feature_importances_stacking.csv
  - stacking_training_output/model_stacking.pkl
  - stacking_training_output/features.json
"""

import os
import csv
import math
import json
import warnings
from datetime import timedelta
from pathlib import Path

import numpy as np
import pandas as pd

warnings.filterwarnings("ignore")

import geopandas as gpd
from libpysal.weights import Queen, Rook

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier
from sklearn.impute import SimpleImputer
from xgboost import XGBClassifier
import joblib


# ------------------------------------------------------------
# GEOID normalize yardÄ±mcÄ± fonksiyonu
# ------------------------------------------------------------
def normalize_geoid(val: object, length: int = 11) -> str | None:
    if val is None or (isinstance(val, float) and math.isnan(val)):
        return None
    s = str(val).strip()

    if s.endswith(".0"):
        s = s[:-2]

    try:
        if "e" in s.lower():
            s = str(int(float(s)))
    except Exception:
        pass

    if s.isdigit() and length:
        s = s.zfill(length)

    return s


def main() -> None:
    # ============================================================
    # 0) PATH / ORTAM AYARLARI
    # ============================================================
    # CRIME_DATA_DIR verilmiyorsa, script'in bulunduÄŸu klasÃ¶r varsayÄ±lÄ±r
    base_dir = Path(os.environ.get("CRIME_DATA_DIR", Path(__file__).resolve().parent)).resolve()
    print(f"ğŸ“‚ CRIME_DATA_DIR: {base_dir}")

    raw_path = base_dir / "fr_crime_09.csv"
    clean_path = base_dir / "fr_crime_09_clean.csv"

    shp_env = os.environ.get("SF_CELLS_PATH", "")
    if shp_env:
        shp_path = Path(shp_env).resolve()
    else:
        shp_path = base_dir / "sf_cells.geojson"

    if not raw_path.exists():
        raise FileNotFoundError(f"âŒ fr_crime_09.csv bulunamadÄ±: {raw_path}")
    if not shp_path.exists():
        raise FileNotFoundError(f"âŒ sf_cells.geojson bulunamadÄ±: {shp_path}")

    print(f"ğŸ“„ RAW_PATH   : {raw_path}")
    print(f"ğŸ“„ CLEAN_PATH : {clean_path}")
    print(f"ğŸ—º  SHP_PATH   : {shp_path}")

    # ============================================================
    # 1) CSV TEMÄ°ZLEME (ParserError iÃ§in) â†’ fr_crime_09_clean.csv
    # ============================================================
    bad_rows = 0
    print("\nğŸ“¥ Orijinal CSV satÄ±r satÄ±r okunuyor:", raw_path)

    with open(raw_path, "r", encoding="utf-8", errors="ignore", newline="") as fin, \
         open(clean_path, "w", encoding="utf-8", newline="") as fout:

        reader = csv.reader(fin)
        writer = csv.writer(fout)

        header = next(reader, None)
        if header is None:
            raise RuntimeError("âŒ CSV boÅŸ gÃ¶rÃ¼nÃ¼yor, header yok.")

        expected_cols = len(header)
        writer.writerow(header)
        print(f"ğŸ”§ Beklenen kolon sayÄ±sÄ± (header'dan): {expected_cols}")

        for i, row in enumerate(reader, start=2):
            if len(row) == expected_cols:
                writer.writerow(row)
            else:
                bad_rows += 1
                # GitHub loglarÄ±nÄ± Ã§ok ÅŸiÅŸirmemek iÃ§in satÄ±rÄ± yazdÄ±rmÄ±yoruz
                # print(f"âš ï¸ HatalÄ± satÄ±r: {i} â†’ kolon sayÄ±sÄ±: {len(row)} (beklenen: {expected_cols})")

    print(f"\nğŸ§¹ TEMÄ°ZLEME TAMAM. Toplam hatalÄ± satÄ±r sayÄ±sÄ±: {bad_rows}")
    print("âœ” Temiz CSV yazÄ±ldÄ±:", clean_path)

    # ============================================================
    # 2) VERÄ°YÄ° YÃœKLE & DÃœZENLE
    # ============================================================
    df = pd.read_csv(clean_path, low_memory=False)
    print("ğŸ“Š Temiz veri shape:", df.shape)

    df.columns = [c.strip() for c in df.columns]

    if "geoid" in df.columns:
        df["geoid"] = df["geoid"].astype(str)
    elif "GEOID" in df.columns:
        df["geoid"] = df["GEOID"].astype(str)
    else:
        raise Exception("âŒ CSV iÃ§inde 'GEOID' veya 'geoid' kolonu yok.")

    df["geoid"] = df["geoid"].str.replace(r"\.0$", "", regex=True).str.zfill(11)

    if "date" not in df.columns:
        raise Exception("âŒ 'date' kolonu bulunamadÄ±.")
    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    df = df.dropna(subset=["date"]).reset_index(drop=True)

    if "Y_label" not in df.columns:
        raise Exception("âŒ 'Y_label' kolonu yok, model eÄŸitimi yapÄ±lamaz.")

    print("ğŸ” Kolonlar:", df.columns.tolist())

    # ============================================================
    # 3) 3-SAATLÄ°K ZAMAN ARALIÄI & PIVOT (log amaÃ§lÄ±)
    # ============================================================
    if "hour" not in df.columns:
        df["hour"] = df["date"].dt.hour

    df["hour_range"] = (df["hour"] // 3).astype(int)

    df_grp = df.groupby(["geoid", "date", "hour_range"]).size().reset_index(name="crime_count")

    last_date = df_grp["date"].max()
    df30 = df_grp[df_grp["date"] >= last_date - timedelta(days=30)].copy()
    df30["datetime"] = df30["date"] + pd.to_timedelta(df30["hour_range"] * 3, unit="h")

    pivot_3h = df30.pivot_table(
        index="datetime",
        columns=["geoid", "hour_range"],
        values="crime_count",
        fill_value=0
    ).sort_index(axis=1)

    print("âœ” 3-hour pivot shape:", pivot_3h.shape)

    # ============================================================
    # 4) GEOMETRÄ° VE MEKÃ‚NSAL AÄIRLIK MATRÄ°SÄ° (sf_cells.geojson)
    # ============================================================
    print("\nğŸ“¥ GeoJSON okunuyor:", shp_path)
    gdf = gpd.read_file(shp_path)
    print("âœ” GeoJSON yÃ¼klendi. SÃ¼tunlar:", list(gdf.columns))

    geojson_geo_col = "geoid"
    if geojson_geo_col not in gdf.columns:
        raise Exception(f"âŒ GeoJSON iÃ§inde '{geojson_geo_col}' kolonu yok.")

    gdf["GEOID_norm"] = gdf[geojson_geo_col].apply(lambda x: normalize_geoid(x, 11))
    print("ğŸ” GeoJSON ilk 5 GEOID_norm:", gdf["GEOID_norm"].head().tolist())

    df["GEOID_norm"] = df["geoid"].apply(lambda x: normalize_geoid(x, 11))
    print("ğŸ” CSV ilk 5 GEOID_norm:", df["GEOID_norm"].head().tolist())

    geojson_ids = set(gdf["GEOID_norm"].dropna().unique())
    csv_ids = set(df["GEOID_norm"].dropna().unique())
    common_ids = geojson_ids.intersection(csv_ids)

    print(f"ğŸ“Š GeoJSON GEOID sayÄ±sÄ±: {len(geojson_ids)}")
    print(f"ğŸ“Š CSV GEOID sayÄ±sÄ±    : {len(csv_ids)}")
    print(f"ğŸ“Š ORTAK GEOID sayÄ±sÄ±  : {len(common_ids)}")

    if len(common_ids) == 0:
        raise Exception("âŒ GeoJSON ile fr_crime_09.csv arasÄ±nda normalize edilmiÅŸ GEOID eÅŸleÅŸmesi yok.")

    gdf2 = gdf[gdf["GEOID_norm"].isin(common_ids)].copy().set_index("GEOID_norm")
    print("âœ” EÅŸleÅŸen hÃ¼cre sayÄ±sÄ±:", len(gdf2), "/", len(geojson_ids))

    print("â³ Queen & Rook mekÃ¢nsal aÄŸÄ±rlÄ±k matrisleri oluÅŸturuluyor...")
    W_queen = Queen.from_dataframe(gdf2)
    W_queen.transform = "r"

    W_rook = Rook.from_dataframe(gdf2)
    W_rook.transform = "r"

    print("âœ” Queen matrisi hazÄ±r. Ã–rnek:", dict(list(W_queen.neighbors.items())[:3]))
    print("âœ” Rook matrisi hazÄ±r.  Ã–rnek:", dict(list(W_rook.neighbors.items())[:3]))
    print("ğŸ“Œ MekÃ¢nsal komÅŸuluk yapÄ±sÄ± hazÄ±r, STARIMA iÃ§in kullanÄ±labilir.")

    # ============================================================
    # 5) STACKING ML MODEL (FULL FEATURE SET, son 365 gÃ¼n)
    # ============================================================
    ML_DAYS = int(os.environ.get("FR_ML_DAYS", "365"))
    max_date = df["date"].max()
    cutoff_ml = max_date - pd.Timedelta(days=ML_DAYS)

    df_ml = df[df["date"] >= cutoff_ml].copy()
    df_ml = df_ml.dropna(subset=["Y_label"]).reset_index(drop=True)

    print(f"\nğŸ§ª ML iÃ§in kullanÄ±lan tarih aralÄ±ÄŸÄ±: {cutoff_ml.date()} â†’ {max_date.date()}")
    print(f"ğŸ“ ML veri satÄ±r sayÄ±sÄ± (son {ML_DAYS} gÃ¼n): {len(df_ml)}")

    pos = df_ml[df_ml["Y_label"] == 1]
    neg = df_ml[df_ml["Y_label"] == 0]

    NEG_FRAC = float(os.environ.get("FR_NEG_FRAC", "0.3"))
    if len(neg) > 0:
        neg_sample = neg.sample(frac=NEG_FRAC, random_state=42)
        df_train = pd.concat([pos, neg_sample]).sample(frac=1.0, random_state=42)
    else:
        df_train = pos.copy()

    print(
        f"ğŸ” Denge sonrasÄ±: pozitif={ (df_train['Y_label']==1).sum() }, "
        f"negatif={ (df_train['Y_label']==0).sum() }"
    )

    y = df_train["Y_label"]
    drop_cols = ["Y_label", "date"]
    X = df_train.drop(columns=[c for c in drop_cols if c in df_train.columns])

    num_cols = X.select_dtypes(include=np.number).columns
    cat_cols = X.select_dtypes(include="object").columns

    print(f"ğŸ”¢ Feature sayÄ±sÄ± â†’ numeric={len(num_cols)}, categorical={len(cat_cols)}")

    preprocess = ColumnTransformer([
        ("num", Pipeline([
            ("imp", SimpleImputer(strategy="median")),
            ("sc", StandardScaler())
        ]), num_cols),
        ("cat", Pipeline([
            ("imp", SimpleImputer(strategy="most_frequent")),
            ("ohe", OneHotEncoder(handle_unknown="ignore"))
        ]), cat_cols)
    ])

    estimators = [
        ("rf", RandomForestClassifier(
            n_estimators=80,
            max_depth=10,
            n_jobs=-1,
            random_state=42
        )),
        ("et", ExtraTreesClassifier(
            n_estimators=80,
            max_depth=10,
            n_jobs=-1,
            random_state=42
        )),
        ("xgb", XGBClassifier(
            n_estimators=150,
            learning_rate=0.07,
            max_depth=4,
            subsample=0.8,
            colsample_bytree=0.8,
            objective="binary:logistic",
            eval_metric="logloss",
            n_jobs=-1,
            random_state=42
        ))
    ]

    meta = LogisticRegression(max_iter=300)

    stack_model = Pipeline([
        ("prep", preprocess),
        ("stack", StackingClassifier(
            estimators=estimators,
            final_estimator=meta,
            cv=3,
            n_jobs=-1,
            passthrough=False
        ))
    ])

    print("\nâ³ STACKING modeli (hÄ±zlÄ± mod) eÄŸitiliyor...")
    stack_model.fit(X, y)
    print("âœ” STACKING model fit edildi (hÄ±zlÄ± mod).")

    # ============================================================
    # 6) EXOGENOUS EFFECTS (RF IMPORTANCE)
    # ============================================================
    print("\nâ³ RandomForest ile numerik feature importance hesaplanÄ±yor...")
    rf_imp = RandomForestClassifier(
        n_estimators=200,
        max_depth=10,
        n_jobs=-1,
        random_state=42
    )
    rf_imp.fit(X[num_cols], y)

    imp_norm = rf_imp.feature_importances_ / rf_imp.feature_importances_.sum()
    effects = dict(zip(num_cols, imp_norm))

    print("âœ” Exogenous feature etkileri hesaplandÄ±.")

    # ============================================================
    # 7) FEATURE IMPORTANCE + MODEL & FEATURE LIST KAYDI
    # ============================================================
    feat_imp_df = pd.DataFrame({
        "feature": num_cols,
        "importance": [effects.get(c, 0.0) for c in num_cols]
    }).sort_values("importance", ascending=False)

    feat_imp_path = base_dir / "feature_importances_stacking.csv"
    feat_imp_df.to_csv(feat_imp_path, index=False)

    print("\nğŸ’¾ Feature importance kaydedildi:", feat_imp_path)
    print("ğŸ” En Ã¶nemli ilk 20 feature:")
    print(feat_imp_df.head(20))

    save_dir = base_dir / "stacking_training_output"
    save_dir.mkdir(parents=True, exist_ok=True)

    model_path = save_dir / "model_stacking.pkl"
    feat_path = save_dir / "features.json"

    joblib.dump(stack_model, model_path)
    print("ğŸ’¾ Model kaydedildi â†’", model_path)

    with open(feat_path, "w", encoding="utf-8") as f:
        json.dump(X.columns.tolist(), f, indent=2, ensure_ascii=False)
    print("ğŸ’¾ Feature list kaydedildi â†’", feat_path)

    print("\nâœ… stacking_fr09.py tamamlandÄ±.")

if __name__ == "__main__":
    main()
