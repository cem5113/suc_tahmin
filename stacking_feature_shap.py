#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
stacking_feature_shap.py â€” fr_crime_09.csv Ã¼zerinden:
  - CSV temizleme (ParserError Ã¶nlemi) â†’ fr_crime_09_clean.csv
  - Son FR_ML_DAYS gÃ¼n Ã¼zerinden class-balanced stacking modeli eÄŸitimi
  - RandomForest ile numerik feature importance (exogenous effects)
  - Opsiyonel SHAP feature importance (shap varsa)

Ã‡Ä±ktÄ±lar (FR_OUTPUT_DIR altÄ±nda):
  - fr_crime_09_clean.csv
  - model_stacking_fr09.pkl
  - features_fr09.json
  - feature_importances_stacking_fr09.csv
  - shap_feature_importance_fr09.csv   (shap varsa)
"""

import os
import csv
import math
import json
import warnings
from pathlib import Path
from datetime import timedelta

import numpy as np
import pandas as pd

warnings.filterwarnings("ignore")

# Geo
import geopandas as gpd
from libpysal.weights import Queen, Rook

# ML â€” Stacking
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import (
    RandomForestClassifier,
    ExtraTreesClassifier,
    StackingClassifier
)
from sklearn.impute import SimpleImputer
from xgboost import XGBClassifier
import joblib


# ------------------------------------------------------------
# YardÄ±mcÄ±: GEOID normalize
# ------------------------------------------------------------
def normalize_geoid(val, length: int = 11) -> str | None:
    if val is None or (isinstance(val, float) and math.isnan(val)):
        return None
    s = str(val).strip()

    # 6075010101.0 â†’ 6075010101
    if s.endswith(".0"):
        s = s[:-2]

    # Bilimsel gÃ¶sterim (6.07501E+09 gibi)
    try:
        if "e" in s.lower():
            s = str(int(float(s)))
    except Exception:
        pass

    # Sadece rakamsa, sÄ±fÄ±r dolduralÄ±m
    if s.isdigit() and length:
        s = s.zfill(length)

    return s


def main() -> None:
    # ============================================================
    # 0) PATH / ENV AYARLARI
    # ============================================================
    base_dir = Path(os.environ.get("CRIME_DATA_DIR", ".")).resolve()
    output_dir = Path(os.environ.get("FR_OUTPUT_DIR", base_dir / "fr_outputs")).resolve()
    output_dir.mkdir(parents=True, exist_ok=True)

    raw_path = base_dir / "fr_crime_09.csv"
    clean_path = output_dir / "fr_crime_09_clean.csv"

    # sf_cells.geojson yolu
    sf_cells_path_env = os.environ.get("SF_CELLS_PATH", "")
    if sf_cells_path_env:
        shp_path = Path(sf_cells_path_env).resolve()
    else:
        # varsayÄ±lan: CRIME_DATA_DIR iÃ§inde sf_cells.geojson
        shp_path = base_dir / "sf_cells.geojson"

    print("ğŸ“‚ CRIME_DATA_DIR :", base_dir)
    print("ğŸ“‚ FR_OUTPUT_DIR  :", output_dir)
    print("ğŸ“„ RAW_PATH       :", raw_path)
    print("ğŸ“„ CLEAN_PATH     :", clean_path)
    print("ğŸ—º  SF_CELLS_PATH :", shp_path)

    if not raw_path.exists():
        raise FileNotFoundError(f"âŒ fr_crime_09.csv bulunamadÄ±: {raw_path}")
    if not shp_path.exists():
        raise FileNotFoundError(f"âŒ sf_cells.geojson bulunamadÄ±: {shp_path}")

    # ============================================================
    # 1) CSV TEMÄ°ZLEME â†’ fr_crime_09_clean.csv
    # ============================================================
    bad_rows = 0
    print(f"\nğŸ“¥ Orijinal CSV satÄ±r satÄ±r okunuyor: {raw_path}")

    with open(raw_path, "r", encoding="utf-8", errors="ignore", newline="") as fin, \
         open(clean_path, "w", encoding="utf-8", newline="") as fout:

        reader = csv.reader(fin)
        writer = csv.writer(fout)

        header = next(reader, None)
        if header is None:
            raise RuntimeError("âŒ CSV boÅŸ gÃ¶rÃ¼nÃ¼yor, header yok.")

        expected_cols = len(header)
        writer.writerow(header)
        print(f"ğŸ”§ Beklenen kolon sayÄ±sÄ± (header'dan): {expected_cols}")

        for i, row in enumerate(reader, start=2):
            if len(row) == expected_cols:
                writer.writerow(row)
            else:
                bad_rows += 1
                # GitHub loglarÄ± ÅŸiÅŸmesin diye satÄ±r print'ini kapalÄ± tutuyoruz
                # print(f"âš ï¸ HatalÄ± satÄ±r: {i}  â†’ kolon sayÄ±sÄ±: {len(row)} (beklenen: {expected_cols})")
                pass

    print(f"\nğŸ§¹ TEMÄ°ZLEME TAMAM. Toplam hatalÄ± satÄ±r sayÄ±sÄ±: {bad_rows}")
    print("âœ” Temiz CSV yazÄ±ldÄ±:", clean_path)

    # ============================================================
    # 2) VERÄ°YÄ° YÃœKLE & DÃœZENLE
    # ============================================================
    df = pd.read_csv(clean_path, low_memory=False)
    print("ğŸ“Š Temiz veri shape:", df.shape)

    df.columns = [c.strip() for c in df.columns]

    # GEOID / geoid
    if "geoid" in df.columns:
        df["geoid"] = df["geoid"].astype(str)
    elif "GEOID" in df.columns:
        df["geoid"] = df["GEOID"].astype(str)
    else:
        raise Exception("âŒ CSV iÃ§inde 'GEOID' veya 'geoid' kolonu yok.")

    df["geoid"] = df["geoid"].str.replace(r"\.0$", "", regex=True).str.zfill(11)

    if "date" not in df.columns:
        raise Exception("âŒ 'date' kolonu bulunamadÄ±.")
    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    df = df.dropna(subset=["date"]).reset_index(drop=True)

    if "Y_label" not in df.columns:
        raise Exception("âŒ 'Y_label' kolonu yok, model eÄŸitimi yapÄ±lamaz.")

    print("ğŸ” Kolonlar:", df.columns.tolist())

    # ============================================================
    # 3) 3-SAATLÄ°K ZAMAN ARALIÄI & PIVOT (Son 30 gÃ¼n, log amaÃ§lÄ±)
    # ============================================================
    if "hour" not in df.columns:
        df["hour"] = df["date"].dt.hour

    df["hour_range"] = (df["hour"] // 3).astype(int)

    df_grp = df.groupby(["geoid", "date", "hour_range"]).size().reset_index(name="crime_count")

    last_date = df_grp["date"].max()
    df30 = df_grp[df_grp["date"] >= last_date - timedelta(days=30)].copy()
    df30["datetime"] = df30["date"] + pd.to_timedelta(df30["hour_range"] * 3, unit="h")

    pivot_3h = df30.pivot_table(
        index="datetime",
        columns=["geoid", "hour_range"],
        values="crime_count",
        fill_value=0
    ).sort_index(axis=1)

    print("âœ” 3-hour pivot shape:", pivot_3h.shape)

    # ============================================================
    # 4) GEOMETRÄ° & MEKÃ‚NSAL AÄIRLIK MATRÄ°SÄ° (sf_cells.geojson)
    # ============================================================
    print("\nğŸ“¥ GeoJSON okunuyor:", shp_path)
    gdf = gpd.read_file(shp_path)
    print("âœ” GeoJSON yÃ¼klendi. SÃ¼tunlar:", list(gdf.columns))

    geojson_geo_col = "geoid"
    if geojson_geo_col not in gdf.columns:
        raise Exception(f"âŒ GeoJSON iÃ§inde '{geojson_geo_col}' kolonu yok.")

    gdf["GEOID_norm"] = gdf[geojson_geo_col].apply(lambda x: normalize_geoid(x, 11))
    df["GEOID_norm"] = df["geoid"].apply(lambda x: normalize_geoid(x, 11))

    print("ğŸ” GeoJSON ilk 5 GEOID_norm:", gdf["GEOID_norm"].head().tolist())
    print("ğŸ” CSV ilk 5 GEOID_norm    :", df["GEOID_norm"].head().tolist())

    geojson_ids = set(gdf["GEOID_norm"].dropna().unique())
    csv_ids = set(df["GEOID_norm"].dropna().unique())
    common_ids = geojson_ids.intersection(csv_ids)

    print(f"ğŸ“Š GeoJSON GEOID sayÄ±sÄ±: {len(geojson_ids)}")
    print(f"ğŸ“Š CSV GEOID sayÄ±sÄ±    : {len(csv_ids)}")
    print(f"ğŸ“Š ORTAK GEOID sayÄ±sÄ±  : {len(common_ids)}")

    if len(common_ids) == 0:
        raise Exception("âŒ GeoJSON ile fr_crime_09.csv arasÄ±nda GEOID eÅŸleÅŸmesi yok.")

    gdf2 = gdf[gdf["GEOID_norm"].isin(common_ids)].copy().set_index("GEOID_norm")
    print("âœ” EÅŸleÅŸen hÃ¼cre sayÄ±sÄ±:", len(gdf2), "/", len(geojson_ids))

    print("â³ Queen & Rook mekÃ¢nsal aÄŸÄ±rlÄ±k matrisleri oluÅŸturuluyor...")
    W_queen = Queen.from_dataframe(gdf2)
    W_queen.transform = "r"
    W_rook = Rook.from_dataframe(gdf2)
    W_rook.transform = "r"

    print("âœ” Queen neighbors (Ã¶rnek):", dict(list(W_queen.neighbors.items())[:3]))
    print("âœ” Rook neighbors  (Ã¶rnek):", dict(list(W_rook.neighbors.items())[:3]))
    print("ğŸ“Œ MekÃ¢nsal komÅŸuluk yapÄ±sÄ± hazÄ±r (STARIMA vb. iÃ§in kullanÄ±labilir).")

    # ============================================================
    # 5) STACKING ML MODEL (FULL FEATURE SET, Son FR_ML_DAYS gÃ¼n)
    # ============================================================
    ML_DAYS = int(os.environ.get("FR_ML_DAYS", "365"))
    max_date = df["date"].max()
    cutoff_ml = max_date - pd.Timedelta(days=ML_DAYS)

    df_ml = df[df["date"] >= cutoff_ml].copy()
    df_ml = df_ml.dropna(subset=["Y_label"]).reset_index(drop=True)

    print(f"\nğŸ§ª ML iÃ§in kullanÄ±lan tarih aralÄ±ÄŸÄ±: {cutoff_ml.date()} â†’ {max_date.date()}")
    print(f"ğŸ“ ML veri satÄ±r sayÄ±sÄ± (son {ML_DAYS} gÃ¼n): {len(df_ml)}")

    pos = df_ml[df_ml["Y_label"] == 1]
    neg = df_ml[df_ml["Y_label"] == 0]

    NEG_FRAC = float(os.environ.get("FR_NEG_FRAC", "0.3"))
    if len(neg) > 0 and NEG_FRAC > 0:
        neg_sample = neg.sample(frac=NEG_FRAC, random_state=42)
        df_train = pd.concat([pos, neg_sample]).sample(frac=1.0, random_state=42)
    else:
        df_train = pos.copy()

    print(
        f"ğŸ” Denge sonrasÄ±: pozitif={ (df_train['Y_label']==1).sum() }, "
        f"negatif={ (df_train['Y_label']==0).sum() }"
    )

    y = df_train["Y_label"]
    drop_cols = ["Y_label", "date"]
    X = df_train.drop(columns=[c for c in drop_cols if c in df_train.columns])

    num_cols = X.select_dtypes(include=np.number).columns
    cat_cols = X.select_dtypes(include="object").columns

    print(f"ğŸ”¢ Feature sayÄ±sÄ± â†’ numeric={len(num_cols)}, categorical={len(cat_cols)}")

    preprocess = ColumnTransformer([
        ("num", Pipeline([
            ("imp", SimpleImputer(strategy="median")),
            ("sc", StandardScaler())
        ]), num_cols),
        ("cat", Pipeline([
            ("imp", SimpleImputer(strategy="most_frequent")),
            ("ohe", OneHotEncoder(handle_unknown="ignore"))
        ]), cat_cols)
    ])

    estimators = [
        (
            "rf",
            RandomForestClassifier(
                n_estimators=80,
                max_depth=10,
                n_jobs=-1,
                random_state=42,
            ),
        ),
        (
            "et",
            ExtraTreesClassifier(
                n_estimators=80,
                max_depth=10,
                n_jobs=-1,
                random_state=42,
            ),
        ),
        (
            "xgb",
            XGBClassifier(
                n_estimators=150,
                learning_rate=0.07,
                max_depth=4,
                subsample=0.8,
                colsample_bytree=0.8,
                objective="binary:logistic",
                eval_metric="logloss",
                n_jobs=-1,
                random_state=42,
            ),
        ),
    ]

    meta = LogisticRegression(max_iter=300)

    stack_model = Pipeline([
        ("prep", preprocess),
        ("stack", StackingClassifier(
            estimators=estimators,
            final_estimator=meta,
            cv=3,
            n_jobs=-1,
            passthrough=False,
        )),
    ])

    print("\nâ³ STACKING modeli (hÄ±zlÄ± mod) eÄŸitiliyor...")
    stack_model.fit(X, y)
    print("âœ” STACKING model fit edildi (hÄ±zlÄ± mod).")

    # ============================================================
    # 6) EXOGENOUS EFFECTS (RF IMPORTANCE)
    # ============================================================
    print("\nâ³ RandomForest ile numerik feature importance hesaplanÄ±yor...")
    rf_imp = RandomForestClassifier(
        n_estimators=200,
        max_depth=10,
        n_jobs=-1,
        random_state=42,
    )
    rf_imp.fit(X[num_cols], y)

    imp_norm = rf_imp.feature_importances_ / rf_imp.feature_importances_.sum()
    effects = dict(zip(num_cols, imp_norm))

    print("âœ” Exogenous feature etkileri hesaplandÄ±.")

    # ============================================================
    # 7) FEATURE IMPORTANCE CSV + MODEL & FEATURE LIST KAYIT
    # ============================================================
    feat_imp_df = pd.DataFrame({
        "feature": num_cols,
        "importance": [effects.get(c, 0.0) for c in num_cols],
    }).sort_values("importance", ascending=False)

    feat_imp_path = output_dir / "feature_importances_stacking_fr09.csv"
    feat_imp_df.to_csv(feat_imp_path, index=False)

    print("\nğŸ’¾ Feature importance kaydedildi:", feat_imp_path)
    print("ğŸ” En Ã¶nemli ilk 20 feature:")
    print(feat_imp_df.head(20))

    model_path = output_dir / "model_stacking_fr09.pkl"
    feat_path = output_dir / "features_fr09.json"

    joblib.dump(stack_model, model_path)
    print("ğŸ’¾ Model kaydedildi â†’", model_path)

    with open(feat_path, "w", encoding="utf-8") as f:
        json.dump(X.columns.tolist(), f, indent=2, ensure_ascii=False)
    print("ğŸ’¾ Feature list kaydedildi â†’", feat_path)

    # ============================================================
    # 8) OPSÄ°YONEL: SHAP FEATURE IMPORTANCE
    # ============================================================
    shap_path = output_dir / "shap_feature_importance_fr09.csv"
    try:
        import shap

        print("\nâ³ SHAP TreeExplainer ile numerik feature SHAP deÄŸeri hesaplanÄ±yor...")
        sample_n = min(5000, len(X))
        X_sample = X[num_cols].sample(n=sample_n, random_state=42)

        explainer = shap.TreeExplainer(rf_imp)
        shap_values = explainer.shap_values(X_sample)

        if isinstance(shap_values, list) and len(shap_values) > 1:
            shap_values_pos = shap_values[1]
        else:
            shap_values_pos = shap_values

        mean_abs = np.mean(np.abs(shap_values_pos), axis=0)
        shap_imp_df = pd.DataFrame({
            "feature": num_cols,
            "mean_abs_shap": mean_abs,
        }).sort_values("mean_abs_shap", ascending=False)

        shap_imp_df.to_csv(shap_path, index=False)
        print("ğŸ’¾ SHAP feature importance kaydedildi:", shap_path)
        print("ğŸ” SHAP'a gÃ¶re ilk 20 feature:")
        print(shap_imp_df.head(20))

    except ImportError:
        print("âš ï¸ 'shap' paketi yÃ¼klÃ¼ deÄŸil, SHAP analizi atlandÄ±.")
    except Exception as e:
        print(f"âš ï¸ SHAP analizi sÄ±rasÄ±nda hata oluÅŸtu: {e}")

    print("\nâœ… fr_crime_09 iÃ§in feature analysis + stacking + (opsiyonel) SHAP aÅŸamasÄ± TAMAMLANDI.")


if __name__ == "__main__":
    main()
