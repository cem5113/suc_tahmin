name: Full SF Crime Pipeline

on:
  schedule:
    - cron: "0 14,15 * * *"   # SF 07:00: 14:00 UTC (yaz), 15:00 UTC (kƒ±≈ü)
  workflow_dispatch:
    inputs:
      persist:
        description: "Sonu√ßlarƒ± nasƒ±l saklayalƒ±m?"
        type: choice
        options: [artifact, commit, none]
        default: artifact
      force:
        description: "Manuel tetiklemede 07:00 kapƒ±sƒ±nƒ± YOK SAY"
        type: boolean
        default: true
      top_k:
        description: "Stacking: her saat dilimi i√ßin √∂nerilecek GEOID sayƒ±sƒ±"
        default: "50"

permissions:
  actions: read
  contents: write

concurrency:
  group: full-pipeline-${{ github.ref }}
  cancel-in-progress: true

env:
  CRIME_DATA_DIR: crime_prediction_data
  GEOID_LEN: "11"

  BACKFILL_DAYS: "0"
  SF_SODA_FETCH_STRATEGY: ${{ vars.SF_SODA_FETCH_STRATEGY || 'bulk' }}
  SF_SODA_PAGE_LIMIT:    ${{ vars.SF_SODA_PAGE_LIMIT    || '50000' }}
  SF_SODA_MAX_PAGES:     ${{ vars.SF_SODA_MAX_PAGES     || '100' }}

  SF911_API_URL:       ${{ vars.SF911_API_URL       || 'https://data.sfgov.org/resource/2zdj-bwza.json' }}
  SF911_AGENCY_FILTER: ${{ vars.SF911_AGENCY_FILTER || 'agency like "%Police%"' }}
  SF911_API_TOKEN:     ${{ secrets.SF911_API_TOKEN }}
  SOCS_APP_TOKEN:      ${{ secrets.SOCS_APP_TOKEN }}

  PATROL_TOP_K: ${{ github.event.inputs.top_k || '50' }}

  PATROL_HORIZON_DAYS: "3"
  WX_LOCATION: "san francisco"
  WX_UNIT: "us"

  ACS_YEAR: ${{ vars.ACS_YEAR || 'LATEST' }}
  DEMOG_WHITELIST: ${{ vars.DEMOG_WHITELIST || '' }}
  CENSUS_GEO_LEVEL: ${{ vars.CENSUS_GEO_LEVEL || 'auto' }}

jobs:
  run:
    runs-on: ubuntu-latest
    steps:

      - name: Checkout (with LFS)
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Ensure LFS objects
        run: |
          git lfs install
          git lfs pull
          echo "Repo root:"; ls -lah
          echo "crime_prediction_data:"; ls -lah crime_prediction_data || true
          echo "CRIME_DATA_DIR:"; ls -lah "${CRIME_DATA_DIR}" || true

      - name: Ensure data dir
        run: mkdir -p "${CRIME_DATA_DIR}"

      # --- SF 07:00 kapƒ±sƒ± (TZ ayarla + saat kontrol√º) ---
      - name: Set runner timezone to America/Los_Angeles
        uses: szenius/set-timezone@v2.0
        with:
          timezoneLinux: "America/Los_Angeles"

      - name: Gate by local SF time == 07 (bypassable)
        id: gate
        run: |
          now="$(date)"
          echo "Runner local time: $now"
          echo "RUN_LOCAL_TIME=$now" >> $GITHUB_ENV
          # Manuel + force=true ise doƒürudan ge√ß
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ "${{ github.event.inputs.force }}" = "true" ]; then
            echo "proceed=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          # Aksi halde 07:00 kontrol√º
          if [ "$(date +%H)" = "07" ]; then
            echo "proceed=true" >> $GITHUB_OUTPUT
          else
            echo "proceed=false" >> $GITHUB_OUTPUT
          fi

      - name: Skip summary (outside 07:00 and not forced)
        if: ${{ steps.gate.outputs.proceed != 'true' }}
        run: |
          {
            echo "## SF Crime Pipeline"
            echo ""
            echo "- √áalƒ±≈üma zamanƒ± (SF): **$(date)**"
            echo "- Not: 07:00 kapƒ±sƒ± nedeniyle adƒ±mlar atlandƒ±. Manuel tetiklerken \`force=true\` verin."
          } >> $GITHUB_STEP_SUMMARY

      - name: System deps for rtree (optional)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: sudo apt-get update && sudo apt-get install -y libspatialindex-dev

      - name: Set up Python 3.11
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install deps
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python -m pip install -U pip wheel setuptools
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          pip install -U "geopandas==1.0.1" "shapely==2.0.4" "pyproj==3.6.1" "pyogrio==0.9.0" "rtree==1.3.0"

      - name: Geo stack smoke test
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python - <<'PY'
          import geopandas, shapely, pyproj, pyogrio, pandas
          print("geopandas", geopandas.__version__)
          print("shapely", shapely.__version__)
          print("pyproj", pyproj.__version__)
          print("pyogrio", pyogrio.__version__)
          print("pandas", pandas.__version__)
          PY

      # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
      # Prefetch sf_crime_y.csv (GH_TOKEN set ‚Üí login YOK)
      - name: 00) Prefetch sf_crime_y.csv from previous successful run (artifact)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GH_TOKEN: ${{ github.token }}              # gh CLI bunu otomatik kullanƒ±r; login YOK
          ARTIFACT_NAME: sf-crime-pipeline-output
          WORKFLOW_NAME: Full SF Crime Pipeline
          TARGET_FILE: sf_crime_y.csv
          OUT_DIR: crime_prediction_data
        run: |
          set -euo pipefail

          mkdir -p "${OUT_DIR}"
          if [ -f "${OUT_DIR}/${TARGET_FILE}" ]; then
            echo "‚ÑπÔ∏è ${OUT_DIR}/${TARGET_FILE} zaten var, indirme atlandƒ±."
            exit 0
          fi

          sudo apt-get update -y >/dev/null
          sudo apt-get install -y unzip >/dev/null

          if ! command -v gh >/dev/null; then
            type -p curl >/dev/null && sudo curl -sSL https://raw.githubusercontent.com/cli/cli/trunk/script/install.sh | sh
          fi
          gh --version || true
          gh auth status || true

          mkdir -p _prev
          FOUND=""

          # Son 10 ba≈üarƒ±lƒ± run i√ßinde ara; ilk bulduƒüunu kopyala
          RUNS=$(gh run list -R "${GITHUB_REPOSITORY}" \
                  --workflow "${WORKFLOW_NAME}" --status success \
                  -L 10 --json databaseId -q '.[].databaseId' || true)

          if [ -z "${RUNS}" ]; then
            echo "‚ÑπÔ∏è Ba≈üarƒ±lƒ± ge√ßmi≈ü run bulunamadƒ±; release fallback devreye girecek."
            exit 0
          fi

          for RID in ${RUNS}; do
            echo "üîé Run #$RID i√ßinde ${TARGET_FILE} aranƒ±yor‚Ä¶"
            rm -rf "_prev/$RID" && mkdir -p "_prev/$RID"
            if gh run download -R "${GITHUB_REPOSITORY}" "$RID" \
                 -n "${ARTIFACT_NAME}" -D "_prev/$RID" >/dev/null 2>&1; then
              F=$(find "_prev/$RID" -type f -iname "${TARGET_FILE}" | head -n1 || true)
              if [ -n "${F:-}" ]; then
                cp -f "$F" "${OUT_DIR}/${TARGET_FILE}"
                echo "‚úÖ Bulundu ve kopyalandƒ±: $F ‚Üí ${OUT_DIR}/${TARGET_FILE}"
                FOUND="yes"
                break
              fi
            else
              echo "  ‚Ü™Ô∏è Artifact indirilemedi veya yok (#$RID)."
            fi
          done

          if [ -z "${FOUND}" ]; then
            echo "‚ÑπÔ∏è ${TARGET_FILE} √∂nceki artifact‚Äôlarda bulunamadƒ±; update_crime.py release fallback kullanacak."
          fi
      # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

      # ---- PIPELINE ----
      - name: 01) Su√ß tabanƒ± ve grid ‚Üí sf_crime.csv + gridler
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -u update_crime.py

      - name: Restore 911 summary cache
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ env.CRIME_DATA_DIR }}/sf_911_last_5_year.csv
          key: 911-${{ runner.os }}-${{ github.run_number }}
          restore-keys: |
            911-${{ runner.os }}-

      # ‚úÖ GH auth: login YOK (GH_TOKEN var)
      - name: Prefetch sf_911_last_5_year_y.csv from previous successful run (artifact)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GH_TOKEN: ${{ github.token }}   # gh i√ßin non-interactive auth
          ARTIFACT_NAME: sf-crime-pipeline-output
          WORKFLOW_NAME: Full SF Crime Pipeline
          TARGET_FILE: sf_911_last_5_year_y.csv
          OUT_DIR: crime_prediction_data
        run: |
          set -euo pipefail

          mkdir -p "${OUT_DIR}"
          if [ -f "${OUT_DIR}/${TARGET_FILE}" ]; then
            echo "‚ÑπÔ∏è ${OUT_DIR}/${TARGET_FILE} zaten var, indirme atlandƒ±."
            exit 0
          fi

          sudo apt-get update -y >/dev/null
          sudo apt-get install -y unzip >/dev/null

          if ! command -v gh >/dev/null; then
            type -p curl >/dev/null && sudo curl -sSL https://raw.githubusercontent.com/cli/cli/trunk/script/install.sh | sh
          fi
          gh --version || true
          gh auth status || true

          mkdir -p _prev
          FOUND=""
          for RID in $(gh run list -R "${GITHUB_REPOSITORY}" \
                        --workflow "${WORKFLOW_NAME}" --status success \
                        -L 10 --json databaseId -q '.[].databaseId'); do
            echo "üîé Run #$RID i√ßinde ${TARGET_FILE} aranƒ±yor‚Ä¶"
            rm -rf "_prev/$RID" && mkdir -p "_prev/$RID"
            if gh run download -R "${GITHUB_REPOSITORY}" "$RID" \
                 -n "${ARTIFACT_NAME}" -D "_prev/$RID" >/dev/null 2>&1; then
              F=$(find "_prev/$RID" -type f -iname "${TARGET_FILE}" | head -n1 || true)
              if [ -n "${F:-}" ]; then
                cp -f "$F" "${OUT_DIR}/${TARGET_FILE}"
                echo "‚úÖ Bulundu ve kopyalandƒ±: $F ‚Üí ${OUT_DIR}/${TARGET_FILE}"
                FOUND="yes"
                break
              fi
            else
              echo "  ‚Ü™Ô∏è Artifact indirilemedi veya yok (#$RID)."
            fi
          done

          if [ -z "${FOUND}" ]; then
            echo "‚ÑπÔ∏è ${TARGET_FILE} √∂nceki artifact‚Äôlarda bulunamadƒ±; script release fallback kullanacak."
          fi

      # 911: g√ºn g√ºn yerine bulk (limit=50k)
      - name: 02) 911 ‚Üí sf_crime_01.csv (bulk 50k)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          SF_SODA_FETCH_STRATEGY: ${{ env.SF_SODA_FETCH_STRATEGY }}
          SF_SODA_PAGE_LIMIT:    ${{ env.SF_SODA_PAGE_LIMIT }}
          SF_SODA_MAX_PAGES:     ${{ env.SF_SODA_MAX_PAGES }}
          SF911_API_URL:         ${{ env.SF911_API_URL }}
          SF911_AGENCY_FILTER:   ${{ env.SF911_AGENCY_FILTER }}
          SF911_API_TOKEN:       ${{ env.SF911_API_TOKEN }}
          SOCS_APP_TOKEN:        ${{ env.SOCS_APP_TOKEN }}
          BACKFILL_DAYS:         ${{ env.BACKFILL_DAYS }}
        run: |
          set -e
          python -u update_911.py

      - name: Restore 311 summary cache
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ env.CRIME_DATA_DIR }}/sf_311_last_5_years.csv
          key: 311-${{ runner.os }}-${{ github.run_number }}
          restore-keys: |
            311-${{ runner.os }}-

      # ‚úÖ YENƒ∞: 311 √∂zet dosyasƒ± i√ßin artifact prefetch (GH login YOK)
      - name: Prefetch sf_311_last_5_years_y.csv from previous successful run (artifact)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GH_TOKEN: ${{ github.token }}
          ARTIFACT_NAME: sf-crime-pipeline-output
          WORKFLOW_NAME: Full SF Crime Pipeline
          TARGET_FILE: sf_311_last_5_years_y.csv
          OUT_DIR: crime_prediction_data
        run: |
          set -euo pipefail

          mkdir -p "${OUT_DIR}"
          if [ -f "${OUT_DIR}/${TARGET_FILE}" ]; then
            echo "‚ÑπÔ∏è ${OUT_DIR}/${TARGET_FILE} zaten var, indirme atlandƒ±."
            exit 0
          fi

          sudo apt-get update -y >/dev/null
          sudo apt-get install -y unzip >/dev/null

          if ! command -v gh >/dev/null; then
            type -p curl >/dev/null && sudo curl -sSL https://raw.githubusercontent.com/cli/cli/trunk/script/install.sh | sh
          fi
          gh --version || true
          gh auth status || true

          mkdir -p _prev
          FOUND=""
          for RID in $(gh run list -R "${GITHUB_REPOSITORY}" \
                        --workflow "${WORKFLOW_NAME}" --status success \
                        -L 10 --json databaseId -q '.[].databaseId'); do
            echo "üîé Run #$RID i√ßinde ${TARGET_FILE} aranƒ±yor‚Ä¶"
            rm -rf "_prev/$RID" && mkdir -p "_prev/$RID"
            if gh run download -R "${GITHUB_REPOSITORY}" "$RID" \
                 -n "${ARTIFACT_NAME}" -D "_prev/$RID" >/dev/null 2>&1; then
              F=$(find "_prev/$RID" -type f -iname "${TARGET_FILE}" | head -n1 || true)
              if [ -n "${F:-}" ]; then
                cp -f "$F" "${OUT_DIR}/${TARGET_FILE}"
                echo "‚úÖ Bulundu ve kopyalandƒ±: $F ‚Üí ${OUT_DIR}/${TARGET_FILE}"
                FOUND="yes"
                break
              fi
            else
              echo "  ‚Ü™Ô∏è Artifact indirilemedi veya yok (#$RID)."
            fi
          done

          if [ -z "${FOUND}" ]; then
            echo "‚ÑπÔ∏è ${TARGET_FILE} √∂nceki artifact‚Äôlarda bulunamadƒ±; update_311.py kendi akƒ±≈üƒ±yla devam edecek."
          fi

      # 311: aynƒ± ≈üekilde bulk √ßekim
      - name: 03) 311 ‚Üí sf_crime_02.csv (bulk 50k)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          SF_SODA_FETCH_STRATEGY: ${{ env.SF_SODA_FETCH_STRATEGY }}
          SF_SODA_PAGE_LIMIT:    ${{ env.SF_SODA_PAGE_LIMIT }}
          SF_SODA_MAX_PAGES:     ${{ env.SF_SODA_MAX_PAGES }}
          SOCS_APP_TOKEN:        ${{ env.SOCS_APP_TOKEN }}
          BACKFILL_DAYS:         ${{ env.BACKFILL_DAYS }}
        run: |
          set -e
          if [ -f update_311.py ]; then
            python -u update_311.py
          else
            python -u scripts/update_311.py
          fi

      - name: Ensure population CSV (local file)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          mkdir -p "${CRIME_DATA_DIR}"
          DEST="${CRIME_DATA_DIR}/sf_population.csv"
          CANDIDATES=(
            "${CRIME_DATA_DIR}/sf_population.csv"
            "sf_population.csv"
            "data/sf_population.csv"
            "inputs/sf_population.csv"
          )
          FOUND=""
          for p in "${CANDIDATES[@]}"; do
            if [ -f "$p" ]; then
              if [ -e "$DEST" ] && [ "$p" -ef "$DEST" ]; then
                echo "‚ÑπÔ∏è Zaten hedefte: $DEST"
              else
                cp -f "$p" "$DEST"
                echo "‚úÖ Copied: $p ‚Üí $DEST"
              fi
              FOUND="yes"
              break
            fi
          done
          if [ -z "$FOUND" ]; then
            echo "‚ùå sf_population.csv bulunamadƒ±. L√ºtfen repo‚Äôya ekleyin veya ${CRIME_DATA_DIR} altƒ±na yerle≈ütirin."
            exit 2
          fi
          echo "---- sf_population.csv (head) ----"
          head -n 5 "$DEST" || true

      - name: (Opsiyonel) sf_population.csv ba≈ülƒ±k normalize
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python - <<'PY'
          import os, pandas as pd
          p = os.path.join(os.environ["CRIME_DATA_DIR"], "sf_population.csv")
          df = pd.read_csv(p)
          low = {c.lower(): c for c in df.columns}

          # GEOID standardizasyonu
          if 'geoid' not in low and 'geography_id' in low:
              df.rename(columns={low['geography_id']: 'GEOID'}, inplace=True)

          # N√ºfus s√ºtununu 'population' yap
          for cand in ['population','total_population','b01003_001e','estimate','total','value']:
              if cand in low:
                  if 'population' not in df.columns:
                      df.rename(columns={low[cand]: 'population'}, inplace=True)
                  break

          df.to_csv(p, index=False)
          print("Normalized headers:", df.columns.tolist())
          PY

      - name: 04) N√ºfus ‚Üí sf_crime_03.csv (demografi + n√ºfus)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          POPULATION_PATH:  ${{ env.CRIME_DATA_DIR }}/sf_population.csv
          CENSUS_GEO_LEVEL: ${{ env.CENSUS_GEO_LEVEL }}
          ACS_YEAR:         ${{ env.ACS_YEAR }}
          DEMOG_WHITELIST:  ${{ env.DEMOG_WHITELIST }}
        run: |
          set -e
          echo "---- update_population.py (ilk 30 satƒ±r) ----"
          sed -n '1,30p' update_population.py || true
          python -u update_population.py

      - name: 05) Otob√ºs ‚Üí sf_crime_04.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -u update_bus.py

      - name: 06) Tren (BART) ‚Üí sf_crime_05.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -u update_train.py

      - name: 07) POI zenginle≈ütirme ‚Üí sf_crime_06.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          if [ -f update_poi.py ]; then python -u update_poi.py;
          elif [ -f pipeline_make_sf_crime_06.py ]; then python -u pipeline_make_sf_crime_06.py;
          else echo "POI adƒ±mƒ± bulunamadƒ±"; exit 2; fi

      - name: 08) Polis & Devlet binalarƒ± ‚Üí sf_crime_07.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          if [ -f update_police_gov.py ]; then python -u update_police_gov.py;
          elif [ -f scripts/enrich_police_gov_06_to_07.py ]; then python -u scripts/enrich_police_gov_06_to_07.py;
          else echo "Polis/Gov adƒ±mƒ± bulunamadƒ±"; exit 2; fi

      # 09) Hava durumu ‚Üí sadece ar≈üiv (sf_weather_5years.csv)
      - name: 09) Hava durumu ‚Üí sf_weather_5years.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GITHUB_TOKEN: ${{ github.token }}         # update_weather.py bunu okuyor
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }} # √ßƒ±ktƒ± burada yazƒ±lacak
        run: |
          set -euo pipefail
          if [ -f update_weather.py ]; then
            python -u update_weather.py
          elif [ -f scripts/update_weather.py ]; then
            python -u scripts/update_weather.py
          else
            echo "‚ùå Weather script not found"; exit 2
          fi
          
          test -f "sf_weather_5years.csv" || { echo "‚ùå Beklenen √ßƒ±ktƒ± yok: sf_weather_5years.csv"; exit 2; }
          mkdir -p "${CRIME_DATA_DIR}"
          cp -f sf_weather_5years.csv "${CRIME_DATA_DIR}/sf_weather_5years.csv"
          echo "sf_weather_5years.csv ‚Äî ilk 5 satƒ±r:"
          head -n 5 "${CRIME_DATA_DIR}/sf_weather_5years.csv" || true
          echo "sf_weather_5years.csv ‚Äî satƒ±r sayƒ±sƒ±:"
          wc -l "${CRIME_DATA_DIR}/sf_weather_5years.csv" || true
      
      # (Opsiyonel) Grid √∂zetini normalize et (kom≈üuluk/08/09'dan baƒüƒ±msƒ±z)
      - name: Normalize grid outputs (put under CRIME_DATA_DIR)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          mkdir -p "${CRIME_DATA_DIR}"
          CANDIDATES=(
            "sf_crime_grid_full_labeled.csv"
            "data/sf_crime_grid_full_labeled.csv"
            "outputs/sf_crime_grid_full_labeled.csv"
            "${CRIME_DATA_DIR}/sf_crime_grid_full_labeled.csv"
          )
          FOUND=""
          for p in "${CANDIDATES[@]}"; do
            if [ -f "$p" ]; then
              cp -f "$p" "${CRIME_DATA_DIR}/sf_crime_grid_full_labeled.csv"
              echo "‚úÖ Grid kopyalandƒ±: $p ‚Üí ${CRIME_DATA_DIR}/sf_crime_grid_full_labeled.csv"
              FOUND="yes"
              break
            fi
          done
          if [ -z "$FOUND" ]; then
            echo "‚ö†Ô∏è sf_crime_grid_full_labeled.csv bulunamadƒ±; stacking bazƒ± g√∂r√ºn√ºmleri atlayabilir."
          fi
      
      # 10) neighbors.csv (kullan/normalize/√ºret + kalite kontrol)
      - name: 10) neighbors.csv (kullan/normalize/√ºret + kalite kontrol)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          STACKING_DATASET: ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.csv
          NEIGHBOR_FILE: ${{ env.CRIME_DATA_DIR }}/neighbors.csv
          NEIGHBOR_STRATEGY: queen         # queen|rook
          MIN_NEIGHBOR_DEG: "3"            # en az derece
          KNN_K: "5"                       # takviye i√ßin k
          NEIGHBOR_POLY: ${{ env.CRIME_DATA_DIR }}/sf_grid.geojson
        run: |
          set -euo pipefail
          DEST="${CRIME_DATA_DIR}/neighbors.csv"
          mkdir -p "${CRIME_DATA_DIR}"
      
          echo "‚Üí hedef: $DEST"
          # 1) Varsa repodan kopyala
          for p in \
            "${CRIME_DATA_DIR}/neighbors.csv" \
            "crime_prediction_data/neighbors.csv" \
            "neighbors.csv" "data/neighbors.csv" "inputs/neighbors.csv"
          do
            if [ -f "$p" ]; then
              if [ -e "$DEST" ] && [ "$p" -ef "$DEST" ]; then
                echo "‚ÑπÔ∏è Zaten hedefte: $DEST"
              else
                cp -f "$p" "$DEST"
                echo "‚úÖ Copied: $p ‚Üí $DEST"
              fi
              break
            fi
          done
      
          # 2) Ba≈ülƒ±klarƒ± normalize et ‚Üí geoid,neighbor
          python - <<'PY'
          import os, pandas as pd, sys, re
          p = os.path.join(os.environ["CRIME_DATA_DIR"], "neighbors.csv")
          try:
              df = pd.read_csv(p, dtype=str)
          except Exception:
              sys.exit(0)
          if df.empty:
              sys.exit(0)
          low = {c.lower(): c for c in df.columns}
          src = low.get("geoid") or low.get("src") or low.get("source")
          dst = (low.get("neighbor") or low.get("neighbor_geoid") or
                 low.get("neighborgeoid") or low.get("dst") or low.get("target"))
          if not src or not dst:
              print("‚ö†Ô∏è neighbors.csv ba≈ülƒ±klarƒ± tanƒ±nmadƒ±:", df.columns.tolist()); sys.exit(0)
          df = df.rename(columns={src: "geoid", dst: "neighbor"})
          L = int(os.environ.get("GEOID_LEN","11"))
          for c in ("geoid","neighbor"):
              df[c] = df[c].astype(str).str.extract(r"(\d+)", expand=False).str.zfill(L)
          # √ßift y√∂nl√º ve tekille
          df = pd.concat([df, df.rename(columns={"geoid":"neighbor","neighbor":"geoid"})], ignore_index=True)
          df = df.dropna().drop_duplicates()
          df = df[df["geoid"] != df["neighbor"]]
          df[["geoid","neighbor"]].to_csv(p, index=False)
          print("Neighbors headers ‚Üí", ["geoid","neighbor"], f"(rows={len(df)})")
          PY
      
          # 3) Kalite kontrol
          python - <<'PY'
          import os, pandas as pd, sys
          from pathlib import Path
          p = Path(os.environ["CRIME_DATA_DIR"]) / "neighbors.csv"
          min_deg = int(os.environ.get("MIN_NEIGHBOR_DEG","3"))
          if not p.exists() or p.stat().st_size == 0:
              need=True; reason="yok/bo≈ü"
          else:
              try:
                  df = pd.read_csv(p, dtype=str)
                  ok = set(df.columns) >= {"geoid","neighbor"} and len(df)>0
                  mindeg = (df.groupby("geoid")["neighbor"].nunique().min() if ok else 0)
                  need = (not ok) or (mindeg is None) or (mindeg < min_deg)
                  reason = f"min_deg={mindeg}<{min_deg}" if ok else "ge√ßersiz ba≈ülƒ±k/bo≈ü"
              except Exception as e:
                  need=True; reason=f"hata:{e}"
          if need:
              Path(os.environ["CRIME_DATA_DIR"]).joinpath(".rebuild_neighbors").write_text(reason)
              print("REBUILD=1 ‚Üí", reason)
          else:
              print("REBUILD=0 ‚Üí yeterli kalite")
          PY
      
          # 4) Yeniden √ºretim gerekiyorsa
          if [ -f "${CRIME_DATA_DIR}/.rebuild_neighbors" ]; then
            if [ ! -f "${NEIGHBOR_POLY}" ]; then
              echo "‚ö†Ô∏è Polygon katmanƒ± yok: ${NEIGHBOR_POLY}. Mevcut neighbors.csv kullanƒ±lacak."
            else
              echo "üîß Kom≈üuluk yeniden olu≈üturuluyor (${NEIGHBOR_STRATEGY}, min_deg=${MIN_NEIGHBOR_DEG}, knn_k=${KNN_K})"
              cat > _rebuild_neighbors.py <<'PY'
          from __future__ import annotations
          import os, re
          from pathlib import Path
          import pandas as pd
          try:
              import geopandas as gpd
          except Exception:
              gpd = None
      
          CRIME_DIR  = Path(os.environ.get("CRIME_DATA_DIR","crime_prediction_data"))
          GEOID_LEN  = int(os.environ.get("GEOID_LEN","11"))
          STRATEGY   = os.environ.get("NEIGHBOR_STRATEGY","queen").lower()
          MIN_DEG    = int(os.environ.get("MIN_NEIGHBOR_DEG","3"))
          KNN_K      = int(os.environ.get("KNN_K","5"))
          OUT_PATH   = Path(os.environ.get("NEIGHBOR_FILE", str(CRIME_DIR/"neighbors.csv")))
          GRID_CSV   = Path(os.environ.get("STACKING_DATASET", str(CRIME_DIR/"sf_crime_grid_full_labeled.csv")))
          POLY_HINT  = os.environ.get("NEIGHBOR_POLY","")
      
          def _norm(s: pd.Series) -> pd.Series:
              return s.astype(str).str.extract(r"(\d+)", expand=False).str.zfill(GEOID_LEN)
          def _sym(df: pd.DataFrame) -> pd.DataFrame:
              rev = df.rename(columns={"geoid":"neighbor","neighbor":"geoid"})
              out = pd.concat([df, rev], ignore_index=True)
              out = out[out["geoid"] != out["neighbor"]]
              return out.drop_duplicates()
          def _neighbors_from_polygons(path: str) -> pd.DataFrame | None:
              if not gpd:
                  return None
              try:
                  gdf = gpd.read_file(path)
              except Exception:
                  return None
              low = {re.sub(r"[^a-z0-9]","", c.lower()): c for c in gdf.columns}
              gcol = low.get("geoid") or low.get("geoid10") or low.get("geographyid") or list(gdf.columns)[0]
              if "geometry" not in gdf.columns:
                  return None
              gdf = gdf[[gcol,"geometry"]].rename(columns={gcol:"geoid"}).copy()
              gdf["geoid"] = _norm(gdf["geoid"])
              gdf = gdf[gdf["geometry"].notna()].reset_index(drop=True)
              pairs = gdf.sindex.query_bulk(gdf.geometry, predicate="touches")
              i, j = pairs
              m = pd.DataFrame({"i": i, "j": j})
              m = m[m["i"] < m["j"]]
              df = pd.DataFrame({
                  "geoid":    gdf.loc[m["i"], "geoid"].to_numpy(),
                  "neighbor": gdf.loc[m["j"], "geoid"].to_numpy(),
              })
              return _sym(df.drop_duplicates())
          def _centroids_table(poly_ok: bool) -> pd.DataFrame | None:
              if poly_ok and gpd:
                  gdf = gpd.read_file(POLY_HINT)
                  low = {re.sub(r"[^a-z0-9]","", c.lower()): c for c in gdf.columns}
                  gcol = low.get("geoid") or low.get("geoid10") or low.get("geographyid") or list(gdf.columns)[0]
                  c = gdf[[gcol,"geometry"]].rename(columns={gcol:"geoid"}).copy()
                  c["geoid"] = _norm(c["geoid"])
                  cent = c.geometry.centroid
                  return pd.DataFrame({"geoid": c["geoid"], "x": cent.x, "y": cent.y})
              if GRID_CSV.exists():
                  df = pd.read_csv(GRID_CSV, dtype=str, low_memory=False)
                  low = {re.sub(r"[^a-z0-9]","", c.lower()): c for c in df.columns}
                  gcol = low.get("geoid") or low.get("geoid10") or low.get("geographyid")
                  lat  = low.get("lat") or low.get("latitude") or low.get("centroidlat")
                  lon  = low.get("lon") or low.get("lng") or low.get("longitude") or low.get("centroidlon")
                  if gcol and lat and lon:
                      t = df[[gcol, lat, lon]].rename(columns={gcol:"geoid", lat:"y", lon:"x"}).copy()
                      t["geoid"] = _norm(t["geoid"])
                      t["x"] = pd.to_numeric(t["x"], errors="coerce")
                      t["y"] = pd.to_numeric(t["y"], errors="coerce")
                      return t.dropna()
              return None
          def _ensure_min_degree(nb: pd.DataFrame, geos: pd.DataFrame) -> pd.DataFrame:
              import numpy as np
              if geos is None or geos.empty:
                  return nb
              deg = nb.groupby("geoid")["neighbor"].nunique()
              need = set(deg.index[deg < MIN_DEG] if not deg.empty else geos["geoid"])
              if not need:
                  return nb
              idx = {g:i for i,g in enumerate(geos["geoid"])}
              XY  = geos[["x","y"]].to_numpy()
              add = []
              for g in need:
                  i = idx[g]
                  d = np.sum((XY - XY[i])**2, axis=1)
                  order = np.argsort(d)
                  picks = []
                  for j in order:
                      gj = geos.iloc[j]["geoid"]
                      if gj == g:
                          continue
                      picks.append(gj)
                      if len(picks) >= KNN_K:
                          break
                  add += [(g, n) for n in picks]
              nb2 = pd.concat([nb, pd.DataFrame(add, columns=["geoid","neighbor"])], ignore_index=True)
              return _sym(nb2.drop_duplicates())
          def main():
              poly_ok = bool(POLY_HINT) and Path(POLY_HINT).exists()
              nb = _neighbors_from_polygons(POLY_HINT) if poly_ok else pd.DataFrame(columns=["geoid","neighbor"])
              cent = _centroids_table(poly_ok)
              nb = _ensure_min_degree(nb, cent)
              if nb.empty: nb = pd.DataFrame(columns=["geoid","neighbor"])
              nb["geoid"]    = _norm(nb["geoid"])
              nb["neighbor"] = _norm(nb["neighbor"])
              nb = _sym(nb).drop_duplicates().sort_values(["geoid","neighbor"])
              OUT_PATH.parent.mkdir(parents=True, exist_ok=True)
              nb.to_csv(OUT_PATH, index=False)
              deg = nb.groupby("geoid")["neighbor"].nunique()
              print(f"neighbors.csv yazƒ±ldƒ± ‚Üí {OUT_PATH} | n_edges={len(nb)} | n_nodes={len(deg)} | min_deg={int(deg.min()) if len(deg) else 0} | mean_deg={round(float(deg.mean()),2) if len(deg) else 0.0}")
          if __name__ == "__main__":
              main()
          PY
              python _rebuild_neighbors.py
              rm -f _rebuild_neighbors.py
              rm -f "${CRIME_DATA_DIR}/.rebuild_neighbors"
            fi
          fi
      
          echo "neighbors.csv (ilk 10 satƒ±r):"
          head -n 10 "${DEST}" || true
      
      # 11) 07 + neighbors ‚Üí sf_crime_08.csv  (GEOID join)
      - name: 11) 07 + neighbors ‚Üí sf_crime_08.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, re, pandas as pd
          from pathlib import Path
      
          CRIME_DIR = os.getenv("CRIME_DATA_DIR","crime_prediction_data")
          L = int(os.getenv("GEOID_LEN","11"))
      
          def _norm(s: pd.Series) -> pd.Series:
              import pandas as pd
              return s.astype(str).str.extract(r"(\d+)", expand=False).str.zfill(L)
      
          p07  = Path(CRIME_DIR) / "sf_crime_07.csv"
          pNB  = Path(CRIME_DIR) / "neighbors.csv"
          pOUT = Path(CRIME_DIR) / "sf_crime_08.csv"
      
          if not p07.exists():
              raise FileNotFoundError(f"{p07} yok (√∂nce 08: Polis&Gov adƒ±mƒ±).")
          if not pNB.exists():
              raise FileNotFoundError(f"{pNB} yok (neighbors adƒ±mƒ± ba≈üarƒ±sƒ±z).")
      
          df7 = pd.read_csv(p07, low_memory=False, dtype=str)
          low7 = {re.sub(r"[^a-z0-9]","", c.lower()): c for c in df7.columns}
          gcol7 = low7.get("geoid") or low7.get("geoid10") or low7.get("geographyid")
          if not gcol7:
              raise SystemExit(f"sf_crime_07.csv i√ßinde GEOID benzeri s√ºtun bulunamadƒ±: {df7.columns.tolist()}")
      
          df7[gcol7] = _norm(df7[gcol7])
      
          nb = pd.read_csv(pNB, dtype=str)
          if set(nb.columns) >= {"geoid","neighbor"}:
              pass
          else:
              raise SystemExit(f"neighbors.csv ba≈ülƒ±klarƒ± beklenmedik: {nb.columns.tolist()}")
      
          nb["geoid"] = _norm(nb["geoid"])
          nb["neighbor"] = _norm(nb["neighbor"])
      
          # Basit kom≈üuluk √∂zelliƒüi: derece (isteƒüe g√∂re geni≈ületilebilir)
          deg = nb.groupby("geoid")["neighbor"].nunique().rename("neighbor_degree").reset_index()
      
          # 07 + degree
          out = df7.merge(deg, left_on=gcol7, right_on="geoid", how="left").drop(columns=["geoid"])
          out["neighbor_degree"] = pd.to_numeric(out["neighbor_degree"], errors="coerce").fillna(0).astype("int16")
      
          out.to_csv(pOUT, index=False)
          print(f"‚úÖ sf_crime_08 hazƒ±r ‚Üí {pOUT} | rows={len(out)} | cols={len(out.columns)}")
          print("üìÑ ƒ∞lk 5 satƒ±r:")
          print(out.head(5).to_string(index=False))
          PY
      
      # 12) 08 ‚Üí 09 normalize (common.py)
      - name: 12) 08 ‚Üí 09 normalize (common.py)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python - <<'PY'
          import os
          import pandas as pd
          from pathlib import Path
          from scripts.common import clean_and_save_crime_09
      
          CRIME_DIR = os.getenv("CRIME_DATA_DIR","crime_prediction_data")
          src = Path(CRIME_DIR) / "sf_crime_08.csv"
          if not src.exists():
              raise FileNotFoundError("sf_crime_08.csv not found in known locations")
          dst = Path(CRIME_DIR) / "sf_crime_09.csv"
      
          clean_and_save_crime_09(str(src), str(dst))
          print("‚úÖ sf_crime_09 hazƒ±r ‚Üí", dst)
          try:
              df = pd.read_csv(dst, low_memory=False)
              with pd.option_context("display.max_columns", None, "display.width", 2000):
                  print("üìÑ sf_crime_09.csv ‚Äî ilk 5 satƒ±r:")
                  print(df.head(5).to_string(index=False))
          except Exception as e:
              print(f"‚ö†Ô∏è √ñnizleme okunamadƒ±: {e}")
          PY
                
      - name: 11) Install ML deps for stacking
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python -m pip install -U pip wheel setuptools
          pip install pandas numpy scikit-learn joblib
          pip install xgboost lightgbm || true

      - name: 12) Run stacking risk pipeline
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          STACKING_DATASET: ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.csv
          ENABLE_SPATIAL_TE: "1"
          TE_ALPHA: "50"
          NEIGHBOR_FILE: ${{ env.CRIME_DATA_DIR }}/neighbors.csv
          ENABLE_TE_ABLATION: "1"
          ABLASYON_BASIS: "ohe"
        run: |
          set -e
          if [ ! -f "${STACKING_DATASET}" ]; then
            echo "‚ùå STACKING_DATASET bulunamadƒ±: ${STACKING_DATASET}"
            exit 2
          fi
          python -u stacking_risk_pipeline.py

      - name: 13) Install extra deps for patrol
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python -m pip install -U requests tzdata

      - name: 14) Produce yarin.csv & week.csv (Visual Crossing)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          VISUAL_CROSSING_API_KEY: ${{ secrets.VISUAL_CROSSING_API_KEY }}
          WX_LOCATION: ${{ env.WX_LOCATION }}         # √∂r: "San Francisco,CA"
          WX_UNIT: ${{ env.WX_UNIT }}                 # "metric" (√∂nerilir) ya da "us"
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}   # √∂r: "crime_data"
          HOT_THRESHOLD_C: "30"                       # is_hot e≈üiƒüi istersen deƒüi≈ütir
        run: |
          set -e
          python - <<'PY'
          import os, io, sys, requests, pandas as pd
          from datetime import datetime, timedelta, date
          from zoneinfo import ZoneInfo
          from urllib.parse import quote
      
          API_KEY = os.getenv("VISUAL_CROSSING_API_KEY", "")
          if not API_KEY:
              sys.exit("Missing VISUAL_CROSSING_API_KEY")
      
          LOCATION = os.getenv("WX_LOCATION", "San Francisco,CA")
          UNIT = os.getenv("WX_UNIT", "metric")  # metric: ¬∞C & mm
          OUTDIR = os.getenv("CRIME_DATA_DIR", "crime_data")
          HOT_THRESHOLD = float(os.getenv("HOT_THRESHOLD_C", "30"))
      
          BASE = "https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline"
      
          # SF yerel tarihleri (VC 'date1/date2' yerel saat kabul eder, biz SF'e g√∂re hesaplƒ±yoruz)
          SF = ZoneInfo("America/Los_Angeles")
          today_sf = datetime.now(SF).date()
          tomorrow = today_sf + timedelta(days=1)
          week_end = tomorrow + timedelta(days=6)  # yarƒ±n dahil 7 g√ºn
      
          def fetch_days_csv(location: str, start: date, end: date) -> pd.DataFrame:
              url = (
                  f"{BASE}/{quote(location)}/{start.isoformat()}/{end.isoformat()}"
                  f"?unitGroup={UNIT}&include=days"
                  f"&elements=datetime,temp,tempmin,tempmax,precip"
                  f"&contentType=csv&key={API_KEY}"
              )
              r = requests.get(url, timeout=60)
              try:
                  r.raise_for_status()
              except requests.HTTPError:
                  print("HTTP status:", r.status_code)
                  print("Response preview:", r.text[:300])
                  raise
              df = pd.read_csv(io.StringIO(r.text))
              need = {"datetime","temp","tempmin","tempmax","precip"}
              miss = need - set(df.columns)
              if miss:
                  raise RuntimeError(f"Beklenen kolon(lar) yok: {miss} | Gelenler: {list(df.columns)}")
              # yeniden adlandƒ±r
              df = df.rename(columns={
                  "datetime":"date","temp":"tavg","tempmin":"tmin","tempmax":"tmax","precip":"prcp"
              })
              # g√ºn adƒ± (TR)
              tr_days = {0:"Pazartesi",1:"Salƒ±",2:"√áar≈üamba",3:"Per≈üembe",4:"Cuma",5:"Cumartesi",6:"Pazar"}
              dd = pd.to_datetime(df["date"], errors="coerce")
              df["day"] = dd.dt.weekday.map(tr_days)
              # t√ºretilmi≈üler
              df["temp_range"] = df["tmax"] - df["tmin"]
              df["is_rainy"] = (df["prcp"] > 0).astype(int)
              df["is_hot"] = (df["tmax"] >= HOT_THRESHOLD).astype(int)
              # kolon sƒ±rasƒ±
              return df[["date","tavg","tmin","tmax","prcp","temp_range","day","is_rainy","is_hot"]]
      
          os.makedirs(OUTDIR, exist_ok=True)
      
          # yarƒ±n (tek g√ºn)
          df_t = fetch_days_csv(LOCATION, tomorrow, tomorrow)
          df_t.to_csv(os.path.join(OUTDIR, "yarin.csv"), index=False, encoding="utf-8")
          print("‚úì", os.path.join(OUTDIR, "yarin.csv"))
      
          # hafta (7 g√ºn)
          df_w = fetch_days_csv(LOCATION, tomorrow, week_end)
          # g√ºvenlik: tam 7 g√ºn deƒüilse (model verememi≈üse) yine ne geldiyse yaz
          df_w.to_csv(os.path.join(OUTDIR, "week.csv"), index=False, encoding="utf-8")
          print("‚úì", os.path.join(OUTDIR, "week.csv"))
          PY
      
          echo "---- yarin.csv (head) ----"; head -n 5 "${CRIME_DATA_DIR}/yarin.csv" || true
          echo "---- week.csv (head) ----";  head -n 5 "${CRIME_DATA_DIR}/week.csv" || true

      - name: 15) Run post_patrol.py (uses risk_hourly + yarin/week)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          PATROL_TOP_K:   ${{ env.PATROL_TOP_K }}
          PATROL_HORIZON_DAYS: ${{ env.PATROL_HORIZON_DAYS }}
        run: |
          set -e
          test -f "${CRIME_DATA_DIR}/risk_hourly.csv" || { echo "‚ùå risk_hourly.csv bulunamadƒ± (stacking √ßƒ±ktƒ±larƒ±nƒ± kontrol et)"; exit 2; }
          python -u scripts/post_patrol.py
          echo "---- patrol_recs_multi.csv (head) ----"
          head -n 20 "${CRIME_DATA_DIR}/patrol_recs_multi.csv" || true

      - name: 16) Quick preview (stacking outputs)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          echo "risk_hourly* (ilk 5)"
          head -n 5 ${CRIME_DATA_DIR}/risk_hourly*.csv 2>/dev/null || true
          echo "patrol_recs* (ilk 5)"
          head -n 5 ${CRIME_DATA_DIR}/patrol_recs*.csv 2>/dev/null || true
          echo "metrics_* (ilk 5)"
          head -n 5 ${CRIME_DATA_DIR}/metrics_*.csv 2>/dev/null || true

      # 17) CSV ‚Üí Parquet i√ßin baƒüƒ±mlƒ±lƒ±klar
      - name: Install converter deps
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python -m pip install -U pip
          pip install --upgrade polars pyarrow

      # CSV ‚Üí Parquet d√∂n√º≈ü√ºm√º
      - name: Convert CSV ‚Üí Parquet
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          mkdir -p parquet_out
          python scripts/csv_to_parquet.py \
            --input "${{ env.CRIME_DATA_DIR }}/" \
            --output parquet_out/ \
            --compression zstd \
            --stats

      # Parquet'leri ayrƒ± artifact olarak y√ºkle
      - name: Upload Parquet artifact (opsiyonel)
        if: ${{ steps.gate.outputs.proceed == 'true' && (github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.persist == 'artifact')) }}
        uses: actions/upload-artifact@v4
        with:
          name: sf-crime-parquet
          path: parquet_out/
          if-no-files-found: warn

      # ---- √áIKTILAR ----
      - name: Upload artifact (varsayƒ±lan)
        if: ${{ steps.gate.outputs.proceed == 'true' && (github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.persist == 'artifact')) }}
        uses: actions/upload-artifact@v4
        with:
          name: sf-crime-pipeline-output
          path: |
            ${{ env.CRIME_DATA_DIR }}/sf_crime.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_y.csv
            ${{ env.CRIME_DATA_DIR }}/sf_population.csv
            ${{ env.CRIME_DATA_DIR }}/neighbors.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_09.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_08.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_07.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_06.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_05.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_04.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_03.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_02.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_01.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.csv
            ${{ env.CRIME_DATA_DIR }}/sf_311_last_5_years*.csv
            ${{ env.CRIME_DATA_DIR }}/sf_311_last_5_years_y.csv
            ${{ env.CRIME_DATA_DIR }}/sf_911_last_5_year.csv
            ${{ env.CRIME_DATA_DIR }}/sf_911_last_5_year_y.csv
            ${{ env.CRIME_DATA_DIR }}/sf_bus_stops_with_geoid.csv
            ${{ env.CRIME_DATA_DIR }}/sf_train_stops_with_geoid.csv
            ${{ env.CRIME_DATA_DIR }}/sf_pois_cleaned_with_geoid.csv
            ${{ env.CRIME_DATA_DIR }}/sf_weather_5years.csv
            ${{ env.CRIME_DATA_DIR }}/sf_weather_5years_y.csv
            ${{ env.CRIME_DATA_DIR }}/*.geojson
            ${{ env.CRIME_DATA_DIR }}/risk_hourly*.csv
            ${{ env.CRIME_DATA_DIR }}/patrol_recs*.csv
            ${{ env.CRIME_DATA_DIR }}/metrics_*.csv
            ${{ env.CRIME_DATA_DIR }}/models/*.joblib
            ${{ env.CRIME_DATA_DIR }}/ablation_spatial_te.csv
            ${{ env.CRIME_DATA_DIR }}/oof_base_probs*.npz
            ${{ env.CRIME_DATA_DIR }}/yarin.csv
            ${{ env.CRIME_DATA_DIR }}/week.csv
          if-no-files-found: warn
          retention-days: 14

      - name: Commit & push (opsiyonel)
        if: ${{ steps.gate.outputs.proceed == 'true' && github.event_name == 'workflow_dispatch' && github.event.inputs.persist == 'commit' }}
        run: |
          set -e
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add -A
          git commit -m "üîÑ Full pipeline + stacking outputs [skip ci]" || echo "No changes"
          git push || echo "Nothing to push"

      # --- SENDGRID HTTP: Ba≈üarƒ± bildirimi ---
      - name: Notify success (SendGrid HTTP)
        if: ${{ steps.gate.outputs.proceed == 'true' && success() }}
        env:
          SG_KEY: ${{ secrets.SENDGRID_API_KEY }}
        run: |
          if [ -z "$SG_KEY" ]; then echo "‚ùå SENDGRID_API_KEY yok/bo≈ü"; exit 1; fi
          curl -s -X POST https://api.sendgrid.com/v3/mail/send \
            -H "Authorization: Bearer $SG_KEY" \
            -H "Content-Type: application/json" \
            -d @- <<'EOF' -o /dev/stderr -w "\nHTTP:%{http_code}\n"
          {
            "personalizations":[ {"to":[ {"email":"cem5113@hotmail.com"}, {"email":"cemeroglu5113@gmail.com"} ]} ],
            "from":{"email":"${{ secrets.SG_FROM }}","name":"SF Crime Pipeline"},
            "reply_to":{"email":"cem5113@hotmail.com","name":"Cem"},
            "subject":"‚úÖ SF Crime Pipeline + Stacking ba≈üarƒ± (#${{ github.run_number }})",
            "categories":["sf-crime-pipeline","success"],
            "content":[{"type":"text/html","value":"<p>Pipeline ba≈üarƒ±yla tamamlandƒ±.</p><ul><li>Run #: <b>${{ github.run_number }}</b></li><li>Run ID: <b>${{ github.run_id }}</b></li><li>Ref: <code>${{ github.ref }}</code></li><li>Runner local time (SF): <code>${{ env.RUN_LOCAL_TIME }}</code></li></ul><p>√áƒ±ktƒ±lar artifact olarak kaydedildi: <b>sf-crime-pipeline-output</b></p><p>Stacking dahil: <code>risk_hourly*</code>, <code>patrol_recs*</code>, <code>metrics_*</code>, <code>models/*.joblib</code></p><p><a href='${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}'>Log ve detaylar</a></p>"}]
          }
          EOF

      # --- SENDGRID HTTP: Hata bildirimi ---
      - name: Notify failure (SendGrid HTTP)
        if: ${{ steps.gate.outputs.proceed == 'true' && failure() }}
        env:
          SG_KEY: ${{ secrets.SENDGRID_API_KEY }}
        run: |
          if [ -z "$SG_KEY" ]; then echo "‚ùå SENDGRID_API_KEY yok/bo≈ü"; exit 1; fi
          curl -s -X POST https://api.sendgrid.com/v3/mail/send \
            -H "Authorization: Bearer $SG_KEY" \
            -H "Content-Type: application/json" \
            -d @- <<'EOF' -o /dev/stderr -w "\nHTTP:%{http_code}\n"
          {
            "personalizations":[ {"to":[ {"email":"cem5113@hotmail.com"}, {"email":"cemeroglu5113@gmail.com"} ]} ],
            "from":{"email":"${{ secrets.SG_FROM }}","name":"SF Crime Pipeline"},
            "reply_to":{"email":"cem5113@hotmail.com","name":"Cem"},
            "subject":"‚ùå SF Crime Pipeline + Stacking ba≈üarƒ±sƒ±z (#${{ github.run_number }})",
            "categories":["sf-crime-pipeline","failure"],
            "content":[{"type":"text/html","value":"<p>Pipeline ba≈üarƒ±sƒ±z oldu.</p><ul><li>Run #: <b>${{ github.run_number }}</b></li><li>Run ID: <b>${{ github.run_id }}</b></li><li>Ref: <code>${{ github.ref }}</code></li><li>Runner local time (SF): <code>${{ env.RUN_LOCAL_TIME }}</code></li></ul><p><a href='${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}'>Log ve detaylar</a></p>"}]
          }
          EOF

      - name: Write run summary
        if: ${{ steps.gate.outputs.proceed == 'true' && success() }}
        run: |
          {
            echo "## SF Crime Pipeline (Stacking dahil)"
            echo ""
            echo "- √áalƒ±≈üma zamanƒ± (SF): **$(date)**"
            echo "- √áƒ±ktƒ±lar: \`sf_crime_01..09.csv\`, 911/311 √∂zetleri, geojson"
            echo "- Stacking: \`risk_hourly*\`, \`patrol_recs*\`, \`metrics_*\`, \`models/*.joblib\`"
            echo "- Artifact adƒ±: **sf-crime-pipeline-output**"
            echo "- PATROL_TOP_K: **${PATROL_TOP_K}**"
          } >> $GITHUB_STEP_SUMMARY
