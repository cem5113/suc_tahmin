name: Full SF Crime Pipeline

on:
  schedule:
    - cron: "0 14,15 * * *" # SF 07:00: 14:00 UTC (yaz), 15:00 UTC (kƒ±≈ü)
  workflow_dispatch:
    inputs:
      persist:
        description: "Sonu√ßlarƒ± nasƒ±l saklayalƒ±m?"
        type: choice
        options: [artifact, commit, none]
        default: artifact
      force:
        description: "Manuel tetiklemede 07:00 kapƒ±sƒ±nƒ± YOK SAY"
        type: boolean
        default: true
      top_k: 
        description: "Stacking: her saat dilimi i√ßin √∂nerilecek GEOID sayƒ±sƒ±"
        default: "50"

permissions:
  actions: read
  contents: write

concurrency:
  group: full-pipeline-${{ github.ref }}
  cancel-in-progress: true

env:
  CRIME_DATA_DIR: ${{ github.workspace }}
  GEOID_LEN: "11"

  BACKFILL_DAYS: "0"   # ‚Üê Mevcut satƒ±r KALDI; override a≈üaƒüƒ±da yapƒ±lƒ±yor
  SF_SODA_FETCH_STRATEGY: ${{ vars.SF_SODA_FETCH_STRATEGY || 'bulk' }}
  SF_SODA_PAGE_LIMIT: ${{ vars.SF_SODA_PAGE_LIMIT || '50000' }}
  SF_SODA_MAX_PAGES: ${{ vars.SF_SODA_MAX_PAGES || '100' }}

  SF911_API_URL: ${{ vars.SF911_API_URL || 'https://data.sfgov.org/resource/2zdj-bwza.json' }}
  SF911_AGENCY_FILTER: ${{ vars.SF911_AGENCY_FILTER || 'agency like "%Police%"' }}
  SF911_API_TOKEN: ${{ secrets.SF911_API_TOKEN }}
  SOCS_APP_TOKEN: ${{ secrets.SOCS_APP_TOKEN }}

  PATROL_TOP_K: ${{ github.event.inputs.top_k || '50' }}

  PATROL_HORIZON_DAYS: "1"
  WX_LOCATION: "san francisco"
  WX_UNIT: "us"

  ACS_YEAR: ${{ vars.ACS_YEAR || 'LATEST' }}
  DEMOG_WHITELIST: ${{ vars.DEMOG_WHITELIST || '' }}
  CENSUS_GEO_LEVEL: ${{ vars.CENSUS_GEO_LEVEL || 'auto' }}

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout (with LFS)
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Ensure LFS objects
        run: |
          git lfs install
          git lfs pull
          echo "Repo root:"; ls -lah
          echo "crime_prediction_data:"; ls -lah crime_prediction_data || true
          echo "CRIME_DATA_DIR:"; ls -lah "${CRIME_DATA_DIR}" || true

      - name: Ensure data dir
        run: mkdir -p "${CRIME_DATA_DIR}"

      - name: Echo paths
        run: |
          pwd
          echo "CRIME_DATA_DIR=${CRIME_DATA_DIR}"
          ls -lah "${CRIME_DATA_DIR}" || true

      # --- SF 07:00 kapƒ±sƒ± (TZ ayarla + saat kontrol√º) ---
      - name: Set runner timezone to America/Los_Angeles
        uses: szenius/set-timezone@v2.0
        with:
          timezoneLinux: "America/Los_Angeles"

      - name: Gate by local SF time == 07 (bypassable)
        id: gate
        run: |
          now="$(date)"
          echo "Runner local time: $now"
          echo "RUN_LOCAL_TIME=$now" >> $GITHUB_ENV
          # Manuel + force=true ise doƒürudan ge√ß
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ "${{ github.event.inputs.force }}" = "true" ]; then
            echo "proceed=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          # Aksi halde 07:00 kontrol√º
          if [ "$(date +%H)" = "07" ]; then
            echo "proceed=true" >> $GITHUB_OUTPUT
          else
            echo "proceed=false" >> $GITHUB_OUTPUT
          fi

      - name: Skip summary (outside 07:00 and not forced)
        if: ${{ steps.gate.outputs.proceed != 'true' }}
        run: |
          {
            echo "## SF Crime Pipeline"
            echo ""
            echo "- √áalƒ±≈üma zamanƒ± (SF): **$(date)**"
            echo "- Not: 07:00 kapƒ±sƒ± nedeniyle adƒ±mlar atlandƒ±. Manuel tetiklerken \`force=true\` verin."
          } >> $GITHUB_STEP_SUMMARY

      # ================== EK: ENV BOOTSTRAP (mevcut satƒ±rlarƒ± deƒüi≈ütirmeden override) ==================
      - name: Bootstrap env overrides (BACKFILL & FRESHNESS)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          VARS_BACKFILL: ${{ vars.BACKFILL_DAYS }}           # org/repo ‚Üí Settings ‚Üí Variables ‚Üí BACKFILL_DAYS
          VARS_FRESH_LAG: ${{ vars.FRESHNESS_SF_MAX_LAG_DAYS }}
        run: |
          set -euo pipefail
          # BACKFILL_DAYS √∂ncelik: vars.BACKFILL_DAYS > mevcut env BACKFILL_DAYS > 30
          BF="${VARS_BACKFILL:-${BACKFILL_DAYS:-30}}"
          echo "BACKFILL_DAYS=${BF}" >> "$GITHUB_ENV"
          echo "Resolved BACKFILL_DAYS=${BF}"

          # FRESHNESS_SF_MAX_LAG_DAYS: vars > mevcut > 2
          FL="${VARS_FRESH_LAG:-${FRESHNESS_SF_MAX_LAG_DAYS:-2}}"
          echo "FRESHNESS_SF_MAX_LAG_DAYS=${FL}" >> "$GITHUB_ENV"
          echo "Resolved FRESHNESS_SF_MAX_LAG_DAYS=${FL}"
      # =================================================================================================

      - name: System deps for rtree (optional)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: sudo apt-get update && sudo apt-get install -y libspatialindex-dev

      - name: Set up Python 3.11
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install deps
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python -m pip install -U pip wheel setuptools
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          pip install -U "geopandas==1.0.1" "shapely==2.0.4" "pyproj==3.6.1" "pyogrio==0.9.0" "rtree==1.3.0"

      - name: Geo stack smoke test
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python - <<'PY'
          import geopandas, shapely, pyproj, pyogrio, pandas
          print("geopandas", geopandas.__version__)
          print("shapely", shapely.__version__)
          print("pyproj", pyproj.__version__)
          print("pyogrio", pyogrio.__version__)
          print("pandas", pandas.__version__)
          PY

      # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
      # Prefetch sf_crime_y.csv (GH_TOKEN set ‚Üí login YOK)
      - name: 00) Prefetch sf_crime_y.csv from previous successful run (artifact)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GH_TOKEN: ${{ github.token }} # gh CLI bunu otomatik kullanƒ±r; login YOK
          ARTIFACT_NAME: sf-crime-pipeline-output
          WORKFLOW_NAME: Full SF Crime Pipeline
          TARGET_FILE: sf_crime_y.csv
          OUT_DIR: crime_prediction_data
        run: |
          set -euo pipefail

          mkdir -p "${OUT_DIR}"
          if [ -f "${OUT_DIR}/${TARGET_FILE}" ]; then
            echo "‚ÑπÔ∏è ${OUT_DIR}/${TARGET_FILE} zaten var, indirme atlandƒ±."
            exit 0
          fi

          sudo apt-get update -y >/dev/null
          sudo apt-get install -y unzip >/dev/null

          if ! command -v gh >/dev/null; then
            type -p curl >/dev/null && sudo curl -sSL https://raw.githubusercontent.com/cli/cli/trunk/script/install.sh | sh
          fi
          gh --version || true
          gh auth status || true

          mkdir -p _prev
          FOUND=""

          # Son 10 ba≈üarƒ±lƒ± run i√ßinde ara; ilk bulduƒüunu kopyala
          RUNS=$(gh run list -R "${GITHUB_REPOSITORY}" \
                  --workflow "${WORKFLOW_NAME}" --status success \
                  -L 10 --json databaseId -q '.[].databaseId' || true)

          if [ -z "${RUNS}" ]; then
            echo "‚ÑπÔ∏è Ba≈üarƒ±lƒ± ge√ßmi≈ü run bulunamadƒ±; release fallback devreye girecek."
            exit 0
          fi

          for RID in ${RUNS}; do
            echo "üîé Run #$RID i√ßinde ${TARGET_FILE} aranƒ±yor‚Ä¶"
            rm -rf "_prev/$RID" && mkdir -p "_prev/$RID"
            if gh run download -R "${GITHUB_REPOSITORY}" "$RID" \
                 -n "${ARTIFACT_NAME}" -D "_prev/$RID" >/dev/null 2>&1; then
              F=$(find "_prev/$RID" -type f -iname "${TARGET_FILE}" | head -n1 || true)
              if [ -n "${F:-}" ]; then
                cp -f "$F" "${OUT_DIR}/${TARGET_FILE}"
                echo "‚úÖ Bulundu ve kopyalandƒ±: $F ‚Üí ${OUT_DIR}/${TARGET_FILE}"
                FOUND="yes"
                break
              fi
            else
              echo "  ‚Ü™Ô∏è Artifact indirilemedi veya yok (#$RID)."
            fi
          done

          if [ -z "${FOUND}" ]; then
            echo "‚ÑπÔ∏è ${TARGET_FILE} √∂nceki artifact‚Äôlarda bulunamadƒ±; update_crime.py release fallback kullanacak."
          fi
      # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

      # ---- PIPELINE ----
      - name: 01) Su√ß tabanƒ± ve grid ‚Üí sf_crime.csv + gridler
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -u update_crime.py

      - name: Restore 911 summary cache
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ env.CRIME_DATA_DIR }}/sf_911_last_5_year.csv
          key: 911-${{ runner.os }}-${{ github.run_number }}

      # ‚úÖ GH auth: login YOK (GH_TOKEN var)
      - name: Prefetch sf_911_last_5_year_y.csv from previous successful run (artifact)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GH_TOKEN: ${{ github.token }} # gh i√ßin non-interactive auth
          ARTIFACT_NAME: sf-crime-pipeline-output
          WORKFLOW_NAME: Full SF Crime Pipeline
          TARGET_FILE: sf_911_last_5_year_y.csv
          OUT_DIR: crime_prediction_data
        run: |
          set -euo pipefail

          mkdir -p "${OUT_DIR}"
          if [ -f "${OUT_DIR}/${TARGET_FILE}" ]; then
            echo "‚ÑπÔ∏è ${OUT_DIR}/${TARGET_FILE} zaten var, indirme atlandƒ±."
            exit 0
          fi

          sudo apt-get update -y >/dev/null
          sudo apt-get install -y unzip >/dev/null

          if ! command -v gh >/dev/null; then
            type -p curl >/dev/null && sudo curl -sSL https://raw.githubusercontent.com/cli/cli/trunk/script/install.sh | sh
          fi
          gh --version || true
          gh auth status || true

          mkdir -p _prev
          FOUND=""
          for RID in $(gh run list -R "${GITHUB_REPOSITORY}" \
                        --workflow "${WORKFLOW_NAME}" --status success \
                        -L 10 --json databaseId -q '.[].databaseId'); do
            echo "üîé Run #$RID i√ßinde ${TARGET_FILE} aranƒ±yor‚Ä¶"
            rm -rf "_prev/$RID" && mkdir -p "_prev/$RID"
            if gh run download -R "${GITHUB_REPOSITORY}" "$RID" \
                 -n "${ARTIFACT_NAME}" -D "_prev/$RID" >/dev/null 2>&1; then
              F=$(find "_prev/$RID" -type f -iname "${TARGET_FILE}" | head -n1 || true)
              if [ -n "${F:-}" ]; then
                cp -f "$F" "${OUT_DIR}/${TARGET_FILE}"
                echo "‚úÖ Bulundu ve kopyalandƒ±: $F ‚Üí ${OUT_DIR}/${TARGET_FILE}"
                FOUND="yes"
                break
              fi
            else
              echo "  ‚Ü™Ô∏è Artifact indirilemedi veya yok (#$RID)."
            fi
          done

          if [ -z "${FOUND}" ]; then
            echo "‚ÑπÔ∏è ${TARGET_FILE} √∂nceki artifact‚Äôlarda bulunamadƒ±; script release fallback kullanacak."
          fi

      # 911: g√ºn g√ºn yerine bulk (limit=50k)
      - name: 02) 911 ‚Üí sf_crime_01.csv (bulk 50k)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          SF_SODA_FETCH_STRATEGY: ${{ env.SF_SODA_FETCH_STRATEGY }}
          SF_SODA_PAGE_LIMIT: ${{ env.SF_SODA_PAGE_LIMIT }}
          SF_SODA_MAX_PAGES: ${{ env.SF_SODA_MAX_PAGES }}
          SF911_API_URL: ${{ env.SF911_API_URL }}
          SF911_AGENCY_FILTER: ${{ env.SF911_AGENCY_FILTER }}
          SF911_API_TOKEN: ${{ env.SF911_API_TOKEN }}
          SOCS_APP_TOKEN: ${{ env.SOCS_APP_TOKEN }}
          BACKFILL_DAYS: ${{ env.BACKFILL_DAYS }}
          DAILY_OUT: ${{ env.CRIME_DATA_DIR }}/sf_crime_01.csv
        run: |
          set -e
          python -u update_911.py
          
      - name: 02.fix) Move root-level sf_crime_01.csv into CRIME_DATA_DIR if present
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          if [ -f "sf_crime_01.csv" ]; then
            echo "‚Ü™Ô∏è Root'ta sf_crime_01.csv bulundu; CRIME_DATA_DIR altƒ±na ta≈üƒ±nƒ±yor."
            mv -f "sf_crime_01.csv" "${CRIME_DATA_DIR}/sf_crime_01.csv"
          fi

      - name: 02.1) Verify sf_crime_01 presence & head
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          D="${CRIME_DATA_DIR}"
          F1="${D}/sf_crime_01.csv"
          echo "‚Üí Beklenen dosya: $F1"
          [ -f "$F1" ] || { echo "‚ùå sf_crime_01.csv yok! update_911.py loglarƒ±nƒ± kontrol et (DAILY_OUT doƒüru mu?)."; exit 2; }
          echo "---- sf_crime_01.csv (head) ----"
          head -n 5 "$F1" || true
          
      - name: Restore 311 summary cache
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ env.CRIME_DATA_DIR }}/sf_311_last_5_years.csv
          key: 311-${{ runner.os }}-${{ github.run_number }}

      # ‚úÖ YENƒ∞: 311 cache sonrasƒ± tazelik kontrol√º (hard gate)
      - name: Freshness check after 311 cache restore (hard gate)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, sys, pandas as pd
          from datetime import datetime, timedelta
          from zoneinfo import ZoneInfo
          p = os.path.join(os.environ["CRIME_DATA_DIR"], "sf_311_last_5_years.csv")
          if not os.path.exists(p):
              sys.exit(0)
          CANDS = ["requested_datetime","requested_date","opened_date","created_date",
                   "created_datetime","date","datetime"]
          hdr = pd.read_csv(p, nrows=0, low_memory=False)
          col = next((c for c in CANDS if c in hdr.columns), None)
          if not col:
              print("‚ö†Ô∏è 311: tarih kolonu bulunamadƒ± (cache)."); sys.exit(0)
          mx = None
          for ch in pd.read_csv(p, usecols=[col], chunksize=200_000, low_memory=False):
              s = pd.to_datetime(ch[col], errors="coerce").dt.date
              m = s.max()
              if pd.notna(m) and (mx is None or m > mx): mx = m
          if mx is None:
              print("‚ùå 311 cache parse edilemedi"); sys.exit(3)
          today_sf = datetime.now(ZoneInfo("America/Los_Angeles")).date()
          lag = int(os.getenv("FRESHNESS_SF_MAX_LAG_DAYS","2"))
          if mx < today_sf - timedelta(days=lag):
              print(f"‚ùå 311 cache STALE: max={mx}  (limit {lag} g√ºn)")
              sys.exit(4)
          print(f"‚úÖ 311 cache OK: max={mx}")
          PY

      # ‚úÖ 311 √∂zet dosyasƒ± i√ßin artifact prefetch (GH login YOK)
      - name: Prefetch sf_311_last_5_years_y.csv from previous successful run (artifact)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GH_TOKEN: ${{ github.token }}
          ARTIFACT_NAME: sf-crime-pipeline-output
          WORKFLOW_NAME: Full SF Crime Pipeline
          TARGET_FILE: sf_311_last_5_years_y.csv
          OUT_DIR: crime_prediction_data
        run: |
          set -euo pipefail

          mkdir -p "${OUT_DIR}"
          if [ -f "${OUT_DIR}/${TARGET_FILE}" ]; then
            echo "‚ÑπÔ∏è ${OUT_DIR}/${TARGET_FILE} zaten var, indirme atlandƒ±."
            exit 0
          fi

          sudo apt-get update -y >/dev/null
          sudo apt-get install -y unzip >/dev/null

          if ! command -v gh >/dev/null; then
            type -p curl >/dev/null && sudo curl -sSL https://raw.githubusercontent.com/cli/cli/trunk/script/install.sh | sh
          fi
          gh --version || true
          gh auth status || true

          mkdir -p _prev
          FOUND=""
          for RID in $(gh run list -R "${GITHUB_REPOSITORY}" \
                        --workflow "${WORKFLOW_NAME}" --status success \
                        -L 10 --json databaseId -q '.[].databaseId'); do
            echo "üîé Run #$RID i√ßinde ${TARGET_FILE} aranƒ±yor‚Ä¶"
            rm -rf "_prev/$RID" && mkdir -p "_prev/$RID"
            if gh run download -R "${GITHUB_REPOSITORY}" "$RID" \
                 -n "${ARTIFACT_NAME}" -D "_prev/$RID" >/dev/null 2>&1; then
              F=$(find "_prev/$RID" -type f -iname "${TARGET_FILE}" | head -n1 || true)
              if [ -n "${F:-}" ]; then
                cp -f "$F" "${OUT_DIR}/${TARGET_FILE}"
                echo "‚úÖ Bulundu ve kopyalandƒ±: $F ‚Üí ${OUT_DIR}/${TARGET_FILE}"
                FOUND="yes"
                break
              fi
            else
              echo "  ‚Ü™Ô∏è Artifact indirilemedi veya yok (#$RID)."
            fi
          done

          if [ -z "${FOUND}" ]; then
            echo "‚ÑπÔ∏è ${TARGET_FILE} √∂nceki artifact‚Äôlarda bulunamadƒ±; update_311.py kendi akƒ±≈üƒ±yla devam edecek."
          fi

      # 311: aynƒ± ≈üekilde bulk √ßekim
      - name: 03) 311 ‚Üí sf_crime_02.csv (bulk 50k)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          SF_SODA_FETCH_STRATEGY: ${{ env.SF_SODA_FETCH_STRATEGY }}
          SF_SODA_PAGE_LIMIT: ${{ env.SF_SODA_PAGE_LIMIT }}
          SF_SODA_MAX_PAGES: ${{ env.SF_SODA_MAX_PAGES }}
          SOCS_APP_TOKEN: ${{ env.SOCS_APP_TOKEN }}
          BACKFILL_DAYS: ${{ env.BACKFILL_DAYS }}
          DAILY_IN:  ${{ env.CRIME_DATA_DIR }}/sf_crime_01.csv
          DAILY_OUT: ${{ env.CRIME_DATA_DIR }}/sf_crime_02.csv
        run: |
          set -e
          if [ -f update_311.py ]; then
            python -u update_311.py
          else
            python -u scripts/update_311.py
          fi
      
      - name: 03.debug) Show 311 IO
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          echo "DAILY_IN=${DAILY_IN}"
          echo "DAILY_OUT=${DAILY_OUT}"
          ls -lah "${CRIME_DATA_DIR}" | sed -n '1,100p'

      - name: Ensure population CSV (local file)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          mkdir -p "${CRIME_DATA_DIR}"
          DEST="${CRIME_DATA_DIR}/sf_population.csv"
          CANDIDATES=(
            "${CRIME_DATA_DIR}/sf_population.csv"
            "sf_population.csv"
            "data/sf_population.csv"
            "inputs/sf_population.csv"
          )
          FOUND=""
          for p in "${CANDIDATES[@]}"; do
            if [ -f "$p" ]; then
              if [ -e "$DEST" ] && [ "$p" -ef "$DEST" ]; then
                echo "‚ÑπÔ∏è Zaten hedefte: $DEST"
              else
                cp -f "$p" "$DEST"
                echo "‚úÖ Copied: $p ‚Üí $DEST"
              fi
              FOUND="yes"
              break
            fi
          done
          if [ -z "$FOUND" ]; then
            echo "‚ùå sf_population.csv bulunamadƒ±. L√ºtfen repo‚Äôya ekleyin veya ${CRIME_DATA_DIR} altƒ±na yerle≈ütirin."
            exit 2
          fi
          echo "---- sf_population.csv (head) ----"
          head -n 5 "$DEST" || true

      - name: 03.1) Verify sf_crime_01/02 presence & heads
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          D="${CRIME_DATA_DIR}"
          F1="${D}/sf_crime_01.csv"
          F2="${D}/sf_crime_02.csv"

          echo "‚Üí Beklenen dosyalar:"
          echo "  - $F1"
          echo "  - $F2"

          [ -f "$F1" ] || { echo "‚ùå sf_crime_01.csv yok! update_911.py √ßƒ±ktƒ±sƒ±nƒ± kontrol et."; exit 2; }
          [ -f "$F2" ] || { echo "‚ùå sf_crime_02.csv yok! update_311.py DAILY_IN/DAILY_OUT ve loglarƒ±nƒ± kontrol et."; exit 3; }

          echo "---- sf_crime_01.csv (head) ----"
          head -n 5 "$F1" || true
          echo "---- sf_crime_02.csv (head) ----"
          head -n 5 "$F2" || true

      - name: (Opsiyonel) sf_population.csv ba≈ülƒ±k normalize
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python - <<'PY'
          import os, pandas as pd
          p = os.path.join(os.environ["CRIME_DATA_DIR"], "sf_population.csv")
          df = pd.read_csv(p)
          low = {c.lower(): c for c in df.columns}

          # GEOID standardizasyonu
          if 'geoid' not in low and 'geography_id' in low:
              df.rename(columns={low['geography_id']: 'GEOID'}, inplace=True)

          # N√ºfus s√ºtununu 'population' yap
          for cand in ['population','total_population','b01003_001e','estimate','total','value']:
              if cand in low:
                  if 'population' not in df.columns:
                      df.rename(columns={low[cand]: 'population'}, inplace=True)
                  break

          df.to_csv(p, index=False)
          print("Normalized headers:", df.columns.tolist())
          PY

      - name: 04) N√ºfus ‚Üí sf_crime_03.csv (demografi + n√ºfus)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          POPULATION_PATH: ${{ env.CRIME_DATA_DIR }}/sf_population.csv
          CENSUS_GEO_LEVEL: ${{ env.CENSUS_GEO_LEVEL }}
          ACS_YEAR: ${{ env.ACS_YEAR }}
          DEMOG_WHITELIST: ${{ env.DEMOG_WHITELIST }}
        run: |
          set -e
          echo "---- update_population.py (ilk 30 satƒ±r) ----"
          sed -n '1,30p' update_population.py || true
          python -u update_population.py

      - name: 05) Otob√ºs ‚Üí sf_crime_04.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -u update_bus.py

      - name: 06) Tren (BART) ‚Üí sf_crime_05.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -u update_train.py

      - name: 07) POI zenginle≈ütirme ‚Üí sf_crime_06.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          if [ -f update_poi.py ]; then python -u update_poi.py;
          elif [ -f pipeline_make_sf_crime_06.py ]; then python -u pipeline_make_sf_crime_06.py;
          else echo "POI adƒ±mƒ± bulunamadƒ±"; exit 2; fi

      - name: 08) Polis & Devlet binalarƒ± ‚Üí sf_crime_07.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          if [ -f update_police_gov.py ]; then python -u update_police_gov.py;
          elif [ -f scripts/enrich_police_gov_06_to_07.py ]; then python -u scripts/enrich_police_gov_06_to_07.py;
          else echo "Polis/Gov adƒ±mƒ± bulunamadƒ±"; exit 2; fi

      - name: 09) Hava durumu ‚Äî sadece update
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          if [ -f update_weather.py ]; then
            python -u update_weather.py
          elif [ -f scripts/update_weather.py ]; then
            python -u scripts/update_weather.py
          else
            echo "‚ùå Weather script not found"; exit 2
          fi
          echo "sf_weather_5years.csv ‚Äî ilk 5 satƒ±r:"
          head -n 5 "${CRIME_DATA_DIR}/sf_weather_5years.csv" || head -n 5 sf_weather_5years.csv || true

      - name: Normalize grid outputs (put under CRIME_DATA_DIR)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          mkdir -p "${CRIME_DATA_DIR}"
          CANDIDATES=(
            "sf_crime_grid_full_labeled.csv"
            "data/sf_crime_grid_full_labeled.csv"
            "outputs/sf_crime_grid_full_labeled.csv"
            "${CRIME_DATA_DIR}/sf_crime_grid_full_labeled.csv"
          )
          FOUND=""
          for p in "${CANDIDATES[@]}"; do
            if [ -f "$p" ]; then
              cp -f "$p" "${CRIME_DATA_DIR}/sf_crime_grid_full_labeled.csv"
              echo "‚úÖ Grid kopyalandƒ±: $p ‚Üí ${CRIME_DATA_DIR}/sf_crime_grid_full_labeled.csv"
              FOUND="yes"
              break
            fi
          done
          if [ -z "${FOUND}" ]; then
            echo "‚ö†Ô∏è sf_crime_grid_full_labeled.csv bulunamadƒ±; stacking bazƒ± g√∂r√ºn√ºmleri atlayabilir."
          fi

      - name: 10.5) neighbors.csv (kullan/normalize/√ºret + kalite kontrol)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          STACKING_DATASET: ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.csv
          NEIGH_FILE: ${{ env.CRIME_DATA_DIR }}/neighbors.csv
          NEIGHBOR_STRATEGY: queen         # queen|rook
          MIN_NEIGHBOR_DEG: "3"            # en az derece
          KNN_K: "5"                       # takviye i√ßin k
          NEIGHBOR_POLY: ${{ env.CRIME_DATA_DIR }}/sf_grid.geojson
        run: |
          set -euo pipefail
          DEST="${CRIME_DATA_DIR}/neighbors.csv"
          mkdir -p "${CRIME_DATA_DIR}"

          echo "‚Üí hedef: $DEST"
          # 1) Varsa repodan kopyala
          for p in \
            "${CRIME_DATA_DIR}/neighbors.csv" \
            "crime_prediction_data/neighbors.csv" \
            "neighbors.csv" "data/neighbors.csv" "inputs/neighbors.csv"
          do
            if [ -f "$p" ]; then
              if [ -e "$DEST" ] && [ "$p" -ef "$DEST" ]; then
                echo "‚ÑπÔ∏è Zaten hedefte: $DEST"
              else
                cp -f "$p" "$DEST"
                echo "‚úÖ Copied: $p ‚Üí $DEST"
              fi
              break
            fi
          done

          # 2) Ba≈ülƒ±klarƒ± normalize et ‚Üí geoid,neighbor
          python - <<'PY'
          import os, pandas as pd, sys, re
          p = os.path.join(os.environ["CRIME_DATA_DIR"], "neighbors.csv")
          try:
              df = pd.read_csv(p, dtype=str)
          except Exception:
              sys.exit(0)
          if df.empty:
              sys.exit(0)
          low = {c.lower(): c for c in df.columns}
          src = low.get("geoid") or low.get("src") or low.get("source")
          dst = (low.get("neighbor") or low.get("neighbor_geoid") or
                 low.get("neighborgeoid") or low.get("dst") or low.get("target"))
          if not src or not dst:
              print("‚ö†Ô∏è neighbors.csv ba≈ülƒ±klarƒ± tanƒ±nmadƒ±:", df.columns.tolist()); sys.exit(0)
          df = df.rename(columns={src: "geoid", dst: "neighbor"})
          L = int(os.environ.get("GEOID_LEN","11"))
          for c in ("geoid","neighbor"):
              df[c] = df[c].astype(str).str.extract(r"(\d+)", expand=False).str.zfill(L)
          # √ßift y√∂nl√º ve tekille
          df = pd.concat([df, df.rename(columns={"geoid":"neighbor","neighbor":"geoid"})], ignore_index=True)
          df = df.dropna().drop_duplicates()
          df = df[df["geoid"] != df["neighbor"]]
          df[["geoid","neighbor"]].to_csv(p, index=False)
          print("Neighbors headers ‚Üí", ["geoid","neighbor"], f"(rows={len(df)})")
          PY

          # 3) Kalite kontrol
          python - <<'PY'
          import os, pandas as pd, sys, json, pathlib
          from pathlib import Path
          p = Path(os.environ["CRIME_DATA_DIR"]) / "neighbors.csv"
          min_deg = int(os.environ.get("MIN_NEIGHBOR_DEG","3"))
          if not p.exists() or p.stat().st_size == 0:
              need=True; reason="yok/bo≈ü"
          else:
              try:
                  df = pd.read_csv(p, dtype=str)
                  ok = set(df.columns) >= {"geoid","neighbor"} and len(df)>0
                  mindeg = (df.groupby("geoid")["neighbor"].nunique().min() if ok else 0)
                  need = (not ok) or (mindeg is None) or (mindeg < min_deg)
                  reason = f"min_deg={mindeg}<{min_deg}" if ok else "ge√ßersiz ba≈ülƒ±k/bo≈ü"
              except Exception as e:
                  need=True; reason=f"hata:{e}"
          if need:
              Path(os.environ["CRIME_DATA_DIR"]).joinpath(".rebuild_neighbors").write_text(reason)
              print("REBUILD=1 ‚Üí", reason)
          else:
              print("REBUILD=0 ‚Üí yeterli kalite")
          PY

          # 4) Yeniden √ºretim gerekiyorsa
          if [ -f "${CRIME_DATA_DIR}/.rebuild_neighbors" ]; then
            if [ ! -f "${NEIGHBOR_POLY}" ]; then
              echo "‚ö†Ô∏è Polygon katmanƒ± yok: ${NEIGHBOR_POLY}. Mevcut neighbors.csv kullanƒ±lacak."
            else
              echo "üîß Kom≈üuluk yeniden olu≈üturuluyor (${NEIGHBOR_STRATEGY}, min_deg=${MIN_NEIGHBOR_DEG}, knn_k=${KNN_K})"

              cat > _rebuild_neighbors.py <<'PY'
          #!/usr/bin/env python3
          from __future__ import annotations
          import os, re
          from pathlib import Path
          import pandas as pd

          try:
              import geopandas as gpd
          except Exception:
              gpd = None

          CRIME_DIR = Path("crime_prediction_data")
          GEOID_LEN  = int(os.environ.get("GEOID_LEN","11"))
          STRATEGY   = os.environ.get("NEIGHBOR_STRATEGY","queen").lower()
          MIN_DEG    = int(os.environ.get("MIN_NEIGHBOR_DEG","3"))
          KNN_K      = int(os.environ.get("KNN_K","5"))
          OUT_PATH   = Path(os.environ.get("NEIGHBOR_FILE", str(CRIME_DIR/"neighbors.csv")))
          GRID_CSV   = Path(os.environ.get("STACKING_DATASET", str(CRIME_DIR/"sf_crime_grid_full_labeled.csv")))
          POLY_HINT  = os.environ.get("NEIGHBOR_POLY","")

          def _norm(s: pd.Series) -> pd.Series:
              return s.astype(str).str.extract(r"(\d+)", expand=False).str.zfill(GEOID_LEN)
          def _sym(df: pd.DataFrame) -> pd.DataFrame:
              rev = df.rename(columns={"geoid":"neighbor","neighbor":"geoid"})
              out = pd.concat([df, rev], ignore_index=True)
              out = out[out["geoid"] != out["neighbor"]]
              return out.drop_duplicates()
          def _neighbors_from_polygons(path: str) -> pd.DataFrame | None:
              if not gpd:
                  return None
              try:
                  gdf = gpd.read_file(path)
              except Exception:
                  return None
              low = {re.sub(r"[^a-z0-9]","", c.lower()): c for c in gdf.columns}
              gcol = low.get("geoid") or low.get("geoid10") or low.get("geographyid") or list(gdf.columns)[0]
              if "geometry" not in gdf.columns:
                  return None
              gdf = gdf[[gcol,"geometry"]].rename(columns={gcol:"geoid"}).copy()
              gdf["geoid"] = _norm(gdf["geoid"])
              gdf = gdf[gdf["geometry"].notna()].reset_index(drop=True)
              pairs = gdf.sindex.query_bulk(gdf.geometry, predicate="touches")
              i, j = pairs
              m = pd.DataFrame({"i": i, "j": j})
              m = m[m["i"] < m["j"]]
              df = pd.DataFrame({
                  "geoid":    gdf.loc[m["i"], "geoid"].to_numpy(),
                  "neighbor": gdf.loc[m["j"], "geoid"].to_numpy(),
              })
              return _sym(df.drop_duplicates())
          def _centroids_table(poly_ok: bool) -> pd.DataFrame | None:
              if poly_ok and gpd:
                  gdf = gpd.read_file(POLY_HINT)
                  low = {re.sub(r"[^a-z0-9]","", c.lower()): c for c in gdf.columns}
                  gcol = low.get("geoid") or low.get("geoid10") or low.get("geographyid") or list(gdf.columns)[0]
                  c = gdf[[gcol,"geometry"]].rename(columns={gcol:"geoid"}).copy()
                  c["geoid"] = _norm(c["geoid"])
                  cent = c.geometry.centroid
                  return pd.DataFrame({"geoid": c["geoid"], "x": cent.x, "y": cent.y})
              if GRID_CSV.exists():
                  df = pd.read_csv(GRID_CSV, dtype=str, low_memory=False)
                  low = {re.sub(r"[^a-z0-9]","", c.lower()): c for c in df.columns}
                  gcol = low.get("geoid") or low.get("geoid10") or low.get("geographyid")
                  lat  = low.get("lat") or low.get("latitude") or low.get("centroidlat")
                  lon  = low.get("lon") or low.get("lng") or low.get("longitude") or low.get("centroidlon")
                  if gcol and lat and lon:
                      t = df[[gcol, lat, lon]].rename(columns={gcol:"geoid", lat:"y", lon:"x"}).copy()
                      t["geoid"] = _norm(t["geoid"])
                      t["x"] = pd.to_numeric(t["x"], errors="coerce")
                      t["y"] = pd.to_numeric(t["y"], errors="coerce")
                      return t.dropna()
              return None
          def _ensure_min_degree(nb: pd.DataFrame, geos: pd.DataFrame) -> pd.DataFrame:
              import numpy as np
              if geos is None or geos.empty:
                  return nb
              deg = nb.groupby("geoid")["neighbor"].nunique()
              need = set(deg.index[deg < MIN_DEG] if not deg.empty else geos["geoid"])
              if not need:
                  return nb
              idx = {g:i for i,g in enumerate(geos["geoid"])}
              XY  = geos[["x","y"]].to_numpy()
              add = []
              for g in need:
                  i = idx[g]
                  d = np.sum((XY - XY[i])**2, axis=1)
                  order = np.argsort(d)
                  picks = []
                  for j in order:
                      gj = geos.iloc[j]["geoid"]
                      if gj == g:
                          continue
                      picks.append(gj)
                      if len(picks) >= KNN_K:
                          break
                  add += [(g, n) for n in picks]
              nb2 = pd.concat([nb, pd.DataFrame(add, columns=["geoid","neighbor"])], ignore_index=True)
              return _sym(nb2.drop_duplicates())
          def main():
              poly_ok = bool(POLY_HINT) and Path(POLY_HINT).exists()
              nb = _neighbors_from_polygons(POLY_HINT) if poly_ok else pd.DataFrame(columns=["geoid","neighbor"])
              cent = _centroids_table(poly_ok)
              nb = _ensure_min_degree(nb, cent)
              if nb.empty:
                  nb = pd.DataFrame(columns=["geoid","neighbor"])
              nb["geoid"] = _norm(nb["geoid"])
              nb["neighbor"] = _norm(nb["neighbor"])
              nb = _sym(nb).drop_duplicates().sort_values(["geoid","neighbor"])
              OUT_PATH.parent.mkdir(parents=True, exist_ok=True)
              nb.to_csv(OUT_PATH, index=False)
              deg = nb.groupby("geoid")["neighbor"].nunique()
              print(f"neighbors.csv yazƒ±ldƒ± ‚Üí {OUT_PATH} | n_edges={len(nb)} | n_nodes={len(deg)} | min_deg={int(deg.min()) if len(deg) else 0} | mean_deg={round(float(deg.mean()),2) if len(deg) else 0.0}")
          if __name__ == "__main__":
              main()
          PY
              python _rebuild_neighbors.py
              rm -f _rebuild_neighbors.py
              rm -f "${CRIME_DATA_DIR}/.rebuild_neighbors"
            fi
          fi

          echo "neighbors.csv (ilk 10 satƒ±r):"
          head -n 10 "${DEST}" || true

      - name: 10.6) sf_crime_07 ‚Üí sf_crime_08 (kom≈üuluk ekleme)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          NEIGH_FILE: ${{ env.CRIME_DATA_DIR }}/neighbors.csv
        run: |
          set -e
          [ -f "${CRIME_DATA_DIR}/sf_crime_07.csv" ] || { echo "‚ùå sf_crime_07.csv yok"; exit 2; }
          [ -f "${NEIGH_FILE}" ] || { echo "‚ùå neighbors.csv yok: ${NEIGH_FILE}"; exit 3; }

          # Doƒüru betik yolunu bul ve √ßalƒ±≈ütƒ±r
          if [ -f scripts/make_neighbors.py ]; then
            PY=scripts/make_neighbors.py
          elif [ -f make_neighbors.py ]; then
            PY=make_neighbors.py
          else
            echo "‚ùå make_neighbors.py bulunamadƒ± (scripts/ altƒ±nda olmalƒ±)."
            exit 4
          fi

          python -u "$PY"

          # √áƒ±ktƒ±yƒ± doƒürula
          [ -f "${CRIME_DATA_DIR}/sf_crime_08.csv" ] || { echo "‚ùå sf_crime_08.csv olu≈ümadƒ±"; exit 5; }
          echo "---- sf_crime_08.csv (head) ----"
          head -n 10 "${CRIME_DATA_DIR}/sf_crime_08.csv" || true
          
      - name: 11) Install ML deps for stacking
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python -m pip install -U pip wheel setuptools
          pip install pandas numpy scikit-learn joblib
          pip install xgboost lightgbm || true

      - name: 12) Run stacking risk pipeline
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          STACKING_DATASET: ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.csv
          ENABLE_SPATIAL_TE: "1"
          TE_ALPHA: "50"
          NEIGH_FILE: ${{ env.CRIME_DATA_DIR }}/neighbors.csv
          ENABLE_TE_ABLATION: "1"
          ABLASYON_BASIS: "ohe"
          PATROL_HORIZON_DAYS: ${{ env.PATROL_HORIZON_DAYS }}
          PATROL_TOP_K: ${{ env.PATROL_TOP_K }}
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
        run: |
          set -e
          echo "HORIZON=${PATROL_HORIZON_DAYS}, TOP_K=${PATROL_TOP_K}"
          if [ ! -f "${STACKING_DATASET}" ]; then
            echo "‚ùå STACKING_DATASET bulunamadƒ±: ${STACKING_DATASET}"
            exit 2
          fi
          python -u stacking_risk_pipeline.py
      
      - name: 12.1) Inspect risk_hourly dates
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          echo "---- risk_hourly.csv (dates) ----"
          if [ -f "${CRIME_DATA_DIR}/risk_hourly.csv" ]; then
            cut -d, -f1 "${CRIME_DATA_DIR}/risk_hourly.csv" | head -n 30
            echo "---- unique dates (summary) ----"
            cut -d, -f1 "${CRIME_DATA_DIR}/risk_hourly.csv" | tail -n +2 | sort | uniq -c
          else
            echo "risk_hourly.csv bulunamadƒ±"
          fi

      - name: 13) Produce yarin.csv & week.csv (Visual Crossing)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          VISUAL_CROSSING_API_KEY: ${{ secrets.VISUAL_CROSSING_API_KEY }}
          WX_LOCATION: ${{ env.WX_LOCATION }}
          WX_UNIT: ${{ env.WX_UNIT }}
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
        run: |
          set -e
          python - <<'PY'
          import os, requests, pandas as pd
          from datetime import datetime, timedelta
          from zoneinfo import ZoneInfo

          API_KEY = os.environ["VISUAL_CROSSING_API_KEY"]
          LOCATION = os.environ.get("WX_LOCATION","san francisco")
          UNIT = os.environ.get("WX_UNIT","us")
          OUTDIR = os.environ.get("CRIME_DATA_DIR","crime_data")

          url = f"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{LOCATION}?unitGroup={UNIT}&key={API_KEY}&contentType=json"
          r = requests.get(url, timeout=30); r.raise_for_status()
          data = r.json()

          SF = ZoneInfo("America/Los_Angeles")
          today = datetime.now(SF).date()
          tomorrow = today + timedelta(days=1)
          week_end = tomorrow + timedelta(days=6)

          def rowpick(d):
              return {
                  "date": d.get("datetime"),
                  "tempmax": d.get("tempmax"),
                  "tempmin": d.get("tempmin"),
                  "precip": d.get("precip", 0),
                  "precipprob": d.get("precipprob"),
                  "windspeed": d.get("windspeed"),
                  "humidity": d.get("humidity"),
                  "description": d.get("description"),
                  "icon": d.get("icon"),
                  "unit_group": UNIT,
              }

          days = data.get("days", [])

          # yarin.csv
          t_key = tomorrow.strftime("%Y-%m-%d")
          td = next((d for d in days if d.get("datetime") == t_key), None)
          df_t = pd.DataFrame([rowpick(td)]) if td else pd.DataFrame([{"date": t_key, "unit_group": UNIT}])
          os.makedirs(OUTDIR, exist_ok=True)
          df_t.to_csv(os.path.join(OUTDIR, "yarin.csv"), index=False)

          # week.csv (yarƒ±ndan ba≈ülayarak 7 g√ºn)
          rows = []
          for d in days:
              try:
                  dd = datetime.strptime(d.get("datetime",""), "%Y-%m-%d").date()
              except Exception:
                  continue
              if tomorrow <= dd <= week_end:
                  rows.append(rowpick(d))
          pd.DataFrame(rows).to_csv(os.path.join(OUTDIR, "week.csv"), index=False)
          PY
          echo "---- yarin.csv (head) ----"; head -n 5 "${CRIME_DATA_DIR}/yarin.csv" || true
          echo "---- week.csv (head) ----";  head -n 5 "${CRIME_DATA_DIR}/week.csv" || true

      - name: 15) Run post_patrol.py (uses risk_hourly + yarin/week)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          PATROL_TOP_K: ${{ env.PATROL_TOP_K }}
          PATROL_HORIZON_DAYS: ${{ env.PATROL_HORIZON_DAYS }}
        run: |
          set -e
          test -f "${CRIME_DATA_DIR}/risk_hourly.csv" || { echo "‚ùå risk_hourly.csv bulunamadƒ± (stacking √ßƒ±ktƒ±larƒ±nƒ± kontrol et)"; exit 2; }
          python -u scripts/post_patrol.py
          echo "---- patrol_recs_multi.csv (head) ----"
          head -n 20 "${CRIME_DATA_DIR}/patrol_recs_multi.csv" || true

      - name: 14) Quick preview (stacking outputs)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          echo "risk_hourly* (ilk 5)"
          head -n 5 ${CRIME_DATA_DIR}/risk_hourly*.csv 2>/dev/null || true
          echo "patrol_recs* (ilk 5)"
          head -n 5 ${CRIME_DATA_DIR}/patrol_recs*.csv 2>/dev/null || true
          echo "metrics_* (ilk 5)"
          head -n 5 ${CRIME_DATA_DIR}/metrics_*.csv 2>/dev/null || true

      # 15) CSV ‚Üí Parquet i√ßin baƒüƒ±mlƒ±lƒ±klar
      - name: Install converter deps
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python -m pip install -U pip
          pip install --upgrade polars pyarrow

      # CSV ‚Üí Parquet d√∂n√º≈ü√ºm√º
      - name: Convert CSV ‚Üí Parquet
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          mkdir -p parquet_out
          python scripts/csv_to_parquet.py \
            --input "${{ env.CRIME_DATA_DIR }}/" \
            --output parquet_out/ \
            --compression zstd \
            --stats

      # ======================= NEW: FRESHNESS GUARDS & METADATA =======================
      - name: Emit single-row ‚Äúlatest date‚Äù CSVs (with header)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, pandas as pd
          from pathlib import Path

          base = Path(os.environ.get("CRIME_DATA_DIR","crime_prediction_data"))

          # Hedef dosyalar + olasƒ± tarih kolonlarƒ±
          targets = [
            ("sf_crime.csv",                    ["incident_datetime","datetime","date"]),
            ("sf_911_last_5_year.csv",          ["call_datetime","incident_datetime","date"]),
            ("sf_311_last_5_years.csv",         ["requested_datetime","opened_date","date"]),
            ("sf_weather_5years.csv",           ["date"]),
            ("sf_crime_grid_full_labeled.csv",  ["date"]),
            ("risk_hourly.csv",                 ["date"]),
          ]

          overview = []
          for fname, candidates in targets:
            fpath = base / fname
            if not fpath.exists():
              continue
            try:
              df_head = pd.read_csv(fpath, nrows=5000, low_memory=False)
              rowcount = sum(1 for _ in open(fpath, 'r', encoding='utf-8', errors='ignore')) - 1
            except Exception:
              continue

            col = next((c for c in candidates if c in df_head.columns), None)
            if col is None:
              continue

            mx = pd.to_datetime(df_head[col], errors="coerce").dt.date.max()
            if pd.isna(mx):
              continue

            latest_path = fpath.with_name(fpath.stem + "_latest.csv")
            pd.DataFrame([{col: mx.isoformat()}]).to_csv(latest_path, index=False)

            overview.append({
              "file": fname,
              "date_column": col,
              "max_date": mx.isoformat(),
              "rows_total_approx": rowcount if rowcount >= 0 else ""
            })

          if overview:
            pd.DataFrame(overview, columns=["file","date_column","max_date","rows_total_approx"]).to_csv(base / "latest_overview.csv", index=False)
          PY

          echo "‚Äî latest_overview.csv (head) ‚Äî"
          head -n 10 "${CRIME_DATA_DIR}/latest_overview.csv" || true

          echo "‚Äî produced *_latest.csv ‚Äî"
          ls -1 "${CRIME_DATA_DIR}"/*_latest.csv 2>/dev/null || true

      - name: Sanity packlist (mtime + head)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          echo "== PACK CANDIDATES (mtime) =="
          while IFS= read -r p; do
            [ -f "$p" ] && printf "%-48s  %s\n" "$(basename "$p")" "$(stat -c %y "$p" 2>/dev/null || stat -f %Sm "$p")"
          done <<EOF
          ${{ env.CRIME_DATA_DIR }}/sf_crime.csv
          ${{ env.CRIME_DATA_DIR }}/sf_crime_y.csv
          ${{ env.CRIME_DATA_DIR }}/sf_911_last_5_year.csv
          ${{ env.CRIME_DATA_DIR }}/sf_311_last_5_years.csv
          ${{ env.CRIME_DATA_DIR }}/sf_weather_5years.csv
          ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.csv
          ${{ env.CRIME_DATA_DIR }}/risk_hourly.csv
          EOF

          echo "== quick heads =="
          head -n 3 "${{ env.CRIME_DATA_DIR }}/sf_crime.csv" || true
          head -n 3 "${{ env.CRIME_DATA_DIR }}/sf_911_last_5_year.csv" || true
          head -n 3 "${{ env.CRIME_DATA_DIR }}/sf_311_last_5_years.csv" || true
          head -n 3 "${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.csv" || true

      - name: Write build info
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          {
            echo "run_number=${GITHUB_RUN_NUMBER}"
            echo "run_id=${GITHUB_RUN_ID}"
            echo "commit=${GITHUB_SHA}"
            echo "branch=${GITHUB_REF_NAME}"
            echo "built_at_utc=$(date -u +'%Y-%m-%dT%H:%M:%SZ')"
          } > "${CRIME_DATA_DIR}/_build_info.txt"
          cat "${CRIME_DATA_DIR}/_build_info.txt"

      - name: Clobber duplicates outside CRIME_DATA_DIR (optional)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          find . -type f -name 'sf_911_last_5_year*.csv' ! -path "./${{ env.CRIME_DATA_DIR }}/*" -delete || true
          find . -type f -name 'sf_311_last_5_years*.csv' ! -path "./${{ env.CRIME_DATA_DIR }}/*" -delete || true
      # ======================= /NEW =======================

      # Parquet'leri ayrƒ± artifact olarak y√ºkle
      - name: Upload Parquet artifact (opsiyonel)
        if: ${{ steps.gate.outputs.proceed == 'true' && (github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.persist == 'artifact')) }}
        uses: actions/upload-artifact@v4
        with:
          name: sf-crime-parquet
          path: parquet_out/
          if-no-files-found: warn

      # ---- √áIKTILAR ----
      - name: Upload artifact (varsayƒ±lan)
        if: ${{ steps.gate.outputs.proceed == 'true' && (github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.persist == 'artifact')) }}
        uses: actions/upload-artifact@v4
        with:
          name: sf-crime-pipeline-output
          path: |
            ${{ env.CRIME_DATA_DIR }}/sf_crime.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_y.csv
            ${{ env.CRIME_DATA_DIR }}/sf_population.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_08.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_07.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_06.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_05.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_04.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_03.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_02.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_01.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.csv
            ${{ env.CRIME_DATA_DIR }}/sf_311_last_5_years*.csv
            ${{ env.CRIME_DATA_DIR }}/sf_311_last_5_years_y.csv
            ${{ env.CRIME_DATA_DIR }}/sf_911_last_5_year.csv
            ${{ env.CRIME_DATA_DIR }}/sf_911_last_5_year_y.csv
            ${{ env.CRIME_DATA_DIR }}/sf_bus_stops_with_geoid.csv
            ${{ env.CRIME_DATA_DIR }}/sf_train_stops_with_geoid.csv
            ${{ env.CRIME_DATA_DIR }}/sf_pois_cleaned_with_geoid.csv
            ${{ env.CRIME_DATA_DIR }}/sf_weather_5years.csv
            ${{ env.CRIME_DATA_DIR }}/sf_weather_5years_y.csv
            ${{ env.CRIME_DATA_DIR }}/*.geojson
            ${{ env.CRIME_DATA_DIR }}/risk_hourly*.csv
            ${{ env.CRIME_DATA_DIR }}/patrol_recs*.csv
            ${{ env.CRIME_DATA_DIR }}/metrics_*.csv
            ${{ env.CRIME_DATA_DIR }}/models/*.joblib
            ${{ env.CRIME_DATA_DIR }}/ablation_spatial_te.csv
            ${{ env.CRIME_DATA_DIR }}/oof_base_probs*.npz
            ${{ env.CRIME_DATA_DIR }}/yarin.csv
            ${{ env.CRIME_DATA_DIR }}/week.csv
          if-no-files-found: warn
          retention-days: 14
