name: Full SF Crime Pipeline

on:
  schedule:
    - cron: "0 14,15 * * *" # SF 07:00: 14:00 UTC (yaz), 15:00 UTC (kƒ±≈ü)
  workflow_dispatch:
    inputs:
      persist:
        description: "Sonu√ßlarƒ± nasƒ±l saklayalƒ±m?"
        type: choice
        options: [artifact, commit, none]
        default: artifact
      force:
        description: "Manuel tetiklemede 07:00 kapƒ±sƒ±nƒ± YOK SAY"
        type: boolean
        default: true
      top_k:
        description: "Stacking: her saat dilimi i√ßin √∂nerilecek GEOID sayƒ±sƒ±"
        default: "50"

permissions:
  actions: read
  contents: write

concurrency:
  group: full-pipeline-${{ github.ref }}
  cancel-in-progress: true

env:
  CRIME_DATA_DIR: ${{ github.workspace }}
  GEOID_LEN: "11"

  BACKFILL_DAYS: "0"   # ‚Üê Mevcut satƒ±r KALDI; override a≈üaƒüƒ±da yapƒ±lƒ±yor
  SF_SODA_FETCH_STRATEGY: ${{ vars.SF_SODA_FETCH_STRATEGY || 'bulk' }}
  SF_SODA_PAGE_LIMIT:    ${{ vars.SF_SODA_PAGE_LIMIT    || '50000' }}
  SF_SODA_MAX_PAGES:     ${{ vars.SF_SODA_MAX_PAGES     || '100' }}

  SF911_API_URL:       ${{ vars.SF911_API_URL || 'https://data.sfgov.org/resource/2zdj-bwza.json' }}
  SF911_AGENCY_FILTER: ${{ vars.SF911_AGENCY_FILTER || 'agency like "%Police%"' }}
  SF911_API_TOKEN:     ${{ secrets.SF911_API_TOKEN }}
  SOCS_APP_TOKEN:      ${{ secrets.SOCS_APP_TOKEN }}

  PATROL_TOP_K: ${{ github.event.inputs.top_k || '50' }}

  PATROL_HORIZON_DAYS: "1"
  WX_LOCATION: "san francisco"
  WX_UNIT: "us"

  ACS_YEAR:         ${{ vars.ACS_YEAR || 'LATEST' }}
  DEMOG_WHITELIST:  ${{ vars.DEMOG_WHITELIST || '' }}
  CENSUS_GEO_LEVEL: ${{ vars.CENSUS_GEO_LEVEL || 'auto' }}

jobs:
  run:
    runs-on: ubuntu-latest

    steps:
      # -------------------- SETUP & GATE --------------------
      - name: Checkout (with LFS)
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Ensure LFS objects
        run: |
          set -e
          git lfs install
          git lfs pull
          echo "üìÅ Repo root:"; ls -lah
          echo "üìÅ crime_prediction_data:"; ls -lah crime_prediction_data || true
          echo "üìÅ CRIME_DATA_DIR:"; ls -lah "${CRIME_DATA_DIR}" || true

      - name: Ensure data dir
        run: mkdir -p "${CRIME_DATA_DIR}"

      - name: Echo paths
        run: |
          pwd
          echo "CRIME_DATA_DIR=${CRIME_DATA_DIR}"
          ls -lah "${CRIME_DATA_DIR}" || true

      - name: Set runner timezone to America/Los_Angeles
        uses: szenius/set-timezone@v2.0
        with:
          timezoneLinux: "America/Los_Angeles"

      - name: Gate by local SF time == 07 (bypassable)
        id: gate
        run: |
          set -e
          now="$(date)"
          echo "Runner local time: $now"
          echo "RUN_LOCAL_TIME=$now" >> $GITHUB_ENV
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ "${{ github.event.inputs.force }}" = "true" ]; then
            echo "proceed=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          [ "$(date +%H)" = "07" ] && echo "proceed=true" >> $GITHUB_OUTPUT || echo "proceed=false" >> $GITHUB_OUTPUT

      - name: Skip summary (outside 07:00 and not forced)
        if: ${{ steps.gate.outputs.proceed != 'true' }}
        run: |
          {
            echo "## SF Crime Pipeline"
            echo ""
            echo "- √áalƒ±≈üma zamanƒ± (SF): **$(date)**"
            echo "- Not: 07:00 kapƒ±sƒ± nedeniyle adƒ±mlar atlandƒ±. Manuel tetiklerken \`force=true\` verin."
          } >> $GITHUB_STEP_SUMMARY

      - name: Bootstrap env overrides (BACKFILL & FRESHNESS)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          VARS_BACKFILL: ${{ vars.BACKFILL_DAYS }}
          VARS_FRESH_LAG: ${{ vars.FRESHNESS_SF_MAX_LAG_DAYS }}
        run: |
          set -euo pipefail
          BF="${VARS_BACKFILL:-${BACKFILL_DAYS:-30}}"
          echo "BACKFILL_DAYS=${BF}" >> "$GITHUB_ENV"
          echo "Resolved BACKFILL_DAYS=${BF}"
          FL="${VARS_FRESH_LAG:-${FRESHNESS_SF_MAX_LAG_DAYS:-2}}"
          echo "FRESHNESS_SF_MAX_LAG_DAYS=${FL}" >> "$GITHUB_ENV"
          echo "Resolved FRESHNESS_SF_MAX_LAG_DAYS=${FL}"

      # -------------------- PYTHON STACK --------------------
      - name: System deps for rtree (optional)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: sudo apt-get update && sudo apt-get install -y libspatialindex-dev

      - name: Set up Python 3.11
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install deps
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -m pip install -U pip wheel setuptools
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          pip install -U "geopandas==1.0.1" "shapely==2.0.4" "pyproj==3.6.1" "pyogrio==0.9.0" "rtree==1.3.0"

      - name: Geo stack smoke test
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          python - <<'PY'
          import geopandas, shapely, pyproj, pyogrio, pandas
          print("geopandas", geopandas.__version__)
          print("shapely", shapely.__version__)
          print("pyproj", pyproj.__version__)
          print("pyogrio", pyogrio.__version__)
          print("pandas", pandas.__version__)
          PY

      # -------------------- PREFETCH (crime_y, 911_y, 311_y) --------------------
      - name: 00) Prefetch sf_crime_y.csv from previous successful run (artifact)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GH_TOKEN: ${{ github.token }}
          ARTIFACT_NAME: sf-crime-pipeline-output
          WORKFLOW_NAME: Full SF Crime Pipeline
          TARGET_FILE: sf_crime_y.csv
          OUT_DIR: crime_prediction_data
        run: |
          set -euo pipefail
          mkdir -p "${OUT_DIR}"
          [ -f "${OUT_DIR}/${TARGET_FILE}" ] && { echo "‚ÑπÔ∏è ${TARGET_FILE} zaten var"; exit 0; }
          sudo apt-get update -y >/dev/null
          sudo apt-get install -y unzip >/dev/null
          if ! command -v gh >/dev/null; then
            type -p curl >/dev/null && sudo curl -sSL https://raw.githubusercontent.com/cli/cli/trunk/script/install.sh | sh
          fi
          mkdir -p _prev
          for RID in $(gh run list -R "${GITHUB_REPOSITORY}" --workflow "${WORKFLOW_NAME}" --status success -L 10 --json databaseId -q '.[].databaseId' || true); do
            rm -rf "_prev/$RID" && mkdir -p "_prev/$RID"
            if gh run download -R "${GITHUB_REPOSITORY}" "$RID" -n "${ARTIFACT_NAME}" -D "_prev/$RID" >/dev/null 2>&1; then
              F=$(find "_prev/$RID" -type f -iname "${TARGET_FILE}" | head -n1 || true)
              [ -n "${F:-}" ] && cp -f "$F" "${OUT_DIR}/${TARGET_FILE}" && echo "‚úÖ Kopyalandƒ±." && break
            fi
          done

      # -------------------- CORE PIPELINE --------------------
      - name: 01) Su√ß tabanƒ± ve grid ‚Üí sf_crime.csv + gridler
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -u update_crime.py

      # ----- 911 -----
      - name: Restore 911 summary cache
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ env.CRIME_DATA_DIR }}/sf_911_last_5_year.csv
          key: 911-${{ runner.os }}-${{ github.run_number }}

      - name: Prefetch sf_911_last_5_year_y.csv from previous successful run (artifact)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GH_TOKEN: ${{ github.token }}
          ARTIFACT_NAME: sf-crime-pipeline-output
          WORKFLOW_NAME: Full SF Crime Pipeline
          TARGET_FILE: sf_911_last_5_year_y.csv
          OUT_DIR: crime_prediction_data
        run: |
          set -euo pipefail
          mkdir -p "${OUT_DIR}"
          [ -f "${OUT_DIR}/${TARGET_FILE}" ] && { echo "‚ÑπÔ∏è ${TARGET_FILE} zaten var"; exit 0; }
          sudo apt-get update -y >/dev/null
          sudo apt-get install -y unzip >/dev/null
          if ! command -v gh >/dev/null; then
            type -p curl >/dev/null && sudo curl -sSL https://raw.githubusercontent.com/cli/cli/trunk/script/install.sh | sh
          fi
          mkdir -p _prev
          for RID in $(gh run list -R "${GITHUB_REPOSITORY}" --workflow "${WORKFLOW_NAME}" --status success -L 10 --json databaseId -q '.[].databaseId'); do
            rm -rf "_prev/$RID" && mkdir -p "_prev/$RID"
            if gh run download -R "${GITHUB_REPOSITORY}" "$RID" -n "${ARTIFACT_NAME}" -D "_prev/$RID" >/dev/null 2>&1; then
              F=$(find "_prev/$RID" -type f -iname "${TARGET_FILE}" | head -n1 || true)
              [ -n "${F:-}" ] && cp -f "$F" "${OUT_DIR}/${TARGET_FILE}" && echo "‚úÖ Kopyalandƒ±." && break
            fi
          done

      - name: 02) 911 ‚Üí sf_crime_01.csv (bulk 50k)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          SF_SODA_FETCH_STRATEGY: ${{ env.SF_SODA_FETCH_STRATEGY }}
          SF_SODA_PAGE_LIMIT:    ${{ env.SF_SODA_PAGE_LIMIT }}
          SF_SODA_MAX_PAGES:     ${{ env.SF_SODA_MAX_PAGES }}
          SF911_API_URL:         ${{ env.SF911_API_URL }}
          SF911_AGENCY_FILTER:   ${{ env.SF911_AGENCY_FILTER }}
          SF911_API_TOKEN:       ${{ env.SF911_API_TOKEN }}
          SOCS_APP_TOKEN:        ${{ env.SOCS_APP_TOKEN }}
          BACKFILL_DAYS:         ${{ env.BACKFILL_DAYS }}
          DAILY_OUT:             ${{ env.CRIME_DATA_DIR }}/sf_crime_01.csv
        run: |
          set -e
          python -u update_911.py

      # ‚úÖ 911 CSV‚Äôleri CRIME_DATA_DIR‚Äôe ta≈üƒ±/kopyala (varsa k√∂kte/alt klas√∂rlerde kalmƒ±≈üsa)
      - name: 02.fix) Move 911 CSVs into CRIME_DATA_DIR if needed
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          echo "CRIME_DATA_DIR=${CRIME_DATA_DIR}"
          mkdir -p "${CRIME_DATA_DIR}"
          CANDS=(
            "sf_911_last_5_year.csv"
            "sf_911_last_5_year_y.csv"
            "sf_crime_01.csv"
          )
          for n in "${CANDS[@]}"; do
            for p in \
              "$n" \
              "crime_prediction_data/$n" \
              "${CRIME_DATA_DIR}/$n"
            do
              if [ -f "$p" ]; then
                dst="${CRIME_DATA_DIR}/$n"
                if [ "$(realpath "$p")" -ef "$(realpath "$dst")" ]; then
                  echo "‚ÑπÔ∏è Zaten hedefte: $dst"
                else
                  cp -f "$p" "$dst"
                  echo "‚úÖ Kopyalandƒ±: $p ‚Üí $dst"
                fi
              fi
            done
          done
          echo "[GLOB] 911 base summaries"
          ls -l "${CRIME_DATA_DIR}"/sf_911_last_5_year*.csv || true
          echo "[HEAD] sf_crime_01.csv"
          head -n 3 "${CRIME_DATA_DIR}/sf_crime_01.csv" || true

      - name: 02.1) Verify sf_crime_01 presence & head
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          F="${CRIME_DATA_DIR}/sf_crime_01.csv"
          [ -f "$F" ] || { echo "‚ùå sf_crime_01.csv yok!"; exit 2; }
          head -n 5 "$F" || true

      # ----- 311 -----
      - name: Restore 311 summary cache
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        uses: actions/cache@v4
        with:
          path: ${{ env.CRIME_DATA_DIR }}/sf_311_last_5_years.csv
          key: 311-${{ runner.os }}-${{ github.run_number }}
          restore-keys: |
            311-${{ runner.os }}-

      - name: Prefetch sf_311_last_5_years_y.csv from previous successful run (artifact)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          GH_TOKEN: ${{ github.token }}
          ARTIFACT_NAME: sf-crime-pipeline-output
          WORKFLOW_NAME: Full SF Crime Pipeline
          TARGET_FILE: sf_311_last_5_years_y.csv
          OUT_DIR: crime_prediction_data
        run: |
          set -euo pipefail
          mkdir -p "${OUT_DIR}"
          [ -f "${OUT_DIR}/${TARGET_FILE}" ] && { echo "‚ÑπÔ∏è ${TARGET_FILE} zaten var"; exit 0; }
          sudo apt-get update -y >/dev/null
          sudo apt-get install -y unzip >/dev/null
          if ! command -v gh >/dev/null; then
            type -p curl >/dev/null && sudo curl -sSL https://raw.githubusercontent.com/cli/cli/trunk/script/install.sh | sh
          fi
          mkdir -p _prev
          for RID in $(gh run list -R "${GITHUB_REPOSITORY}" --workflow "${WORKFLOW_NAME}" --status success -L 10 --json databaseId -q '.[].databaseId'); do
            rm -rf "_prev/$RID" && mkdir -p "_prev/$RID"
            if gh run download -R "${GITHUB_REPOSITORY}" "$RID" -n "${ARTIFACT_NAME}" -D "_prev/$RID" >/dev/null 2>&1; then
              F=$(find "_prev/$RID" -type f -iname "${TARGET_FILE}" | head -n1 || true)
              [ -n "${F:-}" ] && cp -f "$F" "${OUT_DIR}/${TARGET_FILE}" && echo "‚úÖ Kopyalandƒ±." && break
            fi
          done

      - name: 03) 311 ‚Üí sf_crime_02.csv (bulk 50k)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          SF_SODA_FETCH_STRATEGY: ${{ env.SF_SODA_FETCH_STRATEGY }}
          SF_SODA_PAGE_LIMIT:    ${{ env.SF_SODA_PAGE_LIMIT }}
          SF_SODA_MAX_PAGES:     ${{ env.SF_SODA_MAX_PAGES }}
          SOCS_APP_TOKEN:        ${{ env.SOCS_APP_TOKEN }}
          BACKFILL_DAYS:         ${{ env.BACKFILL_DAYS }}
        run: |
          set -e
          if [ -f update_311.py ]; then
            python -u update_311.py
          else
            python -u scripts/update_311.py
          fi

      # ‚úÖ 311 CSV‚Äôlerini CRIME_DATA_DIR‚Äôde garanti et (senin eklediƒüin blok)
      - name: 03.fix) Move 311 CSVs into CRIME_DATA_DIR if needed
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          echo "CRIME_DATA_DIR=${CRIME_DATA_DIR}"
          mkdir -p "${CRIME_DATA_DIR}"
          CANDS=(
            "sf_311_last_5_years.csv"
            "sf_311_last_5_years_3h.csv"
            "sf_311_last_5_years_y.csv"
            "sf_311_last_5_year.csv"
            "sf_311_last_5_year_y.csv"
            "sf_crime_02.csv"
          )
          for n in "${CANDS[@]}"; do
            for p in \
              "$n" \
              "crime_prediction_data/$n" \
              "${CRIME_DATA_DIR}/$n"
            do
              if [ -f "$p" ]; then
                dst="${CRIME_DATA_DIR}/$n"
                if [ "$(realpath "$p")" -ef "$(realpath "$dst")" ]; then
                  echo "‚ÑπÔ∏è Zaten hedefte: $dst"
                else
                  cp -f "$p" "$dst"
                  echo "‚úÖ Kopyalandƒ±: $p ‚Üí $dst"
                fi
              fi
            done
          done
          echo "---- 311 csv (ls) ----"
          ls -l "${CRIME_DATA_DIR}"/sf_311_last_5_year*.csv || true
          ls -l "${CRIME_DATA_DIR}"/sf_311_last_5_years*.csv || true
          echo "[HEAD] sf_crime_02.csv"
          head -n 3 "${CRIME_DATA_DIR}/sf_crime_02.csv" || true

      # ----- POPULATION/ACS -----
      - name: Ensure population CSV (local file)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          mkdir -p "${CRIME_DATA_DIR}"
          DEST="${CRIME_DATA_DIR}/sf_population.csv"
          CANDIDATES=(
            "${CRIME_DATA_DIR}/sf_population.csv"
            "sf_population.csv"
            "data/sf_population.csv"
            "inputs/sf_population.csv"
          )
          FOUND=""
          for p in "${CANDIDATES[@]}"; do
            if [ -f "$p" ]; then
              if [ -e "$DEST" ] && [ "$p" -ef "$DEST" ]; then
                echo "‚ÑπÔ∏è Zaten hedefte: $DEST"
              else
                cp -f "$p" "$DEST"
                echo "‚úÖ Copied: $p ‚Üí $DEST"
              fi
              FOUND="yes"; break
            fi
          done
          [ -n "$FOUND" ] || { echo "‚ùå sf_population.csv bulunamadƒ±."; exit 2; }
          head -n 5 "$DEST" || true

      - name: 04) N√ºfus ‚Üí sf_crime_03.csv (demografi + n√ºfus)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          POPULATION_PATH: ${{ env.CRIME_DATA_DIR }}/sf_population.csv
          CENSUS_GEO_LEVEL: ${{ env.CENSUS_GEO_LEVEL }}
          ACS_YEAR: ${{ env.ACS_YEAR }}
          DEMOG_WHITELIST: ${{ env.DEMOG_WHITELIST }}
        run: |
          set -e
          python -u update_population.py

      # ----- BUS / TRAIN / POI / POLICE-GOV / WEATHER -----
      - name: 05) Otob√ºs ‚Üí sf_crime_04.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -u update_bus.py

      - name: 06) Tren (BART) ‚Üí sf_crime_05.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -u update_train.py

      - name: POI enrich (GEOID-only) ‚Üí sf_crime_06.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          CRIME_IN:  "${{ env.CRIME_DATA_DIR }}/sf_crime_05.csv"
          CRIME_OUT: "${{ env.CRIME_DATA_DIR }}/sf_crime_06.csv"
        run: |
          set -euo pipefail
          ls -lah "${CRIME_DATA_DIR}/sf_crime_05.csv" || { echo "‚ùå sf_crime_05.csv yok"; exit 2; }
          if [ -f update_poi.py ]; then
            python -u update_poi.py
          elif [ -f pipeline_make_sf_crime_06.py ]; then
            python -u pipeline_make_sf_crime_06.py
          else
            echo "‚ùå POI adƒ±mƒ± scripti yok"; exit 2
          fi

      - name: 07.9) Assert sf_crime_06 presence (before police/gov)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          F="${CRIME_DATA_DIR}/sf_crime_06.csv"
          [ -f "$F" ] || { echo "‚ùå sf_crime_06.csv yok!"; exit 2; }
          head -n 5 "$F" || true

      - name: 08) Polis & Devlet binalarƒ± ‚Üí sf_crime_07.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          if [ -f update_police_gov.py ]; then python -u update_police_gov.py;
          elif [ -f scripts/enrich_police_gov_06_to_07.py ]; then python -u scripts/enrich_police_gov_06_to_07.py;
          else echo "‚ùå Polis/Gov script yok"; exit 2; fi

      - name: 09) Hava durumu ‚Äî sadece update (sf_weather_5years.csv)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          if [ -f update_weather.py ]; then
            python -u update_weather.py
          elif [ -f scripts/update_weather.py ]; then
            python -u scripts/update_weather.py
          else
            echo "‚ùå Weather script not found"; exit 2
          fi
          echo "sf_weather_5years.csv ‚Äî head:"
          head -n 5 "${CRIME_DATA_DIR}/sf_weather_5years.csv" || head -n 5 sf_weather_5years.csv || true

      - name: 10) Merge Weather (date-only) ‚Üí sf_crime_08.csv
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, pandas as pd
          from pathlib import Path
          base = Path(os.environ.get("CRIME_DATA_DIR","."))
          p07 = base/"sf_crime_07.csv"
          pwx = base/"sf_weather_5years.csv"
          if not p07.exists(): raise SystemExit("‚ùå sf_crime_07.csv yok")
          if not pwx.exists():
              pwx = Path("sf_weather_5years.csv")
              if not pwx.exists(): raise SystemExit("‚ùå sf_weather_5years.csv yok")
          df = pd.read_csv(p07, low_memory=False)
          wx = pd.read_csv(pwx, low_memory=False)
          def pick_date_col(cols):
              for c in ["date","day","datetime","incident_date"]:
                  if c in cols: return c
              return None
          dcol = pick_date_col(df.columns)
          wcol = pick_date_col(wx.columns)
          if dcol is None or wcol is None: raise SystemExit("‚ùå tarih kolonlarƒ± yok (crime|weather)")
          df["date"] = pd.to_datetime(df[dcol], errors="coerce").dt.date
          wx["date"] = pd.to_datetime(wx[wcol], errors="coerce").dt.date
          keep = [c for c in [
              "precipitation_mm","precip_mm","precip",
              "temp_max","tempmax","temp_min","tempmin",
              "windspeed","wind_speed","humidity","icon","description"
          ] if c in wx.columns]
          wx = wx[["date"] + keep].drop_duplicates("date")
          out = df.merge(wx, on="date", how="left")
          out.to_csv(base/"sf_crime_08.csv", index=False)
          print("sf_crime_08.csv yazƒ±ldƒ±:", len(out), "satƒ±r | cols:", len(out.columns))
          PY
          head -n 5 "${CRIME_DATA_DIR}/sf_crime_08.csv" || true

      # ----- GRID NORMALIZE & NEIGHBORS -----
      - name: Normalize grid outputs (put under CRIME_DATA_DIR)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          mkdir -p "${CRIME_DATA_DIR}"
          DST="${CRIME_DATA_DIR}/sf_crime_grid_full_labeled.csv"
          CANDIDATES=("sf_crime_grid_full_labeled.csv" "data/sf_crime_grid_full_labeled.csv" "outputs/sf_crime_grid_full_labeled.csv")
          FOUND=""
          for p in "${CANDIDATES[@]}"; do
            [ -f "$p" ] || continue
            if [ -e "$DST" ] && [ "$p" -ef "$DST" ]; then
              echo "‚ÑπÔ∏è Grid zaten doƒüru yerde: $DST"
              FOUND="yes"; break
            fi
            cp -f "$p" "$DST"
            echo "‚úÖ Grid kopyalandƒ±: $p ‚Üí $DST"
            FOUND="yes"; break
          done
          [ -n "${FOUND}" ] || echo "‚ö†Ô∏è sf_crime_grid_full_labeled.csv bulunamadƒ±; stacking bazƒ± g√∂r√ºn√ºmleri atlayabilir."

      - name: 10.5) neighbors.csv (kullan/normalize/√ºret + kalite kontrol)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          STACKING_DATASET: ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.csv
          NEIGHBOR_FILE: ${{ env.CRIME_DATA_DIR }}/neighbors.csv
          NEIGHBOR_STRATEGY: queen
          MIN_NEIGHBOR_DEG: "3"
          KNN_K: "5"
          NEIGHBOR_POLY: ${{ env.CRIME_DATA_DIR }}/sf_grid.geojson
        run: |
          set -euo pipefail
          DEST="${CRIME_DATA_DIR}/neighbors.csv"
          mkdir -p "${CRIME_DATA_DIR}"
          for p in "${CRIME_DATA_DIR}/neighbors.csv" "crime_prediction_data/neighbors.csv" "neighbors.csv" "data/neighbors.csv" "inputs/neighbors.csv"; do
            if [ -f "$p" ]; then
              [ -e "$DEST" ] && [ "$p" -ef "$DEST" ] && echo "‚ÑπÔ∏è Zaten hedefte" || cp -f "$p" "$DEST"
              break
            fi
          done
          python - <<'PY'
          import os, pandas as pd, sys, re
          p = os.path.join(os.environ["CRIME_DATA_DIR"], "neighbors.csv")
          try: df = pd.read_csv(p, dtype=str)
          except Exception: sys.exit(0)
          if df.empty: sys.exit(0)
          low = {c.lower(): c for c in df.columns}
          src = low.get("geoid") or low.get("src") or low.get("source")
          dst = low.get("neighbor") or low.get("neighbor_geoid") or low.get("neighborgeoid") or low.get("dst") or low.get("target")
          if not src or not dst: sys.exit(0)
          df = df.rename(columns={src: "geoid", dst: "neighbor"})
          L = int(os.environ.get("GEOID_LEN","11"))
          for c in ("geoid","neighbor"):
              df[c] = df[c].astype(str).str.extract(r"(\d+)", expand=False).str.zfill(L)
          rev = df.rename(columns={"geoid":"neighbor","neighbor":"geoid"})
          pd.concat([df, rev], ignore_index=True).drop_duplicates().to_csv(p, index=False)
          PY
          python - <<'PY'
          import os, pandas as pd, sys
          from pathlib import Path
          p = Path(os.environ["CRIME_DATA_DIR"]) / "neighbors.csv"
          min_deg = int(os.environ.get("MIN_NEIGHBOR_DEG","3"))
          need=True; reason="yok/bo≈ü"
          if p.exists() and p.stat().st_size>0:
              try:
                  df = pd.read_csv(p, dtype=str)
                  ok = set(df.columns) >= {"geoid","neighbor"} and len(df)>0
                  mindeg = (df.groupby("geoid")["neighbor"].nunique().min() if ok else 0)
                  need = (not ok) or (mindeg is None) or (mindeg < min_deg); reason = f"min_deg={mindeg}<{min_deg}" if ok else "ge√ßersiz/bo≈ü"
              except Exception as e: need=True; reason=f"hata:{e}"
          if need:
              Path(os.environ["CRIME_DATA_DIR"]).joinpath(".rebuild_neighbors").write_text(reason)
          PY
          if [ -f "${CRIME_DATA_DIR}/.rebuild_neighbors" ]; then
            echo "‚öíÔ∏è neighbors rebuild gerekiyor (knn fallback)"
            python - <<'PY'
            import os, pandas as pd, numpy as np
            from pathlib import Path
            base = Path(os.environ.get("CRIME_DATA_DIR","."))
            grid = base/"sf_crime_grid_full_labeled.csv"
            outp = base/"neighbors.csv"
            if not grid.exists(): raise SystemExit("no grid")
            df = pd.read_csv(grid, dtype=str, low_memory=False)
            la = "latitude" if "latitude" in df.columns else ("lat" if "lat" in df.columns else "y")
            lo = "longitude" if "longitude" in df.columns else ("lon" if "lon" in df.columns else "x")
            if "GEOID" not in df.columns or la not in df.columns or lo not in df.columns: raise SystemExit("coords yok")
            df = df[["GEOID", la, lo]].dropna().copy()
            df[la] = pd.to_numeric(df[la], errors="coerce"); df[lo] = pd.to_numeric(df[lo], errors="coerce"); df = df.dropna()
            df = df.groupby("GEOID", as_index=False).agg({la:"mean", lo:"mean"})
            arr = df[[la,lo]].to_numpy()
            K=5; edges=[]
            for i in range(len(df)):
                d=((arr-arr[i])**2).sum(1)
                idx=np.argsort(d)[1:K+1]
                for j in idx: edges.append((df.iloc[i]["GEOID"], df.iloc[j]["GEOID"]))
            nb=pd.DataFrame(edges, columns=["geoid","neighbor"]).drop_duplicates()
            nb=nb[nb["geoid"]!=nb["neighbor"]]
            nb.to_csv(outp, index=False)
            print("neighbors.csv rebuilt:", len(nb))
            PY
            rm -f "${CRIME_DATA_DIR}/.rebuild_neighbors"
          fi
          head -n 10 "${CRIME_DATA_DIR}/neighbors.csv" || true

      # ----- STACKING & PATROL -----
      - name: 11) Install ML deps for stacking
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -m pip install -U pip wheel setuptools
          pip install pandas numpy scikit-learn joblib
          pip install xgboost lightgbm || true

      - name: 12) Run stacking risk pipeline
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          STACKING_DATASET: ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.csv
          ENABLE_SPATIAL_TE: "1"
          TE_ALPHA: "50"
          NEIGH_FILE: ${{ env.CRIME_DATA_DIR }}/neighbors.csv
          ENABLE_TE_ABLATION: "1"
          ABLASYON_BASIS: "ohe"
          PATROL_HORIZON_DAYS: ${{ env.PATROL_HORIZON_DAYS }}
          PATROL_TOP_K: ${{ env.PATROL_TOP_K }}
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
        run: |
          set -e
          [ -f "${STACKING_DATASET}" ] || { echo "‚ùå STACKING_DATASET yok"; exit 2; }
          python -u stacking_risk_pipeline.py

      - name: 12.1) Inspect risk_hourly dates
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          echo "---- risk_hourly.csv (dates) ----"
          if [ -f "${CRIME_DATA_DIR}/risk_hourly.csv" ]; then
            cut -d, -f1 "${CRIME_DATA_DIR}/risk_hourly.csv" | head -n 30
            echo "---- unique dates (summary) ----"
            cut -d, -f1 "${CRIME_DATA_DIR}/risk_hourly.csv" | tail -n +2 | sort | uniq -c
          else
            echo "risk_hourly.csv bulunamadƒ±"
          fi

      - name: 13) Produce yarin.csv & week.csv (Visual Crossing)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          VISUAL_CROSSING_API_KEY: ${{ secrets.VISUAL_CROSSING_API_KEY }}
          WX_LOCATION: ${{ env.WX_LOCATION }}
          WX_UNIT: ${{ env.WX_UNIT }}
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
        run: |
          set -e
          python - <<'PY'
          import os, requests, pandas as pd
          from datetime import datetime, timedelta
          from zoneinfo import ZoneInfo
          API_KEY = os.environ["VISUAL_CROSSING_API_KEY"]
          LOCATION = os.environ.get("WX_LOCATION","san francisco")
          UNIT = os.environ.get("WX_UNIT","us")
          OUTDIR = os.environ.get("CRIME_DATA_DIR","crime_data")
          url = f"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{LOCATION}?unitGroup={UNIT}&key={API_KEY}&contentType=json"
          r = requests.get(url, timeout=30); r.raise_for_status()
          data = r.json()
          SF = ZoneInfo("America/Los_Angeles")
          today = datetime.now(SF).date()
          tomorrow = today + timedelta(days=1)
          week_end = tomorrow + timedelta(days=6)
          def rowpick(d):
              return {"date": d.get("datetime"),"tempmax": d.get("tempmax"),"tempmin": d.get("tempmin"),
                      "precip": d.get("precip", 0),"precipprob": d.get("precipprob"),"windspeed": d.get("windspeed"),
                      "humidity": d.get("humidity"),"description": d.get("description"),"icon": d.get("icon"),
                      "unit_group": UNIT}
          days = data.get("days", [])
          t_key = tomorrow.strftime("%Y-%m-%d")
          td = next((d for d in days if d.get("datetime") == t_key), None)
          df_t = pd.DataFrame([rowpick(td)]) if td else pd.DataFrame([{"date": t_key, "unit_group": UNIT}])
          os.makedirs(OUTDIR, exist_ok=True)
          df_t.to_csv(os.path.join(OUTDIR, "yarin.csv"), index=False)
          rows = []
          for d in days:
              try: from datetime import datetime as _dt; dd = _dt.strptime(d.get("datetime",""), "%Y-%m-%d").date()
              except Exception: continue
              if tomorrow <= dd <= week_end: rows.append(rowpick(d))
          pd.DataFrame(rows).to_csv(os.path.join(OUTDIR, "week.csv"), index=False)
          PY
          head -n 5 "${CRIME_DATA_DIR}/yarin.csv" || true
          head -n 5 "${CRIME_DATA_DIR}/week.csv" || true

      - name: 15) Run post_patrol.py (uses risk_hourly + yarin/week)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
          PATROL_TOP_K: ${{ env.PATROL_TOP_K }}
          PATROL_HORIZON_DAYS: ${{ env.PATROL_HORIZON_DAYS }}
        run: |
          set -e
          test -f "${CRIME_DATA_DIR}/risk_hourly.csv" || { echo "‚ùå risk_hourly.csv bulunamadƒ±"; exit 2; }
          python -u scripts/post_patrol.py
          head -n 20 "${CRIME_DATA_DIR}/patrol_recs_multi.csv" || true

      - name: 14) Quick preview (stacking outputs)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          head -n 5 ${CRIME_DATA_DIR}/risk_hourly*.csv 2>/dev/null || true
          head -n 5 ${CRIME_DATA_DIR}/patrol_recs*.csv 2>/dev/null || true
          head -n 5 ${CRIME_DATA_DIR}/metrics_*.csv 2>/dev/null || true

      # ----- CSV ‚Üí PARQUET -----
      - name: Install converter deps
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          python -m pip install -U pip
          pip install --upgrade polars pyarrow

      - name: Convert CSV ‚Üí Parquet
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -e
          mkdir -p parquet_out
          python scripts/csv_to_parquet.py \
            --input "${{ env.CRIME_DATA_DIR }}/" \
            --output parquet_out/ \
            --compression zstd \
            --stats

      # ----- FRESHNESS & OVERVIEW -----
      - name: Emit single-row ‚Äúlatest date‚Äù CSVs (with header)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        env:
          CRIME_DATA_DIR: ${{ env.CRIME_DATA_DIR }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, pandas as pd
          from pathlib import Path
          base = Path(os.environ.get("CRIME_DATA_DIR","crime_prediction_data"))
          targets = [
            ("sf_crime.csv",                    ["incident_datetime","datetime","date"]),
            ("sf_911_last_5_year.csv",          ["call_datetime","incident_datetime","date"]),
            ("sf_311_last_5_years.csv",         ["requested_datetime","opened_date","date"]),
            ("sf_weather_5years.csv",           ["date"]),
            ("sf_crime_grid_full_labeled.csv",  ["date"]),
            ("sf_crime_08.csv",                 ["date"]),
            ("risk_hourly.csv",                 ["date"]),
          ]
          overview = []
          for fname, candidates in targets:
            fpath = base / fname
            if not fpath.exists(): continue
            try:
              df_head = pd.read_csv(fpath, nrows=5000, low_memory=False)
              rowcount = sum(1 for _ in open(fpath, 'r', encoding='utf-8', errors='ignore')) - 1
            except Exception: continue
            col = next((c for c in candidates if c in df_head.columns), None)
            if col is None: continue
            mx = pd.to_datetime(df_head[col], errors="coerce").dt.date.max()
            if pd.isna(mx): continue
            latest_path = fpath.with_name(fpath.stem + "_latest.csv")
            pd.DataFrame([{col: mx.isoformat()}]).to_csv(latest_path, index=False)
            overview.append({"file": fname,"date_column": col,"max_date": mx.isoformat(),"rows_total_approx": rowcount if rowcount >= 0 else ""})
          if overview:
            pd.DataFrame(overview, columns=["file","date_column","max_date","rows_total_approx"]).to_csv(base / "latest_overview.csv", index=False)
          PY
          head -n 10 "${CRIME_DATA_DIR}/latest_overview.csv" || true
          ls -1 "${CRIME_DATA_DIR}"/*_latest.csv 2>/dev/null || true

      - name: Sanity packlist (mtime + head)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          echo "== PACK CANDIDATES (mtime) =="
          while IFS= read -r p; do
            [ -f "$p" ] && printf "%-48s  %s\n" "$(basename "$p")" "$(stat -c %y "$p" 2>/dev/null || stat -f %Sm "$p")"
          done <<EOF
          ${{ env.CRIME_DATA_DIR }}/sf_crime.csv
          ${{ env.CRIME_DATA_DIR }}/sf_crime_y.csv
          ${{ env.CRIME_DATA_DIR }}/sf_911_last_5_year.csv
          ${{ env.CRIME_DATA_DIR }}/sf_311_last_5_years.csv
          ${{ env.CRIME_DATA_DIR }}/sf_weather_5years.csv
          ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.csv
          ${{ env.CRIME_DATA_DIR }}/sf_crime_08.csv
          ${{ env.CRIME_DATA_DIR }}/risk_hourly.csv
          EOF
          echo "== quick heads =="
          head -n 3 "${{ env.CRIME_DATA_DIR }}/sf_crime.csv" || true
          head -n 3 "${{ env.CRIME_DATA_DIR }}/sf_911_last_5_year.csv" || true
          head -n 3 "${{ env.CRIME_DATA_DIR }}/sf_311_last_5_years.csv" || true
          head -n 3 "${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.csv" || true
          head -n 3 "${{ env.CRIME_DATA_DIR }}/sf_crime_08.csv" || true

      - name: Write build info
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          {
            echo "run_number=${GITHUB_RUN_NUMBER}"
            echo "run_id=${GITHUB_RUN_ID}"
            echo "commit=${GITHUB_SHA}"
            echo "branch=${GITHUB_REF_NAME}"
            echo "built_at_utc=$(date -u +'%Y-%m-%dT%H:%M:%SZ')"
          } > "${CRIME_DATA_DIR}/_build_info.txt"
          cat "${CRIME_DATA_DIR}/_build_info.txt"

      - name: Clobber duplicates outside CRIME_DATA_DIR (optional)
        if: ${{ steps.gate.outputs.proceed == 'true' }}
        run: |
          set -euo pipefail
          find . -type f -name 'sf_911_last_5_year*.csv' ! -path "./${{ env.CRIME_DATA_DIR }}/*" -delete || true
          find . -type f -name 'sf_311_last_5_years*.csv' ! -path "./${{ env.CRIME_DATA_DIR }}/*" -delete || true

      # ----- ARTIFACTS -----
      - name: Upload Parquet artifact (opsiyonel)
        if: ${{ steps.gate.outputs.proceed == 'true' && (github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.persist == 'artifact')) }}
        uses: actions/upload-artifact@v4
        with:
          name: sf-crime-parquet
          path: parquet_out/
          if-no-files-found: warn

      - name: Upload artifact (varsayƒ±lan)
        if: ${{ steps.gate.outputs.proceed == 'true' && (github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.persist == 'artifact')) }}
        uses: actions/upload-artifact@v4
        with:
          name: sf-crime-pipeline-output
          path: |
            ${{ env.CRIME_DATA_DIR }}/sf_crime.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_y.csv
            ${{ env.CRIME_DATA_DIR }}/sf_population.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_08.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_07.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_06.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_05.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_04.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_03.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_02.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_01.csv
            ${{ env.CRIME_DATA_DIR }}/sf_crime_grid_full_labeled.csv
            ${{ env.CRIME_DATA_DIR }}/sf_311_last_5_year*.csv
            ${{ env.CRIME_DATA_DIR }}/sf_311_last_5_years*.csv
            ${{ env.CRIME_DATA_DIR }}/sf_311_last_5_years_y.csv
            ${{ env.CRIME_DATA_DIR }}/sf_911_last_5_year.csv
            ${{ env.CRIME_DATA_DIR }}/sf_911_last_5_year_y.csv
            ${{ env.CRIME_DATA_DIR }}/sf_bus_stops_with_geoid.csv
            ${{ env.CRIME_DATA_DIR }}/sf_train_stops_with_geoid.csv
            ${{ env.CRIME_DATA_DIR }}/sf_pois_cleaned_with_geoid.csv
            ${{ env.CRIME_DATA_DIR }}/sf_weather_5years.csv
            ${{ env.CRIME_DATA_DIR }}/sf_weather_5years_y.csv
            ${{ env.CRIME_DATA_DIR }}/*.geojson
            ${{ env.CRIME_DATA_DIR }}/risk_hourly*.csv
            ${{ env.CRIME_DATA_DIR }}/patrol_recs*.csv
            ${{ env.CRIME_DATA_DIR }}/metrics_*.csv
            ${{ env.CRIME_DATA_DIR }}/models/*.joblib
            ${{ env.CRIME_DATA_DIR }}/ablation_spatial_te.csv
            ${{ env.CRIME_DATA_DIR }}/oof_base_probs*.npz
            ${{ env.CRIME_DATA_DIR }}/yarin.csv
            ${{ env.CRIME_DATA_DIR }}/week.csv
          if-no-files-found: warn
          retention-days: 14
